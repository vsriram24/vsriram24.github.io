{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Applying PyTorch\"\n",
        "description: \"Using basic PyTorch to process and analyze BRCA data\"\n",
        "author: \"Vivek Sriram\"\n",
        "date: \"8/19/2024\"\n",
        "date-modified: \"8/19/2024\"\n",
        "image: zolaCake.jpg\n",
        "categories:\n",
        "  - Tutorials\n",
        "draft: true\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "In today's blog post, I will take the PyTorch foundation I built in my previous post and apply it to a new dataset with a more complicated neural network and additional hyperparameter training.\n",
        "\n",
        "With background out of the way, let's get started\\~\n",
        "\n",
        "```{{r}}\n",
        "library(reticulate)\n",
        "use_python('/opt/anaconda3/bin/python')\n",
        "```\n",
        "\n",
        "Import required packages.\n"
      ],
      "id": "b066c6b9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import required packages\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "95137ea4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "id": "557808af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch_geometric\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool"
      ],
      "id": "d30176e7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check to see if we can train our model on a hardware accelerator like the GPU or MPS if available. Otherwise, we'll use the CPU.\n"
      ],
      "id": "5958d32f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "id": "64aa78d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data\n",
        "\n",
        "### Importing and transforming the data\n",
        "\n",
        "BRCA TCGA Pan-Cancer Atlas dataset: - patients with survival information and gene expression data from cBioPortal - network of biological interactions between those nodes from Pathway commons. - Dataset contains the gene features of 1082 patients, as well as the overall survival time (in months) of each patient as a label.\n",
        "\n",
        "1082 graphs 9288 nodes 271,771 edges 1082 features\n",
        "\n",
        "Integrated breast cancer BRCA TCGA data from cBioPortal and a biological network for node connections from Pathway commons.\n",
        "\n",
        "Data was preprocessed to form a single dataset that could be converted to PyTorch Geometric data ojbects.\n",
        "\n",
        "Dataset contains gene features of each patient and the overall survival time of each patient\n"
      ],
      "id": "4994219c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ppi = torch_geometric.datasets.PPI(\n",
        "  root='./posts/pytorch-brca/'\n",
        ")"
      ],
      "id": "b42d23dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "brca = torch_geometric.datasets.BrcaTcga(\n",
        "  root='./posts/pytorch-brca/'\n",
        ")"
      ],
      "id": "3baa0566",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "len(brca)\n",
        "brca.num_classes\n",
        "brca.num_node_features"
      ],
      "id": "1753435d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our dataset has 1082 graphs, with 824 classes. Each node in our dataset has 1 feature.\n"
      ],
      "id": "82fa5f43"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "brca_graph0 = brca[0]\n",
        "\n",
        "print(brca_graph0)\n",
        "brca_graph0.is_undirected()\n",
        "\n",
        "# Access the node features, edge indices, and target label\n",
        "node_features = brca_graph0.x\n",
        "edge_index = brca_graph0.edge_index\n",
        "target = brca_graph0.y\n",
        "\n",
        "print(node_features)  # Print the node features\n",
        "print(edge_index)  # Print the edge indices\n",
        "print(target)  # Print the target label"
      ],
      "id": "af2b87a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first graph in our dataset has 9288 nodes, each with a single feature. There are 271,771 directed edges in our graph. And, this data object is holding exactly one graph-level target.\n"
      ],
      "id": "16cd9615"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply 70/30 train/test split\n",
        "splitIndex = round(len(brca)*.7)\n",
        "\n",
        "training_data = brca[:splitIndex]\n",
        "test_data = brca[splitIndex:]"
      ],
      "id": "ee48120b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing data for training with `DataLoaders`\n",
        "\n",
        "Here we define a batch size of 64 - each element in the `DataLoader` iterable will return a batch of 64 features and labels.\n"
      ],
      "id": "596b25e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(\n",
        "  training_data, \n",
        "  batch_size=batch_size, \n",
        "  shuffle=True\n",
        ")\n",
        "\n",
        "for step, data in enumerate(train_dataloader):\n",
        "    data = data.to(device)\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "  test_data, \n",
        "  batch_size=batch_size, \n",
        "  shuffle=False\n",
        ")\n",
        "\n",
        "for step, data in enumerate(test_dataloader):\n",
        "    data = data.to(device)"
      ],
      "id": "dcc3d1f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modeling\n",
        "\n",
        "### Defining a neural network\n"
      ],
      "id": "72a487df"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, hidden_channels, num_node_features):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.conv4(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final regression layer\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x.squeeze()  # Remove the extra dimension"
      ],
      "id": "8df26ace",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = GCN(hidden_channels=64, num_node_features=1).to(device)\n",
        "print(f\"Model structure: {model}\")"
      ],
      "id": "c86ad7f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "id": "58eb88f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizing the Model Parameters\n",
        "\n",
        "#### Hyperparameters\n"
      ],
      "id": "f1d9892b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "id": "8adff2e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Optimization Loop\n"
      ],
      "id": "e48d3f39"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize the loss function and the optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "id": "cbcd8291",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Full Implementation\n"
      ],
      "id": "d6e83719"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    for step, data in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(\n",
        "          data.x, \n",
        "          data.edge_index, \n",
        "          data.batch\n",
        "        )\n",
        "        \n",
        "        loss = loss_fn(\n",
        "          pred, \n",
        "          data.y.view(-1)\n",
        "        )\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "id": "295c9b98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for step, data in enumerate(test_dataloader):\n",
        "            data = data.to(device)\n",
        "\n",
        "            pred = model(\n",
        "              data.x, \n",
        "              data.edge_index, \n",
        "              data.batch\n",
        "            )\n",
        "            \n",
        "            loss += loss_fn(\n",
        "              pred, \n",
        "              data.y.view(-1)\n",
        "            ).item()\n",
        "\n",
        "    test_loss /= num_batches"
      ],
      "id": "d6a771e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and then backpropagates the prediction error to adjust the model’s parameters.\n",
        "\n",
        "We can also check the model’s performance against the test dataset to ensure it is learning.\n",
        "\n",
        "The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch.\n"
      ],
      "id": "ebf12744"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "id": "b8ed0d31",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now use this model to make individual predictions.\n",
        "\n",
        "To use the model, we pass it the input data. This executes the model’s `forward`, along with some background operations. Note that we do not call `model.forward()` directly!\n"
      ],
      "id": "e63e2373"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "predictions = []  # List to store the predicted outputs\n",
        "test_losses = []\n",
        "with torch.no_grad():\n",
        "    test_loss = 0\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)  # Move the batch of data to CUDA device\n",
        "        out = model(data.x.double(), data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y.view(-1).double())\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        average_test_loss = test_loss / len(test_loader)\n",
        "        test_losses.append(average_val_loss)\n",
        "\n",
        "        predictions.append(out.cpu().detach().numpy())  # Convert the predictions to NumPy array\n",
        "\n",
        "# Concatenate the predictions from multiple batches\n",
        "predictions = np.concatenate(predictions)\n",
        "\n",
        "# Print the predictions\n",
        "print(predictions[:10])\n",
        "[43.55725092 44.08028075 38.91240359 49.6856782  57.11545471 49.2488646\n",
        " 46.26797878 69.44459431 40.08783161 47.09477596]\n",
        "Making Predictions and calculating MSE\n",
        "# Convert the test data batches to a list of Data objects\n",
        "test_data_batches = [\n",
        "    Data(x=batch.x, edge_index=batch.edge_index, y=batch.y) for batch in test_loader\n",
        "]\n",
        "\n",
        "# Convert the predictions to PyTorch tensors\n",
        "predictions = torch.tensor(predictions)\n",
        "\n",
        "# Convert the ground truth labels of the test data to a PyTorch tensor\n",
        "y_true = torch.cat([batch.y for batch in test_data_batches])\n",
        "\n",
        "# Calculate the mean squared error (MSE) loss using PyTorch's function\n",
        "mse_loss = torch.nn.functional.mse_loss(predictions.view(-1), y_true.view(-1))\n",
        "\n",
        "print(f\"Mean Squared Error (MSE) Loss: {mse_loss:.4f}\")\n",
        "Mean Squared Error (MSE) Loss: 957.8370\n",
        "Plotting the convergence of the model\n",
        "# Visualize convergence results\n",
        "plt.figure()\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Convergence of Model')\n",
        "plt.show()"
      ],
      "id": "55a6afff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary\n",
        "\n",
        "This concludes my walkthrough of using basic PyTorch to process data and train a neural network. As mentioned, in a future post in the coming weeks, I will iterate on this pipeline, demonstrating a different neural network architecture and more data exploration for a different dataset. Until next time, \\[VS\\]Coders!"
      ],
      "id": "15352c3f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/vsriram/Library/Python/3.13/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}