{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"A Python introduction to neural networks and backpropagation\"\n",
        "description: \"Andrej Karpathy's 'Neural Networks: Zero to Hero' video #1\"\n",
        "author: \"Vivek Sriram\"\n",
        "date: \"7/8/2024\"\n",
        "date-modified: \"7/8/2024\"\n",
        "image: zolaFish.jpeg\n",
        "categories:\n",
        "  - Tutorials\n",
        "draft: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "This is a walkthrough of Andrej Karpathy's video \"[The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0)\". This video is the first in his YouTube series, \"[Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ).\"\n",
        "\n",
        "![A screen-grab of Andrej's video](karpathyVideo.png)\n",
        "\n",
        "[Neural networks](https://aws.amazon.com/what-is/neural-network/#:~:text=A%20neural%20network%20is%20a,that%20resembles%20the%20human%20brain.) are mathematical models used to represent nodes and the signals they send to one another through their links. Neural networks replicate the structure of the brain, where interconnected neurons send messages to each other through electric signals across the synpases that bridge them together. While individual nodes can perform only simple operations, many nodes connected together in a network can perform complex computational tasks.\n",
        "\n",
        "The general structure of a neural network for machine learning includes three types of nodes, grouped into different \"layers\" within the neural network. The first set of nodes are the input nodes, corresponding to input (or training data). The second set of nodes refer to intermediate (hidden) nodes. The final type of node is the output node, corresponding to the result of processing the input data through the hidden nodes. This output layer typically comes in the form of a loss function, characterizing the difference between the output of the model and the expected output. The goal of optimizing a neural network is to minimize this output loss to best match the behavior of the input data.\n",
        "\n",
        "![The basic format of a neural network](nn.jpeg)\n",
        "\n",
        "Backpropagation is an algorithm for supervised learning of neural networks using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). This method will calculate the gradient of each intermediate node in the network with respect to the loss function, allowing us to iteratively tune their weights to minimize the overall loss.\n",
        "\n",
        "In his video tutorial, Andrej shows how to construct a neural network from scratch and perform backpropagation on it to optimize the weights of the network. The code presented in this example is a direct copy of the code walked through in the video, streamlined a bit for interpretation. Writing this blog post helped me solidify my understanding of the material (and also helped me practice writing Python code in Quarto :) ). I would highly recommend following along with this tutorial and further videos for a hands-on, ground-up exploration of neural networks and language models! I aim to work through his other tutorials in the future as well.\n",
        "\n",
        "With background out of the way, let's get started\\~\n",
        "\n",
        "```{{r}}\n",
        "library(reticulate)\n",
        "use_python('/opt/anaconda3/bin/python')\n",
        "```\n"
      ],
      "id": "38631025"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import required packages\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "id": "1f167043",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Defining functions and manually calculating derivatives\n",
        "\n",
        "We can start by thinking about a simple mathematical expression to give us some intuition behind the workings of individual neurons.\n",
        "\n",
        "Let's define a scalar value function *f(*x*)* that takes scalar input and returns scalar output. We can apply this function to a single value or a range of values.\n"
      ],
      "id": "ebecc59b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# e.g. scalar value function that takes scalar input and returns scalar output\n",
        "def f(x):\n",
        "    return 3*x**2 - 4*x + 5\n",
        "\n",
        "# e.g. single value\n",
        "f(3.0)\n",
        "\n",
        "# e.g. range of values\n",
        "xs = np.arange(-5, 5, 0.25)\n",
        "ys = f(xs)\n",
        "ys"
      ],
      "id": "bb1f256d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the output of our function to see the association between our input and output as well.\n"
      ],
      "id": "492c9cfb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(xs, ys)"
      ],
      "id": "0d77a367",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Determining the derivative of *f* would let us identify inflection points in our data. Let's calculate the derivative of *f* at 3 (i.e. *f'(*3)) numerically using the fundamental law of calculus:\n",
        "\n",
        "$$\n",
        "\\lim_{h\\to\\infty} \\frac{f(x+h)-f(x)}{h}\n",
        "$$\n"
      ],
      "id": "9aee44b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "h = 0.0000000001\n",
        "x = 3.0\n",
        "\n",
        "(f(x+h) - f(x))/h"
      ],
      "id": "3ee03401",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that if our *h* is too small for Python, we will end up with a floating point error. With some trial and error for different values of *h*, we can see *f'*(3) = 14\n",
        "\n",
        "Now let's make a function that is a little more complicated: $$\n",
        "d(a, b, c) = a*b + c\n",
        "$$\n"
      ],
      "id": "3d19503a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = 2.0\n",
        "b = -3.0\n",
        "c = 10.0\n",
        "d1 = a*b + c"
      ],
      "id": "5041a9a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, we can calculate the derivative of ***d***. This time, since we have three inputs, we have to pick a variable with respect to which we calculate the derivative. Let's numerically calculate the derivative of ***d*** with respect to *a*.\n"
      ],
      "id": "1a298330"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "h = 0.0000001\n",
        "\n",
        "#derivative wrt a\n",
        "a += h\n",
        "\n",
        "d2 = a*b + c\n",
        "\n",
        "print('d1', d1)\n",
        "print('d2', d2)\n",
        "print('slope', (d2-d1)/h)"
      ],
      "id": "3a4d6544",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can do the same with respect to *b* as well.\n"
      ],
      "id": "12039785"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#derivative wrt b\n",
        "b += h\n",
        "a = 2.0\n",
        "\n",
        "d2 = a*b + c\n",
        "\n",
        "print('d1', d1)\n",
        "print('d2', d2)\n",
        "print('slope', (d2-d1)/h)"
      ],
      "id": "c0e03e0d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now have some intuition for how functions and derivatives work.\n",
        "\n",
        "# The 'Value' Class\n",
        "\n",
        "Let's define a class \"Value\" to store the individual values that come together to make a function / mathematical expression. Each 'Value' can be thought of as a node in a neural network.\n"
      ],
      "id": "4925396e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Value:\n",
        "    def __init__(self, data, _children=(), _op='', label=''):\n",
        "        self.data = data\n",
        "        self.grad = 0.0\n",
        "        self._backward = lambda: None #default: nothing\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op\n",
        "        self.label = label\n",
        "\n",
        "    # Nicer looking way to see what the value actually is instead of an object\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data})\"\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def __radd__(self, other): # other * self\n",
        "        return self + other\n",
        "        \n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def tanh(self):\n",
        "        x = self.data\n",
        "        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n",
        "        out = Value(t, (self, ), 'tanh')\n",
        "        def _backward():\n",
        "            self.grad +=  (1 - t**2) * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def exp(self):\n",
        "        x = self.data\n",
        "        out = Value(math.exp(x), (self, ), 'exp')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.data * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "        out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other * (self.data**(other-1)) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "    \n",
        "    def __truediv__(self, other): #self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        \n",
        "        build_topo(self)\n",
        "        \n",
        "        # call _backward() in the right topological order\n",
        "        self.grad = 1.0\n",
        "        for node in reversed(topo):\n",
        "            node._backward()"
      ],
      "id": "ab78084b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see how to perform mathematical operations using our `Value` class:\n"
      ],
      "id": "93dc05df"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = Value(2.0)\n",
        "b = Value(4.0)\n",
        "a-b"
      ],
      "id": "12bfd87f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's define an example function ***L*** that makes use of our `Value` class:\n",
        "\n",
        "$$\n",
        "L(a, b, c, f) = (a*b + c)*f\n",
        "$$\n"
      ],
      "id": "7ca7ced0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = Value(2.0, label='a')\n",
        "b = Value(-3.0, label='b')\n",
        "c = Value(10.0, label='c')\n",
        "e = a*b; e.label = 'e'\n",
        "d = e + c; d.label = 'd'\n",
        "f = Value(-2.0, label = 'f')\n",
        "L = d * f; L.label = 'L'\n",
        "L"
      ],
      "id": "2ba646b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the code in our `Value` class, we are able to see for each node which nodes came before it and what the operation was to generate the current node.\n"
      ],
      "id": "88da7bd6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d._prev\n",
        "d._op"
      ],
      "id": "aec1b387",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also define a function 'draw_dot' to be able to visualize the components of our function. Here, we build out a graph using the GraphViz API. We then iterate over all nodes and create corresponding nodes and edges (including values and operations as different node types in our network).\n"
      ],
      "id": "6512f4a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "def trace(root):\n",
        "    # builds a set of all nodes and edges in a graph\n",
        "    nodes, edges = set(), set()\n",
        "    def build(v):\n",
        "        if v not in nodes:\n",
        "            nodes.add(v)\n",
        "            for child in v._prev:\n",
        "                edges.add((child, v))\n",
        "                build(child)\n",
        "    build(root)\n",
        "    return nodes, edges\n",
        "\n",
        "def draw_dot(root):\n",
        "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR = left to right\n",
        "\n",
        "    nodes, edges = trace(root)\n",
        "    for n in nodes:\n",
        "        uid = str(id(n))\n",
        "        # for any value in the graph, create a rectangular ('record') node for it\n",
        "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape = 'record')\n",
        "        if n._op:\n",
        "            # if this value is a result of some operation, create an op node for it\n",
        "            dot.node(name = uid + n._op, label = n._op)\n",
        "            # and connect this node to it\n",
        "            dot.edge(uid + n._op, uid)\n",
        "\n",
        "    for n1, n2 in edges:\n",
        "        # connect n1 to the op node of n2\n",
        "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
        "\n",
        "    return dot"
      ],
      "id": "62ef9b38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "draw_dot(L)"
      ],
      "id": "37c7f8e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Manual backpropagation example\n",
        "\n",
        "With our basic function ***L*** now represented as a network of values and operations, let's perform manual backpropagation.\n",
        "\n",
        "We'll start from ***L*** and work backwards, taking the derivative with respect to ***L*** at each intermediate value. This exercise is equivalent to determining the derivative of an output ***L*** with respect to the internal weights of a neural network.\n"
      ],
      "id": "5e294f92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Let's calculate gradient of L wrt a manually using the fundamental theorem of calculus\n",
        "# (f(x+h) - f(x))/h\n",
        "def lol():\n",
        "    h = 0.001\n",
        "    \n",
        "    a = Value(2.0, label='a')\n",
        "    b = Value(-3.0, label='b')\n",
        "    c = Value(10.0, label='c')\n",
        "    e = a*b; e.label = 'e'\n",
        "    d = e + c; d.label = 'd'\n",
        "    f = Value(-2.0, label = 'f')\n",
        "    L = d * f; L.label = 'L'\n",
        "    L1 = L.data\n",
        "\n",
        "    a = Value(2.0 + h, label='a')\n",
        "    b = Value(-3.0, label='b')\n",
        "    c = Value(10.0, label='c')\n",
        "    e = a*b; e.label = 'e'\n",
        "    d = e + c; d.label = 'd'\n",
        "    f = Value(-2.0, label = 'f')\n",
        "    L = d * f; L.label = 'L'\n",
        "    L2 = L.data\n",
        "\n",
        "    print((L2-L1)/h)"
      ],
      "id": "2dae3ec9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#dL/da\n",
        "lol()"
      ],
      "id": "bef6474d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can go through this entire network structure and set the gradients for each node with respect to ***L***.\n"
      ],
      "id": "a8098347"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#We know dL/dL = 1\n",
        "L.grad = 1"
      ],
      "id": "9368c6bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#L = d*f\n",
        "#So dL/df = d\n",
        "#and dL/dd = f\n",
        "\n",
        "f.grad = 4.0 # this is just the value of d\n",
        "d.grad = -2.0 # this is just the value of f"
      ],
      "id": "78043a82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# what is dL/dc?\n",
        "# We can use dL/dd and dd/dc and apply the chain rule\n",
        "# dL / dc = (dL/dd) * (dd/dc) = -2*1 = -2\n",
        "# dL/de is the same, -2\n",
        "c.grad = -2.0 # this is just the value of d\n",
        "e.grad = -2.0 # this is just the value of f"
      ],
      "id": "dfb5489b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# dL/da = dL/de * de/da = -2*b = -2*-3 = 6\n",
        "# dL/db = dL/de * de/db = -2*a = -2*2 = -4\n",
        "a.grad = 6.0 # this is just the value of d\n",
        "b.grad = -4.0 # this is just the value of f"
      ],
      "id": "f9ded7ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "draw_dot(L)"
      ],
      "id": "af53528f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is our key takeaway from this example:\n",
        "\n",
        "**Backpropagation is just the recursive application of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) backwards through the computational graph** **of your neural network.**\n",
        "\n",
        "# Introducing an activation function.\n",
        "\n",
        "In our previous example, we had an output ***L*** that could take on any value. Now let's make use of the hyperbolic tangent ($tanh$) activation function to limit our output to a range of -1 to 1.\n",
        "\n",
        "$Tanh$ looks as follows:\n"
      ],
      "id": "89041f6d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Squashing/activation function - tan(h)\n",
        "plt.plot(np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2)));\n",
        "plt.grid();"
      ],
      "id": "9847cabe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define a new function $o = tanh(x1*w1 + x2*w2 + b)$\n"
      ],
      "id": "599cf1ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# inputs x1, x2\n",
        "x1 = Value(2.0, label = 'x1')\n",
        "x2 = Value(0.0, label = 'x2')\n",
        "\n",
        "# weights w1, w2\n",
        "w1 = Value(-3.0, label = 'w1')\n",
        "w2 = Value(1.0, label = 'w2')\n",
        "\n",
        "# bias of the neuron (crazy bias makes clean output in this example)\n",
        "b = Value (6.881373587019542, label = 'b')\n",
        "\n",
        "#x1*w1 + x2*w2 + b\n",
        "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
        "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
        "\n",
        "x1w1x2w2 = x1w1 + x2w2;\n",
        "x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
        "\n",
        "# n is our cell body activation without the activation function\n",
        "n = x1w1x2w2 + b;\n",
        "n.label = 'n'\n",
        "\n",
        "# Apply activation function (defined in Value class earlier)\n",
        "o = n.tanh(); o.label = 'o'"
      ],
      "id": "fbb6e92f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the network that represents the function we just defined:\n"
      ],
      "id": "709d91e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "draw_dot(o)"
      ],
      "id": "1b266aab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We care most about the derivative of ***o*** with respect to the weights *w1* and *w2*. In a normal neural network, we would have many more input and intermediate nodes (not just the two as in this example). We will calculate the gradients for this network by hand.\n"
      ],
      "id": "201c2548"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "o.grad = 1.0\n",
        "\n",
        "# o = tanh(n)\n",
        "# do/dn = 1-tanh^2(n) = 1 - o^2\n",
        "n.grad = 1-o.data**2\n",
        "\n",
        "# do/db = do/dn * dn/db = (1-o^2)*1 = 1-o^2\n",
        "# d(x1w1x2w2)/db = do/dn * dn/d(x1w1x2w2) = (1-o^2)*1 = 1-o^2\n",
        "x1w1x2w2.grad = 1-o.data**2\n",
        "b.grad = 1-o.data**2\n",
        "\n",
        "# same logic of back-propagation wrt '+'\n",
        "x1w1.grad = 1-o.data**2\n",
        "x2w2.grad = 1-o.data**2\n",
        "\n",
        "#do/dx2 = w2 * do/d(x2w2)\n",
        "x2.grad = w2.data * x2w2.grad\n",
        "#do/dw2 = x2 * do/d/(x2w2)\n",
        "w2.grad = x2.data * x2w2.grad\n",
        "\n",
        "# same logic as for x2/w2\n",
        "x1.grad = w1.data * x1w1.grad\n",
        "w1.grad = x1.data * x1w1.grad"
      ],
      "id": "b492e8bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "draw_dot(o)"
      ],
      "id": "4863c77a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, because *w1*'s gradient is positive, if we want this neuron's output to increase, then we should increase *w1*. *w2* doesn't affect the output of this function because its gradient is 0.\n",
        "\n",
        "# Automating backpropagation\n",
        "\n",
        "Let's stop doing this back-propagation manually! Take a look at the logic for `_backward` and `backward`in the `Value` class to see how we handle this (we apply a topological sort to our data in the `backward` function). We also ensure that we never call `_backward` on a node before we've called it on its children. Lastly, we make sure that we accumulate gradients in the `backward` function.\n"
      ],
      "id": "c794b4a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "o.grad = 1.0\n",
        "\n",
        "o._backward()\n",
        "n._backward()\n",
        "b._backward()\n",
        "x1w1x2w2._backward()\n",
        "x2w2._backward()\n",
        "x1w1._backward()"
      ],
      "id": "694a9db1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "draw_dot(o)"
      ],
      "id": "0c88d9f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "o.backward()\n",
        "draw_dot(o)"
      ],
      "id": "11f0a2ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = Value(3.0, label = 'a')\n",
        "b = a+a; b.label = 'b'\n",
        "b.backward()\n",
        "draw_dot(b)"
      ],
      "id": "3865ca47",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Everything works! Yay!\n",
        "\n",
        "# Breaking up *`tanh`* into its individual components\n",
        "\n",
        "Instead of using a $tanh$ function in our `Value` class, we can break it up into exponent and division functions to see an example of a more complicated network.\n"
      ],
      "id": "0330c087"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# inputs x1, x2\n",
        "x1 = Value(2.0, label = 'x1')\n",
        "x2 = Value(0.0, label = 'x2')\n",
        "\n",
        "# weights w1, w2\n",
        "w1 = Value(-3.0, label = 'w1')\n",
        "w2 = Value(1.0, label = 'w2')\n",
        "\n",
        "# bias of the neuron\n",
        "b = Value (6.881373587019542, label = 'b')\n",
        "\n",
        "#x1*w1 + x2*w2 + b\n",
        "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
        "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
        "\n",
        "x1w1x2w2 = x1w1 + x2w2;\n",
        "x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
        "\n",
        "# n is our cell body activation without the activation function\n",
        "n = x1w1x2w2 + b;\n",
        "n.label = 'n'\n",
        "\n",
        "# Apply activation function (defined in Value class earlier)\n",
        "e = (2*n).exp()\n",
        "o = (e-1)/(e+1)\n",
        "o.label = 'o'\n",
        "o.backward()"
      ],
      "id": "b2b9d281",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "draw_dot(o)"
      ],
      "id": "22168609",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, even after breaking our `tanh` function into its individual components, our forward and backward passes are still correct! Note that the level at which you perform your individual operations is entirely up to you (e.g. `tanh` vs. its individual components). All that matters is that you have input and output and that you can do forward/backward passing of your operations.\n",
        "\n",
        "# Backpropagation with PyTorch\n",
        "\n",
        "Now that we've developed backpropagation manually, let's see how it can be performed in [PyTorch](https://pytorch.org/docs/stable/index.html). With PyTorch, everything is based around [tensors](https://en.wikipedia.org/wiki/Tensor#:~:text=In%20mathematics%2C%20a%20tensor%20is,scalars%2C%20and%20even%20other%20tensors.) rather than scalars.\n"
      ],
      "id": "551edeae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "\n",
        "# Cast to double to get 64bit precision\n",
        "x1 = torch.Tensor([2.0]).double()\n",
        "# by default, pytorch will say leaf nodes don't have gradients to improve efficiency\n",
        "x1.requires_grad = True\n",
        "\n",
        "x2 = torch.Tensor([0.0]).double()\n",
        "x2.requires_grad = True\n",
        "\n",
        "w1 = torch.Tensor([-3.0]).double()\n",
        "w1.requires_grad = True\n",
        "\n",
        "w2 = torch.Tensor([1.0]).double()\n",
        "w2.requires_grad = True\n",
        "\n",
        "b = torch.Tensor([6.8813735870195432]).double()\n",
        "b.requires_grad = True\n",
        "\n",
        "n = x1*w1 + x2*w2 + b\n",
        "o = torch.tanh(n)\n",
        "\n",
        "# PyTorch tensors have data and grad elements\n",
        "print(o.data.item())\n",
        "# PyTorch has a backward function too\n",
        "o.backward()\n",
        "\n",
        "print('---')\n",
        "print('x2', x2.grad.item())\n",
        "print('w2', w2.grad.item())\n",
        "print('x1', x1.grad.item())\n",
        "print('w1', w1.grad.item())"
      ],
      "id": "eb7c24ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch makes all of our calculations much more efficient. We can do all of these operations in parallel with very large tensors and not just scalar values.\n",
        "\n",
        "# A simple neural network\n",
        "\n",
        "We've had enough fun with \"neural network adjacent\" mathematical expressions and their corresponding computational topologies.\n",
        "\n",
        "Let's implement a simple neural network. We will base this off of a [multilayer perceptron](https://www.datacamp.com/tutorial/multilayer-perceptrons-in-machine-learning#) (MLP). We can define a `Neuron` class, `Layer` class, and `MLP` class for our network.\n",
        "\n",
        "A typical neural network neuron looks like the following:\n",
        "\n",
        "![A typical neuron in a neural network](neuron.jpeg)\n"
      ],
      "id": "a61fec44"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Neuron:\n",
        "    def __init__(self, nin):\n",
        "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "        self.b = Value(random.uniform(-1,1))\n",
        "        \n",
        "    # Python goes to __call__ when you use the class as a function\n",
        "    def __call__(self, x):\n",
        "        # w.x + b\n",
        "        # start with self.b, add the dot product of w and x\n",
        "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
        "        out = act.tanh()\n",
        "        return out\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.w + [self.b]"
      ],
      "id": "7d1b7136",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Layer:\n",
        "    # nout is the size of the output of the layer\n",
        "    def __init__(self, nin, nout):\n",
        "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        outs = [n(x) for n in self.neurons]\n",
        "        return outs[0] if len(outs) == 1 else outs\n",
        "\n",
        "    def parameters(self):\n",
        "        params = []\n",
        "        for neuron in self.neurons:\n",
        "            ps = neuron.parameters()\n",
        "            params.extend(ps)\n",
        "        return params\n",
        "        \n",
        "        # Same as:\n",
        "        # return [p for neuron in self.neurons for p in neuron.parameters()]"
      ],
      "id": "e8582a89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MLP:\n",
        "    # nouts is the list of layer sizes we want\n",
        "    def __init__(self, nin, nouts):\n",
        "        sz = [nin] + nouts\n",
        "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for layer in self.layers for p in layer.parameters()]"
      ],
      "id": "06061544",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based upon our defined classes, let's initialize our MLP.\n"
      ],
      "id": "93cc7691"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = [2.0, 3.0, -1.0]\n",
        "n = MLP(3, [4, 4, 1])\n",
        "n(x)"
      ],
      "id": "685d72fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "draw_dot(n(x))"
      ],
      "id": "9f8984aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wow, our function is much crazier than our initial examples! Obviously we're never going to manually backpropagate such an example... let's have PyTorch do it for us.\n",
        "\n",
        "We start by defining some sample input data and our desired targets. We then use our baseline MLP to calculate model outputs from the input data.\n"
      ],
      "id": "efedd6e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example data\n",
        "xs = [\n",
        "    [2.0, 3.0, -1.0],\n",
        "    [3.0, -1.0, 0.5],\n",
        "    [0.5, 1.0, 1.0],\n",
        "    [1.0, 1.0, -1.0]\n",
        "]\n",
        "\n",
        "ys = [1.0, -1.0, -1.0, 1.0] #desired targets\n",
        "\n",
        "# Apply our MLP to predict y from x\n",
        "ypred = [n(x) for x in xs]\n",
        "ypred"
      ],
      "id": "75f6031c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compare our model outputs to the expected outputs using a loss function such as [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE).\n"
      ],
      "id": "f7e7f628"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# loss will measure how good our neural net is\n",
        "# let's do mean squared error\n",
        "loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "loss"
      ],
      "id": "b9a04eb3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's backpropagate (automatically this time)!\n"
      ],
      "id": "1fe1abb6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss.backward()"
      ],
      "id": "949d5506",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the gradient of a weight is positive, then decreasing the weight will decrease the overall loss. Similarly, if the gradient is negative, then increasing the weight will decrease the loss.\n"
      ],
      "id": "97ea137c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# If this gradient is positive, then decreasing this weight will decrease our loss\n",
        "# If this is negative, then increasing this weight will decrease our loss\n",
        "n.layers[0].neurons[0].w[0].grad"
      ],
      "id": "11aaf20e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n.layers[0].neurons[0].w[0].data"
      ],
      "id": "b4516ce8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For every parameter in our neural network, let's change the weights slightly to reduce the overall loss. We increase the weight for negative gradients and decrease the weight for positive gradients.\n"
      ],
      "id": "95b17b02"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# for every parameter in our neural net, let's change the weights slightly to reduce the loss\n",
        "# increase for negative grad, decrease for positive grad\n",
        "for p in n.parameters():\n",
        "    p.data += -0.01*p.grad"
      ],
      "id": "9f89270b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our overall loss should have gone down a bit now. Let's recalculate it.\n"
      ],
      "id": "76bdcd47"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ypred = [n(x) for x in xs]\n",
        "loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "loss"
      ],
      "id": "22a08fd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Propagate\n",
        "loss.backward()"
      ],
      "id": "01613d57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ypred"
      ],
      "id": "5991582d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nice, we're able to train our data better now. Let's formalize this process of updating gradients in a loop. This is the same thing as \"[stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\".\n"
      ],
      "id": "85223ec1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reset the neural net\n",
        "x = [2.0, 3.0, -1.0]\n",
        "n = MLP(3, [4, 4, 1])\n",
        "n(x)"
      ],
      "id": "3ba88685",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize input data and desired targets\n",
        "xs = [\n",
        "    [2.0, 3.0, -1.0],\n",
        "    [3.0, -1.0, 0.5],\n",
        "    [0.5, 1.0, 1.0],\n",
        "    [1.0, 1.0, -1.0]\n",
        "]\n",
        "\n",
        "ys = [1.0, -1.0, -1.0, 1.0]"
      ],
      "id": "4fe24ca0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 20 iterations\n",
        "for k in range(20):\n",
        "    # forward pass\n",
        "    ypred = [n(x) for x in xs]\n",
        "    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "\n",
        "    # backward pass\n",
        "    for p in n.parameters():\n",
        "        p.grad = 0.0\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    # \"stochastic gradient descent\"\n",
        "    for p in n.parameters():\n",
        "        p.data += -0.05 * p.grad\n",
        "\n",
        "    print(k, loss.data)"
      ],
      "id": "30449577",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ypred"
      ],
      "id": "d3a0c1e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ta-da! We now understand the intuition behind developing simple neural networks and performing backpropagation to improve their predictive performance!\n",
        "\n",
        "# Takeaways and summary\n",
        "\n",
        "Neural nets are simple mathematical expressions that take input data and weights. Working with neural networks involves a forward pass of input data followed by the application of a loss function.\n",
        "\n",
        "The goal of a neural network for machine learning is to minimize the output loss to get the model to better predict desired targets. Backpropagation can be applied from the loss function to determine the gradients of the intermediate weights of the network. We can then tune the weights of these nodes against the gradient (i.e. gradient descent) to improve the predictive performance of the model.\n",
        "\n",
        "Simulating a blob of neural tissue in this manner can handle all sorts of interesting problems. Generative Pre-trained Transformers (GPTs) uses massive amounts of text from the internet and then predict the next words in a sentence based on context. These are really just fancy neural networks with hundreds of billions of parameters. Different models may use different loss functions and different methods for gradient descent, but the underlying concepts are all consistent.\n",
        "\n",
        "This concludes my walkthrough of Andrej's first neural networks video tutorial. Until next time, \\[VS\\]Coders!"
      ],
      "id": "4e6dafcc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/vsriram/Library/Python/3.13/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}