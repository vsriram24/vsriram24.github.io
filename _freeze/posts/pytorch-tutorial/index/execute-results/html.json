{
  "hash": "78429bb3e59de8f204f0979a2de39ccf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Introduction to PyTorch\ndescription: Using basic PyTorch to process data and train a neural network\nauthor: Vivek Sriram\ndate: 7/29/2024\ndate-modified: 7/29/2024\nimage: zolaCake.jpg\ncategories:\n  - Tutorials\ndraft: false\n---\n\nIn today's blog post, we'll go through the [\"Introduction to PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)\" tutorial available from the PyTorch's online [learning community](https://pytorch.org/tutorials/index.html).\n\n![A screen-grab of the PyTorch introductory documentation](pytorch.png)\n\nThis tutorial is designed to walk through every component you would need to start developing models in PyTorch. With that, many of the elements show you how to complete certain steps in multiple ways... In this walkthrough, I will be selecting a subset of elements from these components and streamlining the steps of data processing and model training to showcase exactly how one would use these tools for an example dataset. \n\nIn a future blog post, I will take the foundation developed through this walkthrough and explore a modifiedd data analysis workflow, with a more complicated neural network and additional hyperparameter training for a different set of data.\n\nWith background out of the way, let's get started\\~\n\n```{{r}}\nlibrary(reticulate)\nuse_python('/opt/anaconda3/bin/python')\n```\n\nMost machine learning workflows involve uploading data, creating models, optimizing model parameters, and performing predictions on input data. This tutorial introduces you to a complete ML workflow implemented in PyTorch.\n\nIn PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters. Tensors are a specialized data structure that are very similar to arrays and matrices. \n\n::: {#58dcd33a .cell execution_count=1}\n``` {.python .cell-code}\n# Import required packages\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {#a0abb367 .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n```\n:::\n\n\n::: {#75f4655a .cell execution_count=3}\n``` {.python .cell-code}\n#import torchvision\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor, Lambda\nfrom torchvision.io import read_image\n```\n:::\n\n\nWe start by checking to see if we can train our model on a hardware accelerator like the GPU or MPS if available. Otherwise, we'll use the CPU.\n\n::: {#c306642b .cell execution_count=4}\n``` {.python .cell-code}\n# Get cpu, gpu or mps device for training.\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing mps device\n```\n:::\n:::\n\n\nCode for processing data samples can get messy and hard to maintain. We ideally want our dataset code to be de-coupled from model training code for better readability and modularity. We will break our analysis into the following two sections: **Data** and **Modeling**.\n\n## Data\n\n### Importing and transforming the data\n\nPyTorch offers domain-specific libraries such as `TorchText`, `TorchVision`, and `TorchAudio`. In this tutorial, we will be using a `TorchVision` dataset. The `torchvision.datasets` module contains `Dataset` objects for many real-world vision data. Here, we will use the FashionMNIST dataset. Fashion-MNIST is a dataset of images consisting of 60,000 training examples and 10,000 test examples. Each example includes a 28x28 grayscale image and an associated label from one of 10 classes.\n\nEach PyTorch `Dataset` stores a set of samples and their corresponding labels. Data do not always come in the final processed form that is required for training ML algorithms. We use *transforms* to perform some manipulation of the data and make it suitable for training. Every TorchVision `Dataset` includes 2 arguments: `transform` to modify the samples and `target_transform` to modify the labels.\n\nThe FashionMNIST features are in PIL Image format. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. To make this transformation, we use the `ToTensor()` function. `ToTensor()` converts a PIL image or NumPy `ndarray` into a `FloatTensor` and scales the image’s pixel intensity values in the range [0., 1.] \n\n::: {#0a1ab475 .cell execution_count=5}\n``` {.python .cell-code}\n# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n```\n:::\n\n\nWe now have our data uploaded! We can index an input `Dataset` manually (i.e. `training_data[index]`) to get individual samples. Here, we use `matplotlib` to visualize some samples in our training data.\n\n::: {#0ea7bd1a .cell execution_count=6}\n``` {.python .cell-code}\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\n\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=613 height=631}\n:::\n:::\n\n\n### Preparing data for training with `DataLoaders`\n\nWhile training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Pythonic multiprocessing to speed up data retrieval. `DataLoader` is an iterable that abstracts this complexity for us in an easy API. We pass our input `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, supporting automatic batching, sampling, shuffling, and multiprocess data loading in the process.\n\nHere we define a batch size of 64 - each element in the `DataLoader` iterable will return a batch of 64 features and labels.\n\n::: {#4d5c6ec1 .cell execution_count=7}\n``` {.python .cell-code}\nbatch_size = 64\n\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64\n```\n:::\n:::\n\n\n### Iterating over the `DataLoader`\n\nNow that we have loaded our data into a `DataLoader`, we can iterate through the dataset as needed (using `next(iter(DataLoader)`). Each iteration returns a batch of `train_features` and `train_labels` (containing `batch_size=64` features and labels respectively). Because we specified `shuffle=True`, the data are shuffled after we iterate over all of our batches.\n\n::: {#787bbacb .cell execution_count=8}\n``` {.python .cell-code}\n# Display image and label.\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\nimg = train_features[0].squeeze()\nlabel = train_labels[0]\nplt.imshow(img, cmap=\"gray\")\nplt.show()\nprint(f\"Label: {label}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFeature batch shape: torch.Size([64, 1, 28, 28])\nLabels batch shape: torch.Size([64])\nLabel: 5\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-2.png){width=415 height=411}\n:::\n:::\n\n\n## Modeling\n\n### Defining a neural network\n\nNeural networks comprise of layers/modules that perform operations on data. The `torch.nn` namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the `nn.Module`. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\n\nTo define a neural network in PyTorch, we create a class that inherits from `nn.Module`. We define the layers of the network in the `__init__` function and specify how data will pass through the network in the `forward` function.\n\n::: {#10b85434 .cell execution_count=9}\n``` {.python .cell-code}\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n```\n:::\n\n\nNow that we've defined the structure of our `NeuralNetwork`, we can create an instance of it and move it to a faster device (GPU or MPS) if available to accelerate operations. We can also print its structure to see the layers that we've just defined.\n\n::: {#a04a753a .cell execution_count=10}\n``` {.python .cell-code}\nmodel = NeuralNetwork().to(device)\nprint(f\"Model structure: {model}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel structure: NeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n```\n:::\n:::\n\n\nMany layers inside a neural network are parameterized, meaning that they have associated weights and biases that are optimized during training. Subclassing `nn.Module` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s `parameters()` or `named_parameters()` methods.\n\nThe linear layer is a module that applies a linear transformation on the input using its stored weights and biases. Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n\nIn this model, we use `nn.ReLU` between our linear layers, but there are other options for activations to introduce non-linearity in your model.\n\n::: {#f0865109 .cell execution_count=11}\n``` {.python .cell-code}\nfor name, param in model.named_parameters():\n    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLayer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0069, -0.0090, -0.0053,  ...,  0.0014, -0.0055, -0.0313],\n        [-0.0027, -0.0257, -0.0303,  ..., -0.0329,  0.0320, -0.0336]],\n       device='mps:0', grad_fn=<SliceBackward0>) \n\nLayer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0255, -0.0069], device='mps:0', grad_fn=<SliceBackward0>) \n\nLayer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0210, -0.0186,  0.0411,  ...,  0.0346,  0.0432, -0.0231],\n        [-0.0199,  0.0335, -0.0396,  ..., -0.0416,  0.0382,  0.0423]],\n       device='mps:0', grad_fn=<SliceBackward0>) \n\nLayer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0328, -0.0148], device='mps:0', grad_fn=<SliceBackward0>) \n\nLayer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0231, -0.0346, -0.0262,  ..., -0.0030, -0.0107,  0.0297],\n        [ 0.0025, -0.0110, -0.0214,  ...,  0.0298, -0.0307,  0.0295]],\n       device='mps:0', grad_fn=<SliceBackward0>) \n\nLayer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0358,  0.0367], device='mps:0', grad_fn=<SliceBackward0>) \n\n```\n:::\n:::\n\n\n### Optimizing the Model Parameters\n\nNow that we have our data and our model, it’s time to train, validate and test our model by optimizing its parameters on the input data! Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters, and optimizes these parameters using gradient descent.\n\n#### Hyperparameters\n\nHyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates.\n\nWe define the following hyperparameters for training:\n- Number of Epochs - the number times to iterate over the dataset\n- Batch Size - the number of data samples propagated through the network before the parameters are updated\n- Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.\n\n::: {#4762a8a7 .cell execution_count=12}\n``` {.python .cell-code}\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5\n```\n:::\n\n\n#### Optimization Loop\n\nTo train our model, we need a loss function as well as an optimizer.\n\nOnce we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an epoch.\n\nEach epoch consists of two main parts:\n- The Train Loop - iterate over the training dataset and try to converge to optimal parameters.\n- The Validation/Test Loop - iterate over the test dataset to check if model performance is improving.\n\nThe loss function measures the degree of dissimilarity between an obtained result and the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\n\nCommon loss functions include `nn.MSELoss` (Mean Square Error) for regression tasks, and `nn.NLLLoss` (Negative Log Likelihood) for classification. `nn.CrossEntropyLoss` combines `nn.LogSoftmax` and `nn.NLLLoss`.\n\nWe pass our model’s output logits to `nn.CrossEntropyLoss`, which will normalize the logits and compute the prediction error.\n\n::: {#e0f2514b .cell execution_count=13}\n``` {.python .cell-code}\n# Initialize the loss function\nloss_fn = nn.CrossEntropyLoss()\n```\n:::\n\n\nOptimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer; additionally, there are many different optimizers available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.\n\nWe initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter.\n\n::: {#0c4c879d .cell execution_count=14}\n``` {.python .cell-code}\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n```\n:::\n\n\nInside the training loop, optimization happens in three steps:\n- Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n- Backpropagate the prediction loss with a call to `loss.backward()`. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n- Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass.\n\n#### Full Implementation\n\nWe define `train_loop` that loops over our optimization code, and `test_loop` that evaluates the model’s performance against our test data.\n\n::: {#5e10e012 .cell execution_count=15}\n``` {.python .cell-code}\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    # Set the model to training mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X = X.to(device)\n        y = y.to(device)\n        \n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * batch_size + len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\ndef test_loop(dataloader, model, loss_fn):\n    # Set the model to evaluation mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model.eval()\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n    with torch.no_grad():\n        for X, y in dataloader:\n            X = X.to(device)\n            y = y.to(device)\n        \n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n```\n:::\n\n\nIn a single training loop, the model makes predictions on the training dataset (fed to it in batches), and then backpropagates the prediction error to adjust the model’s parameters.\n\nWe can also check the model’s performance against the test dataset to ensure it is learning.\n\nThe training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch.\n\n::: {#ccaff7e7 .cell execution_count=16}\n``` {.python .cell-code}\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\nprint(\"Done!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1\n-------------------------------\nloss: 2.296714  [   64/60000]\nloss: 2.285919  [ 6464/60000]\nloss: 2.289640  [12864/60000]\nloss: 2.269527  [19264/60000]\nloss: 2.238294  [25664/60000]\nloss: 2.245018  [32064/60000]\nloss: 2.233091  [38464/60000]\nloss: 2.209641  [44864/60000]\nloss: 2.167873  [51264/60000]\nloss: 2.210138  [57664/60000]\nTest Error: \n Accuracy: 45.4%, Avg loss: 2.163287 \n\nEpoch 2\n-------------------------------\nloss: 2.148807  [   64/60000]\nloss: 2.146786  [ 6464/60000]\nloss: 2.100028  [12864/60000]\nloss: 2.086866  [19264/60000]\nloss: 2.088841  [25664/60000]\nloss: 2.022998  [32064/60000]\nloss: 2.008321  [38464/60000]\nloss: 1.980193  [44864/60000]\nloss: 1.942847  [51264/60000]\nloss: 1.935418  [57664/60000]\nTest Error: \n Accuracy: 47.7%, Avg loss: 1.898360 \n\nEpoch 3\n-------------------------------\nloss: 1.934336  [   64/60000]\nloss: 1.874416  [ 6464/60000]\nloss: 1.709932  [12864/60000]\nloss: 1.756551  [19264/60000]\nloss: 1.738622  [25664/60000]\nloss: 1.706300  [32064/60000]\nloss: 1.609682  [38464/60000]\nloss: 1.628336  [44864/60000]\nloss: 1.539030  [51264/60000]\nloss: 1.561772  [57664/60000]\nTest Error: \n Accuracy: 59.2%, Avg loss: 1.528465 \n\nEpoch 4\n-------------------------------\nloss: 1.439419  [   64/60000]\nloss: 1.322930  [ 6464/60000]\nloss: 1.461314  [12864/60000]\nloss: 1.465872  [19264/60000]\nloss: 1.336003  [25664/60000]\nloss: 1.346315  [32064/60000]\nloss: 1.377878  [38464/60000]\nloss: 1.287794  [44864/60000]\nloss: 1.289096  [51264/60000]\nloss: 1.287725  [57664/60000]\nTest Error: \n Accuracy: 62.3%, Avg loss: 1.254067 \n\nEpoch 5\n-------------------------------\nloss: 1.233157  [   64/60000]\nloss: 1.233771  [ 6464/60000]\nloss: 1.178213  [12864/60000]\nloss: 1.106026  [19264/60000]\nloss: 1.188458  [25664/60000]\nloss: 1.199469  [32064/60000]\nloss: 1.154457  [38464/60000]\nloss: 1.008805  [44864/60000]\nloss: 1.108838  [51264/60000]\nloss: 1.131137  [57664/60000]\nTest Error: \n Accuracy: 64.1%, Avg loss: 1.088662 \n\nDone!\n```\n:::\n:::\n\n\nWe can now use this model to make individual predictions.\n\nTo use the model, we pass it the input data. This executes the model’s `forward`, along with some background operations. Note that we do not call `model.forward()` directly!\n\n::: {#32e55b06 .cell execution_count=17}\n``` {.python .cell-code}\nclasses = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\nmodel.eval()\nx, y = test_data[0][0], test_data[0][1]\nwith torch.no_grad():\n    x = x.to(device)\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredicted: \"Ankle boot\", Actual: \"Ankle boot\"\n```\n:::\n:::\n\n\n# Summary\n\nThis concludes my walkthrough of using basic PyTorch to process data and train a neural network. As mentioned, in a future post in the coming weeks, I will iterate on this pipeline, demonstrating a different neural network architecture and more data exploration for a different dataset. Until next time, \\[VS\\]Coders!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}