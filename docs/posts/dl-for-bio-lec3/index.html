<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vivek Sriram">
<meta name="dcterms.date" content="2025-11-10">
<meta name="description" content="Deep Learning in Biology - Week 3">

<title>Convolutional Neural Networks (CNNs) – Home</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../vivek_cropped.JPG" rel="icon" type="image/jpeg">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-4174EVHDM6"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-4174EVHDM6', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Home</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">[VS]Codes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching.html"> 
<span class="menu-text">Teaching and Mentorship</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../conferences.html"> 
<span class="menu-text">Presentations and Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../publications.html"> 
<span class="menu-text">Publications and Peer Review</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../blog.xml"> <i class="bi bi-rss" role="img" aria-label="RSS">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Convolutional Neural Networks (CNNs)</h1>
                  <div>
        <div class="description">
          Deep Learning in Biology - Week 3
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Overviews</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Vivek Sriram </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 10, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">November 10, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#why-images-are-hard-for-standard-neural-networks" id="toc-why-images-are-hard-for-standard-neural-networks" class="nav-link active" data-scroll-target="#why-images-are-hard-for-standard-neural-networks">Why images are hard for standard neural networks</a></li>
  <li><a href="#preserving-spatial-relationships" id="toc-preserving-spatial-relationships" class="nav-link" data-scroll-target="#preserving-spatial-relationships">Preserving spatial relationships</a></li>
  <li><a href="#stability-under-transformations" id="toc-stability-under-transformations" class="nav-link" data-scroll-target="#stability-under-transformations">Stability under transformations</a></li>
  <li><a href="#cnns-as-hierarchical-feature-extractors" id="toc-cnns-as-hierarchical-feature-extractors" class="nav-link" data-scroll-target="#cnns-as-hierarchical-feature-extractors">CNNs as hierarchical feature extractors</a></li>
  <li><a href="#anatomy-of-a-cnn" id="toc-anatomy-of-a-cnn" class="nav-link" data-scroll-target="#anatomy-of-a-cnn">Anatomy of a CNN</a></li>
  <li><a href="#what-is-a-convolution" id="toc-what-is-a-convolution" class="nav-link" data-scroll-target="#what-is-a-convolution">What is a convolution?</a></li>
  <li><a href="#beyond-images-cnns-for-audio-and-text" id="toc-beyond-images-cnns-for-audio-and-text" class="nav-link" data-scroll-target="#beyond-images-cnns-for-audio-and-text">Beyond images: CNNs for audio and text</a></li>
  <li><a href="#iconic-cnn-architectures" id="toc-iconic-cnn-architectures" class="nav-link" data-scroll-target="#iconic-cnn-architectures">Iconic CNN architectures</a></li>
  <li><a href="#optimization-and-generalization" id="toc-optimization-and-generalization" class="nav-link" data-scroll-target="#optimization-and-generalization">Optimization and generalization</a></li>
  <li><a href="#looking-ahead" id="toc-looking-ahead" class="nav-link" data-scroll-target="#looking-ahead">Looking ahead</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Imaging data is everywhere, from radiology scans and pathology slides to self-driving cars and satellite imagery. To make sense of this visual information, deep learning relies on Convolutional Neural Networks (CNNs): specialized architectures designed to handle data with grid-like structures such as images.</p>
<p>Welcome to Week 3 of my <a href="https://bios740.github.io/about/">Deep Learning in Biology</a> self-study series! In today’s post, we’ll explore why CNNs are so effective, how they overcome key challenges of high-dimensional visual data, and how modern CNN architectures have evolved over time.</p>
<p><img src="lake.JPG" class="img-fluid"></p>
<section id="why-images-are-hard-for-standard-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="why-images-are-hard-for-standard-neural-networks">Why images are hard for standard neural networks</h2>
<p>Images are high-dimensional - every pixel is a separate feature, and the number of features grows quadratically with image resolution. For instance, a single 1000×1000 RGB image consists of three million input values!</p>
<p>Traditional fully connected networks don’t scale well in these scenarios. Each input pixel connects to every neuron in the next layer, leading to:</p>
<ul>
<li>Billions of learnable parameters</li>
<li>High memory and compute demands</li>
<li>Slow training</li>
<li>Risk of overfitting (too many parameters, not enough data)</li>
</ul>
<p>Convolutional Neural Networks (CNNs) solve these challenges by using local connections and shared weights to drastically reduce the number of parameters and improve efficiency.</p>
</section>
<section id="preserving-spatial-relationships" class="level2">
<h2 class="anchored" data-anchor-id="preserving-spatial-relationships">Preserving spatial relationships</h2>
<p>In an image, nearby pixels are often correlated with one another, forming edges, textures, and patterns. Fully connected networks ignore this structure, treating each pixel independently. CNNs, by contrast, respect spatial locality through the use of <strong>kernels</strong>.</p>
<p>Kernels can be thought of as filters that slide over small regions of the image. Each filter detects specific local features in the image (i.e.&nbsp;boundaries of a figure, edges, corner). These local patterns are combined in deeper layers to represent complex structures (such as faces or organ systems). This spatial awareness makes CNNs especially powerful for object detection and segmentation, where the arrangement of features matters.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Typical_cnn.png" class="img-fluid figure-img"></p>
<figcaption>A depiction of a typical CNN</figcaption>
</figure>
</div>
</section>
<section id="stability-under-transformations" class="level2">
<h2 class="anchored" data-anchor-id="stability-under-transformations">Stability under transformations</h2>
<p>In real-world settings, the same object can appear in many forms: rotated, scaled, flipped, or shifted. <strong>Translational invariance</strong> refers to the concept that a robust model should recognize an image regardless of any of these transformations. CNNs achieve translational invariance through:</p>
<ul>
<li><p>Shared filters that detect patterns no matter where they appear in the image.</p></li>
<li><p><strong>Pooling layers</strong> that summarize nearby activations, reducing sensitivity to position.</p></li>
<li><p>Data augmentation during the training process (i.e.&nbsp;random rotations, crops, flips) to teach invariance explicitly.</p></li>
</ul>
</section>
<section id="cnns-as-hierarchical-feature-extractors" class="level2">
<h2 class="anchored" data-anchor-id="cnns-as-hierarchical-feature-extractors">CNNs as hierarchical feature extractors</h2>
<p>Images contain information at multiple levels. <strong>Low-level</strong> features can be thought of as edges and corners, <strong>mid-level</strong> as textures and motifs, and <strong>high-level</strong> as specific objects and scenes. CNNs naturally build this hierarchy through <strong>stacked convolutional layers</strong>. Early layers detect simple shapes, while deeper layers capture more abstract concepts. This multiscale representation is a key reason CNNs generalize well to complex visual tasks.</p>
</section>
<section id="anatomy-of-a-cnn" class="level2">
<h2 class="anchored" data-anchor-id="anatomy-of-a-cnn">Anatomy of a CNN</h2>
<p>A standard CNN is composed of several types of layers:</p>
<ul>
<li><p><strong>Convolutional layers</strong> extract local features using learnable filters</p></li>
<li><p><strong>ReLU (Activation) layers</strong> introduce non-linearity for richer representations</p></li>
<li><p><strong>Pooling layers</strong> downsample and increase translation invariance</p></li>
<li><p><strong>Fully connected layers</strong> combine extracted features for final classification</p></li>
</ul>
<p>Additional parameters that can affect the behavior of a CNN include:</p>
<ul>
<li><p><strong>Padding</strong>: including pads of empty pixels around the ends of an image preserves image borders during convolution.</p></li>
<li><p><strong>Stride</strong>: manipulating the stride of a kernel controls how far across the image filters move per step, allowing for a balance between granularity and compute.</p></li>
<li><p><strong>Pooling Type:</strong> <em>Max pooling</em> merges features across a group of pixels, allowing for the emphasis of strong features. Meanwhile, <em>average pooling</em> takes the average of a set of pixels, smoothing outputs in the process. Max pooling is generally preferred for classification, while average pooling is better for tasks requiring contextual smoothness.</p></li>
</ul>
</section>
<section id="what-is-a-convolution" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-convolution">What is a convolution?</h2>
<p>Mathematically, a convolution makes use of two parameters: an input (i.e.&nbsp;an image), and a kernel (the filter for the image). These parameters are merged to produce an output (feature map) as follows:</p>
<p><span class="math inline">\((f*g)(i,j)=\Sigma_a \Sigma_b f(a, b) g(i-a, j-b)\)</span></p>
<p>In practice, CNNs use cross-correlation, which is similar but omits the kernel flip. Each kernel acts as a feature detector, highlighting areas of the image that match its pattern.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Convolutional_neural_network,_convolution_worked_example.png" class="img-fluid figure-img"></p>
<figcaption>An example of a convolution calculation</figcaption>
</figure>
</div>
</section>
<section id="beyond-images-cnns-for-audio-and-text" class="level2">
<h2 class="anchored" data-anchor-id="beyond-images-cnns-for-audio-and-text">Beyond images: CNNs for audio and text</h2>
<p>The concepts of locality and translation invariance applies to data types beyond images. Two other potential use cases include <strong>audio</strong> and <strong>text</strong>.</p>
<p>With audio, local patterns in the data may represent short bursts of frequencies or amplitude changes. CNNs can process these one-dimensional signals directly or make use of two-dimensional spectrograms that plot time against frequency.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Dial-up_sound_spectrogram.png" class="img-fluid figure-img"></p>
<figcaption>Audio spectrogram for a dial-up tone</figcaption>
</figure>
</div>
<p>Similarly for text, one-dimensional convolutions can be applied over word embeddings. CNNs can be effective in text for short-range dependencies - however, for longer sequences, we’ll want to turn to more advanced models like transformers.</p>
</section>
<section id="iconic-cnn-architectures" class="level2">
<h2 class="anchored" data-anchor-id="iconic-cnn-architectures">Iconic CNN architectures</h2>
<p>Let’s look at some key milestones that shaped CNN development:</p>
<ul>
<li><p>AlexNet (2012)</p>
<ul>
<li><p>5 convolutional + 3 fully connected layers.</p></li>
<li><p>Introduced ReLU, dropout, and local response normalization.</p></li>
<li><p>Sparked the deep learning revolution after winning ImageNet by a huge margin.</p></li>
</ul></li>
<li><p>VGG (2014)</p>
<ul>
<li><p>Deep and simple: small 3×3 filters stacked in depth.</p></li>
<li><p>Up to 19 layers, with ~140M parameters.</p></li>
<li><p>Still popular for transfer learning due to its clean architecture and pretrained weights.</p></li>
</ul></li>
<li><p>GoogleNet / Inception (2014)</p>
<ul>
<li><p>Used 1×1 convolutions to reduce dimensions of data before applying larger filters.</p></li>
<li><p>Inception modules combined multiple filter sizes in parallel.</p></li>
<li><p>22 layers deep but only 5M parameters, making it extremely efficient.</p></li>
</ul></li>
<li><p>ResNet (2015)</p>
<ul>
<li><p>Introduced residual connections, allowing the network to learn residuals instead of direct mappings.</p></li>
<li><p>Solved the vanishing gradient problem, enabling networks &gt;100 layers deep.</p></li>
</ul></li>
<li><p>Squeeze-and-Excitation (SE) Network</p>
<ul>
<li><p>Added attention to channel relationships.</p></li>
<li><p>Learns which feature maps are most important.</p></li>
</ul></li>
<li><p>MobileNet</p>
<ul>
<li><p>Designed for mobile devices.</p></li>
<li><p>Uses depthwise separable convolutions for efficiency.</p></li>
<li><p>Trades some accuracy for speed and compactness.</p></li>
</ul></li>
</ul>
</section>
<section id="optimization-and-generalization" class="level2">
<h2 class="anchored" data-anchor-id="optimization-and-generalization">Optimization and generalization</h2>
<p>Training deep CNNs is challenging - overfitting, slow convergence, and massive compute demands are common issues. Common remedies include:</p>
<ul>
<li><p><strong>Data Augmentation</strong></p>
<ul>
<li>Randomly modify training images (rotate, crop, flip, color-jitter) to simulate diversity.</li>
</ul></li>
<li><p><strong>Regularization</strong></p>
<ul>
<li><p>Penalize large weights:</p>
<ul>
<li><p>L1 (Lasso): Encourages sparsity</p></li>
<li><p>L2 (Weight Decay): Smooths solutions</p></li>
<li><p>Elastic Net: Combines both</p></li>
</ul></li>
</ul></li>
<li><p><strong>Dropout and Early Stopping</strong></p>
<ul>
<li><p>Dropout: Randomly deactivate neurons to prevent co-adaptation.</p></li>
<li><p>Early Stopping: Halt training when validation performance peaks.</p></li>
</ul></li>
<li><p><strong>Transfer Learning</strong></p>
<ul>
<li>Reuse pretrained CNNs (e.g., VGG, ResNet) and fine-tune on smaller, domain-specific datasets.</li>
</ul></li>
</ul>
</section>
<section id="looking-ahead" class="level2">
<h2 class="anchored" data-anchor-id="looking-ahead">Looking ahead</h2>
<p>CNNs have revolutionized visual understanding in the past decade, allowing for classification, object detection, and segmentation for a variety of imaging data and other data types. Nevertheless, obstacles remain, including:</p>
<ul>
<li><p>High memory demands and long training times.</p></li>
<li><p>Dependence on large labeled datasets.</p></li>
<li><p>Difficulty with small or low-resolution inputs.</p></li>
</ul>
<p>Despite these challenges, CNNs remain foundational to computer vision, and their core ideas of locality, hierarchy, and invariance continue to inspire modern architectures like transformers.</p>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<ul>
<li><p>CNNs reduce the parameters needed to represent a dataset via local connectivity and weight sharing.</p></li>
<li><p>CNNs can capture spatial hierarchies through stacked layers.</p></li>
<li><p>Pooling and data augmentation improve translational invariance.</p></li>
<li><p>Modern CNNs (VGG, ResNet, Inception) balance depth, efficiency, and accuracy.</p></li>
<li><p>CNNs can extend beyond imaging data to other modalities, including audio, text, and multiomics.</p></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/vsriram24\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="vsriram24/vsriram24.github.io" data-repo-id="PLACEHOLDER" data-category="Blog Comments" data-category-id="PLACEHOLDER" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
</div> <!-- /content -->




</body></html>