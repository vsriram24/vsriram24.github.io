[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vivek Sriram, PhD, MA",
    "section": "",
    "text": "Welcome to my website! I am a biomedical data scientist and health machine learning researcher based in Seattle, WA.\nMy general research skills and interests include:\n\ntranslational bioinformatics and personalized medicine\nbig data analysis\ndeep learning and generative AI\ndata visualization\nnetwork science\nhuman-centered design\n\nI am always open to speaking, teaching, collaboration, consulting, and outreach opportunities. Feel free to reach out to me with questions or requests at vivek.sriram@gmail.com."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I consider myself a computational scientist with a passion for people-focused projects. I currently work as a clinical data scientist at the Fred Hutchinson Cancer Center Data Science Lab (DaSL) in Seattle, WA. My role centers on the development and productization of statistical models and machine learning systems for electronic phenotyping and cohort identification across the institution. I also serve as a guest lecturer in the Department of Biostatistics, Epidemiology, and Informatics at the University of Pennsylvania and a collaborator with the Health Futures Group at Microsoft Research.\nPrior to my position at Fred Hutch, I completed my Ph.D. and postdoctoral training in Biomedical Informatics and Computational Genomics at the University of Pennsylvania Perelman School of Medicine. There, I worked with Dr. Dokyoon Kim’s Integrative ’Omics and Biomedical Informatics Lab, developing machine learning and graph-based methods to identify genetic contributors to disease comorbidities from large-scale multimodal biomedical data (You can listen to a recording of my public thesis defense here!)\nIn addition to my doctoral degree, I hold a M.A. in Statistics and Data Science from the Wharton School of Business, as well as a B.Sc. with honors in Statistics, a B.Sc. in Computer Science, and a minor in Computational Biology from Duke University."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "[VS]Codes",
    "section": "",
    "text": "Welcome to [VS]Codes, a biomedical data science and clinical informatics blog mixed with professional advice and miscellaneous detours, written by Vivek Sriram. To subscribe to my blog and get updates on my new posts directly in your inbox, click here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy doctoral research: the background\n\n\n\n\n\n\nPersonal\n\n\n\nContext is everything\n\n\n\n\n\nJun 10, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nBiomedical Data Science sub-disciplines (v0)\n\n\n\n\n\n\nOverviews\n\n\n\nWhat’s in a word?\n\n\n\n\n\nJun 3, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nCorrecting batch effects in single cell RNA-seq data with Monocle 3\n\n\n\n\n\n\nTutorials\n\n\n\nTakeaways from the SASC User Group Workshop #2\n\n\n\n\n\nMay 27, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nMy professional journey\n\n\n\n\n\n\nPersonal\n\n\n\nLife is a highway\n\n\n\n\n\nMay 15, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nHello world!\n\n\n\n\n\n\nPersonal\n\n\n\nThe start of a new chapter\n\n\n\n\n\nMay 14, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Sriram V§, Nam Y§, Shivakumar M, Verma A, Jung S-H, Lee SM*, Kim D*. A Network-Based Analysis of Disease Complication Associations for Obstetric Disorders in the UK Biobank. Journal of Personalized Medicine. 2021; 11(12):1382. https://doi.org/10.3390/jpm11121382\nNam Y§, Jung S-H§, Verma A, Sriram V, Wong H-H, Yun J-S, Kim D*. netCRS: Network-based comorbidity risk score for prediction of myocardial infarction using biobank-scaled PheWAS data. Pacific Symposium on Biocomputing 2022. https://doi.org/10.1142/9789811250477_0030\nSriram V§, Shivakumar M§, Jung S-H, Nam Y, Bang L, Verma A, Lee S, Choe EK, Kim D, NETMAGE: A human disease phenotype map generator for the network-based visualization of phenome-wide association study results, GigaScience, Volume 11, 2022, giac002, https://doi.org/10.1093/gigascience/giac002\nLee S§, Nam Y§, Choi ES, Jung YM, Sriram V, Leiby J, Koo JN, Oh IH, Kim BJ, Kim SM, Kim SY, Kim GM, Joo SK, Shin S, Norwitz E, Park CW, Jun JK, Kim W, Kim D*, Park JS*. Development of early prediction model for pregnancy-associated hypertension with graph-based semi-supervised learning. Scientific Reports, 12(1), 15793, 2022. https://doi.org/10.1038/s41598-022-15391-4\nNam Y§, Jung SH§, Yun JS, Verma A, Sriram V, Shin H, Won H*, Kim D*. Discovering comorbid diseases using an inter-disease interactivity network based on biobank-scale PheWAS data. Bioinformatics 39(1), 2023. https://doi.org/10.1093/bioinformatics/btac822\nWoerner J§, Sriram V§, Nam Y, Verma A, Kim D*. Uncovering genetic associations in the human diseasome using an endophenotype-augmented disease network. Bioinformatics 40(3), 2024. https://doi.org/10.1093/bioinformatics/btae126.\nSriram V, Conard AM, Rosenberg I, Kim D, Hall AK*. Accelerating precision medicine: a proposed framework for large-scale multiomic data integrity, interoperability, analysis, and collaboration in biomedical discovery. medRXiv. https://doi.org/10.1101/2024.03.15.24304358. Preprint.\nNam Y§, Sriram V§, Shivakumar M, Verma A, Yun JS, Kim D*. An enhanced disease network with robust cross-phenotype relationships via variant frequency-inverse phenotype frequency. Preprint.\nSriram V, Woerner J, Ahn YY*, Kim D*. The interplay of sex and genotype in disease associations: a comprehensive network analysis in the UK Biobank. Preprint."
  },
  {
    "objectID": "posts/240514_hello-world/index.html",
    "href": "posts/240514_hello-world/index.html",
    "title": "Hello world!",
    "section": "",
    "text": "Hello world, and welcome to my official blog! I’ve had numerous thoughts swirling around in my head over the past several years, but have had trouble formally committing to bringing them out into the world… however, encouragement from colleagues and recent reading (i.e. Dorie Clark’s The Long Game) have convinced me to finally bring my virtual pen to paper. In particular, I found myself tremendously inspired by the Fred Hutch DaSL Culture and Work Style document and the mantra of “ship as soon as you can.” The following graphic does a great job at reminding us how to avoid the pit of perfectionism!\n\nMy hopes for this blog are to (a) build a community, (b) share my knowledge, and (c) learn alongside my readers. I aim to write weekly (if not more frequent) posts on a variety of subjects, including:\n\nmy personal story\nadvice to others in my field\ntechnical tutorials\nsubject matter overviews, and\nmiscellaneous detours\n\nI have no expectations that my content will be perfect, but then again, learning from mistakes is one of the best parts of life :)\nGiven my background and expertise, my general focus will be on the world of biomedical data science and health technology, but the sky is the limit for the topics we may cover!\nThanks for tuning in - I’m delighted to have you along for the ride…"
  },
  {
    "objectID": "posts/240515_professional-journey/index.html",
    "href": "posts/240515_professional-journey/index.html",
    "title": "My professional journey",
    "section": "",
    "text": "For my first “real” post, I figured it would be a helpful exercise to go through my professional journey and track how I ended up where I currently am. This description is meant to be more of an overview, and I intend to provide more details on individual portions of it in the future.\n\nHaving grown up in the Silicon Valley, I always had a front-row seat to the power and potential of technology to improve people’s lives. All of the biggest tech companies had their headquarters within driving distance from my home, and every day, I could see how they had impacted not only myself but also everyone around me: computers, cellphones, social media, education, automobiles, entertainment… everything was shaped by information technology. Both of my parents were in the software industry too, and seeing the productive, fulfilling jobs that they were able to have made me certain that I wanted to involve technology in my future career. Learning how to code from my mother in high school made me feel like I was being imparted with some special kind of magic - entering the world of software engineering truly felt right at my fingertips.\nAt the same time, growing up in the Silicon Valley felt like growing up in a bubble. I yearned to explore the link between information technology and its downstream applications beyond my baseline understanding of how to write code. So, I sought out more. In high school, I took a breadth of science classes, and I found myself inspired by the concept of “interdisciplinarity”. Instead of being drawn to “information technology,” I was drawn closer and closer to the world of “information science” and its applications to multiple disciplines, including biology and medicine. My drive for interdisciplinary experiences and my desire to explore a world of technological applications outside of the Bay Area led me to Duke University for my undergraduate education - in fact, one of the mantras of the university was “creative thinking across intellectual boundaries”. At Duke, I completed double majors in Computer Science and Statistics. Duke’s affiliated medical campus also gave me chances to explore interdisciplinary applications in the world of biomedical informatics, and I pursued multiple research opportunities, including an Honor’s thesis for my Statistics degree under the supervision of Dr. Li Ma. I also completed a minor in Computational Biology, taking classes such as Computational Genomics with Dr. Alexander Hartemink and Computational Structural Biology with Dr. Bruce Donald. Lastly, I was lucky to have great summer internship mentors (including Li-Yuan Chern at Pharmacyclics and Drs. Zichen Wang and Avi Ma’ayan at the Icahn School of Medicine), who kept me motivated and inspired to stick to my path and pursue an advanced interdisciplinary career.\nBy the end of my undergrad, I knew that I wasn’t going to enter the traditional computer science recruitment cycle for software engineering roles - I instead applied to doctoral programs in biomedical informatics and computational biology that would allow me to build my knowledge base further and prepare me to become a leader in impactful projects that made a clear benefit in people’s lives. I was lucky to earn an admission with the Genomics and Computational Biology program at the University of Pennsylvania Perelman School of Medicine. Again, great mentors from my classes and research rotations (including Drs. Ryan Urbanowicz and Marylyn Ritchie to name a few) helped advance my training and made me a better researcher and scientist day by day. I am most indebted to my PhD advisor, Dr. Dokyoon Kim, for his support and mentorship throughout my PhD and subsequent post-doctoral position. With his leadership style and work ethic, he was a true role model for me throughout my graduate degree. I also fell in love with the combination of technical research and scientific storytelling that came out of my dissertation (to be discussed in a later post). During my PhD, I was able to complete a Master’s degree in Statistics and Data Science from the Wharton School of Business under the supervision of Dr. Anderson Zhang, as well as a summer internship as a User Experience Researcher in Health AI/ML under the supervision of Dr. Mandi Hall with the Health Futures team at Microsoft Research. All of these opportunities helped me to refine a set of motivators for my long-term career:\n\nimpact\nconnection\npassion\nleadership\n\nThese values aided me tremendously in my search for my first job upon the completion of my PhD. Today, I work as a clinical data scientist in the Translational Analytics and Informatics group at the Fred Hutchinson Cancer Center’s Data Science Lab (DaSL) in Seattle, WA (also to be discussed more in a future post). I am tremendously grateful to be a part of a supportive, driven community of fellow data scientists and researchers as we develop the clinical data infrastructure at Fred Hutch, and I look forward to sharing more about my work and career as the years progress!"
  },
  {
    "objectID": "posts/sasc-workshop2/index.html",
    "href": "posts/sasc-workshop2/index.html",
    "title": "Correcting batch effects in single cell RNA-seq data with Monocle 3",
    "section": "",
    "text": "In this week’s blog, I’ll be summarizing takeaways and a code example from the Seattle Area Single Cell (SASC) User Group’s second workshop of the year, which was held on May 16th, 2024. Slides from the workshop can be found here.\nThe SASC User Group, directed by Dr. Mary O’Neill at the Brotman Baty Institute, is designed to create connections and foster community among single-cell researchers at Fred Hutch, UW Medicine, Seattle Children’s, as well as other Seattle-area researchers working with single-cell data. If you are interested in joining the group, you can subscribe to their listserv here. The group holds quarterly meetings rotating across the three campuses, each with a different focus. May’s workshop was dedicated to applying batch correction methods using Monocle 3 to analyze single-cell RNA-seq (scRNA-seq) data.\n\nAll credit for the data and code in this workshop goes to Mary and the folks at the BBI who helped organize this community. I have simply summarized their content and added a few clarifiers in various sections! I claim no significant knowledge myself of working with single cell data - in the future, I hope to release a post that highlights some more of the biological context highlighted through this workshop. You can follow along with the original tutorial and code example at the SASC GitHub page here.\nAnd so, with context out of the way, let’s get started!\n\n\n1. Background\nMonocle 3 is “an analysis toolkit for single-cell RNA-seq data”, developed by the Trapnell Lab at the University of Washington Department of Genome Sciences.\nWhen analyzing any form of data, especially single cell data, it is important to keep the right sources of variation (see Aquino, Bisiaux, Li et al., Nature 2024). Batch effects refer to technical, non-biological factors that cause variation in data, and must be appropriately addressed to avoid confounding in results.\nThe best way to get around batch effects is to avoid introducing them in the first place! Nothing can salvage a poor study design. In a similar vein, it is important to determine whether or not there are actually batch effects in the first place that are influencing your data. Sometimes, batch corrections can introduce more artifacts than they alleviate. So, when applying batch correction methods, apply them thoughtfully. Know what they are doing, what to use them for, and where they can lead you astray.\nA variety of batch correction methods exist for scRNA-seq data (see Antonsson, Melsted, bioRXiv 2024). Generally, single cell analysis falls into two camps:\n\nBatch correction is only for visualization. The batch category is used as a covariate in downstream analysis\nBatch correction is incorporated into the data processing pipeline. Batch corrected data are used in downstream analysis.\n\nFor our code example, the data we are using represent a subset of heart data generated by the BBI after processing through Scale Biosciences’ and Parse Biosciences’ respective single-cell sequencing assays. The same samples are used in both assays - each sample had two different donors. Data were mixed together, and then genetic demultiplexing was performed.\nWe start off by calling our required packages for analysis. In this case, we are using R version 4.4.0. Refer to the following links (or the SASC GitHub page) for help with installing required packages:\n\nBioconductor Installation\nMonocle 3 Installation\nkBET Installation\nHarmony Installation\n\n\nsetwd(\"~/Documents/Developer/vsriram24.github.io/posts/sasc-workshop2\")\n\n#load required packages\nlibrary(monocle3)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(randomcoloR)\nlibrary(kBET)\nlibrary(cowplot)\nlibrary(harmony)\nlibrary(uwot)\nlibrary(batchelor)\n\n\n\n2. Reading in the data\nWith our environment ready to go, we start by reading in our input scRNA-seq data. This dataset is in the form of an S4 object, the standard format for representing sequencing data in Monocle 3. Other packages such as Seurat will have their own file formats to represent data.\n\n#Read in the cell data set containing a random sub-sampling of 35K barcodes from\nsasc &lt;- readRDS(\"BBI_heart_hs_mix_36601humangenes_35000barcodes.RDS\")\n\nWe can use the detect_genes function to count how many cells in our data are expressed above a minimum threshold.\n\nsasc &lt;- detect_genes(sasc)\n\nexpressed &lt;- data.frame(rowData(sasc)) %&gt;% \n  arrange(desc(num_cells_expressed))\n\nWe then use the n.umi attribute from the output of detect_genes to see how many unique molecular identifiers (UMIs) are in our data.\n\nsummary(sasc$n.umi)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    100     306     837    2084    2311  179927 \n\n\nLooking at the donor attribute of our data, and we see that there are numerous doublet (appearing for both donors) as well as unassigned (appearing for neither donor) UMIs in our data that may belong to either of our donors.\n\ntable(sasc$donor)\n\n\n         0          1    doublet unassigned \n     16410      14938        274       3378 \n\n\nWe are also able to see the breakdown of UMIs processed by our Parse and Scale scRNA-seq assays respectively.\n\ntable(sasc$batch)\n\n\nParse Scale \n16263 18737 \n\n\n\n\n3. Quality control\nNow that we have a breakdown of our data, we can perform quality control.\nWe start by calculating the mitochondrial DNA content in our scRNA-seq data. The presence of mitochondrial DNA (mtDNA) in our data represents low quality calls.\n\n# Search for genes with \"MT\" in their name.\nfData(sasc)$MT &lt;- grepl(\n  \"MT-\", \n  rowData(sasc)$gene_short_name\n)\n\ntable(fData(sasc)$MT)\n\n\nFALSE  TRUE \n36588    13 \n\n\nBased upon the mitochondrial DNA content we calculated, we can evaluate the percentage of mitochondrial reads in our data.\n\npData(sasc)$MT_reads &lt;- Matrix::colSums(exprs(sasc)[fData(sasc)$MT,])\npData(sasc)$MT_perc &lt;- pData(sasc)$MT_reads/Matrix::colSums(exprs(sasc))*100\n\nsummary(sasc$MT_perc)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.1303  0.9560  2.8839  3.2258 61.3861       1 \n\n\nWith our calculated mitochondrial percentages, we can start to visualize our data.\nLet’s look at a plot of genes by UMIs, colored by mitochondrial percentage.\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = n.umi, y = num_genes_expressed)) +\n  facet_wrap(~batch, nrow = 1) +\n  geom_point(size = 0.5, alpha = 0.3, aes(color = MT_perc)) +\n  theme_light() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14),\n        axis.title = element_text(size = 16),\n        axis.text.y = element_text(size = 14),\n        aspect.ratio = 1) +\n  scale_color_viridis() +\n  xlab(\"UMIs\") +\n  ylab(\"Number of Genes Captured\") +\n  scale_y_log10() +\n  scale_x_log10() +\n  geom_abline(slope = 1, color = \"grey\") +\n  geom_hline(yintercept = 200, linetype = \"dotted\", color = \"red\") +\n  geom_vline(xintercept = 300, linetype = \"dotted\", color = \"red\")\n\n\n\n\n\n\n\n\nWe can see that in spite of the fact that our two samples were identical, different mitochondrial content are exhibited across our assays. In particular, we see a lot of noise for the UMIs that correspond to under 200 genes.\nNow let’s look at mitochondrial percentage by donor type.\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = donor, y = MT_perc)) +\n  facet_wrap(~batch, nrow = 1, drop = TRUE, scales = \"free_x\") +\n  geom_violin(aes(fill = batch)) +\n  geom_boxplot(notch = T, fill = \"white\", width = 0.25, alpha = 0.3, outlier.shape = NA) +\n  theme_light() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14),\n        axis.title = element_text(size = 16),\n        axis.text.y = element_text(size = 14)) +\n  xlab(\"\") +\n  ylab(\"MT %\") +\n  theme(legend.position = \"none\")\n\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\n\n\n\n\n\n\n\n\n\nWe again see differences across our assays here in terms of MT percentage - in particular, it seems like we would want to have mitochondrial percentage no greater than 10% in our data.\nNow let’s look at UMIs by donor type.\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = donor, y = n.umi)) +\n  facet_wrap(~batch, nrow = 1, drop = TRUE, scales = \"free_x\") +\n  geom_violin(aes(fill = batch)) +\n  geom_boxplot(notch = T, fill = \"white\", width = 0.25, alpha = 0.3, outlier.shape = NA) +\n  theme_light() +\n  geom_hline(yintercept = 300, linetype = \"dashed\", color = \"blue\") + #change to thresholds\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14),\n        axis.title = element_text(size=16),\n        axis.text.y = element_text(size = 14)) +\n  scale_y_log10() +\n  xlab(\"\") +\n  ylab(\"UMIs\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nHere, it seems like data are hard to differentiate under a certain number of UMIs (in this case, 300).\nSo, let’s establish a QC filter for our data: 300 UMIs, 200 genes, and over 10% mitochondrial content.\n\n#Let's remove everything under 300 UMIs, 200 genes, and over 10% mitochondrial percentage\nsasc$qcflag &lt;- ifelse(\n  sasc$n.umi &gt;= 300 & \n  sasc$num_genes_expressed &gt;= 200 & \n  sasc$MT_perc &lt; 10, \n  \"PASS\", \n  \"FAIL\"\n)\n\ntable(sasc$qcflag)\n\n\n FAIL  PASS \n10180 24820 \n\nsasc &lt;- sasc[,sasc$qcflag == \"PASS\"] #filter out failing barcodes\nsasc\n\nclass: cell_data_set \ndim: 36601 24820 \nmetadata(1): cds_version\nassays(1): counts\nrownames(36601): ENSG00000243485 ENSG00000237613 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24820): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(12): barcode n.umi ... MT_perc qcflag\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nLet’s also remove UMIs that were mapped to the wrong genome, nonassignable to a donor, or deemed a doublet.\n\nsasc &lt;- sasc[,sasc$organism == \"human\" & sasc$donor %in% c(0, 1)]\nsasc\n\nclass: cell_data_set \ndim: 36601 24216 \nmetadata(1): cds_version\nassays(1): counts\nrownames(36601): ENSG00000243485 ENSG00000237613 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(12): barcode n.umi ... MT_perc qcflag\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nFinally, we have our semi-cleaned data!\n\ntable(sasc$batch)\n\n\nParse Scale \n11474 12742 \n\ntable(sasc$batch, sasc$donor)\n\n       \n           0    1 doublet unassigned\n  Parse 8637 2837       0          0\n  Scale 4506 8236       0          0\n\n\n\n\n4. Pre-processing and data visualization\nAfter completing quality control of our data, we can start to pre-process it.\n\n# remove non-expressed/non-captured genes \nhist(fData(sasc)$num_cells_expressed)\n\n\n\n\n\n\n\n\nWe can also conduct feature selection for genes if we want. This step is not necessary in every case, but it can help with reducing computational loads. Here, we filter out genes that are not expressed in at least 25 cells.\n\ntable(fData(sasc)$num_cells_expressed &gt; 25)\n\n\nFALSE  TRUE \n15315 21286 \n\n# filter out genes not expressed in at least 25 cells\nsasc &lt;- sasc[fData(sasc)$num_cells_expressed &gt; 25, ]\nsasc \n\nclass: cell_data_set \ndim: 21286 24216 \nmetadata(1): cds_version\nassays(1): counts\nrownames(21286): ENSG00000238009 ENSG00000241860 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(12): barcode n.umi ... MT_perc qcflag\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nNow we apply the estimate_size_factors function from Monocle 3 to evaluate the relative bias in each cell.\n\nsasc &lt;- estimate_size_factors(sasc)\n\nAfter estimating size factors, we can run preprocess_cds, a standardized workflow in the Monocle 3 package that normalizes the data by log and size factor to address depth differences, and then calculates a lower dimensional space that will be used as the input for further dimensionality reduction.\n\nset.seed(1000)\nsasc &lt;- preprocess_cds(sasc) #this may take a few minutes\n\nWe can then call reduce_dimensions from Monocle 3 on our data to get down to the most relevant components.\n\nset.seed(1000)\nsasc &lt;- reduce_dimension(sasc) #this may take a few minutes\n\nNo preprocess_method specified, using preprocess_method = 'PCA'\n\n\nNow let’s plot the cells in our data (first colored by batch and then by donor)\n\nplot_cells(sasc, color_cells_by = \"batch\")\n\nNo trajectory to plot. Has learn_graph() been called yet?\n\n\n\n\n\n\n\n\nplot_cells(sasc, color_cells_by = \"donor\")\n\nNo trajectory to plot. Has learn_graph() been called yet?\n\n\n\n\n\n\n\n\n\nWe now have our dimensionality-reduced data! Let’s use k-means clustering to categorize our cells into clusters.\nOne can (and should) spend a lot of time tweaking their clustering parameters. In this situation, we’ll go with a k of 40 for Leiden clustering. It is advisable to try several k-values and/or resolutions during this exploratory data analysis.\n\nsasc &lt;- cluster_cells(\n  sasc, \n  k=40, \n  cluster_method=\"leiden\", \n  random_seed=1000\n) #this may take a few minutes\n\n# add cluster information for each cell\ncolData(sasc)$k40_leiden_clusters = clusters(sasc) \n\nplot_cells(sasc)\n\n\n\n\n\n\n\n\nLet’s get the top marker genes based on our Leiden clustering:\n\ntop_marker_genes &lt;- top_markers(\n  sasc, \n  group_cells_by=\"k40_leiden_clusters\"\n)\n\nkeep &lt;- top_marker_genes %&gt;%\n  filter(fraction_expressing &gt;= 0.30) %&gt;%\n  group_by(cell_group) %&gt;%\n  top_n(3, marker_score) %&gt;%\n  pull(gene_short_name) %&gt;%\n  unique()\n\n\nplot_genes_by_group(\n  sasc,\n  c(keep),\n  group_cells_by = \"k40_leiden_clusters\", #\"partition\", \"cluster\"\n  ordering_type = \"maximal_on_diag\",\n  max.size = 3\n)\n\n\n\n\n\n\n\n\nLet’s do some more data visualization here. We’ll move out of Monocle 3 and into ggplot2 to improve our flexibility with plotting.\n\n#add UMAP coordinates to the colData for easy plotting\nsasc$UMAP1 &lt;- reducedDim(sasc, \"UMAP\")[,1]\nsasc$UMAP2 &lt;- reducedDim(sasc, \"UMAP\")[,2]\n\n\n#generate a distinguishable color scheme\nset.seed(1000)\ncolpal &lt;- randomcoloR::distinctColorPalette(k=12)\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x=UMAP1, y=UMAP2, color=cell_type)) +\n  facet_wrap(~batch+donor) +\n  geom_point(size=0.5, alpha=0.5) +\n  theme_bw() +\n  scale_color_manual(values=colpal) +\n  theme(legend.position=\"bottom\", aspect.ratio = 1, panel.grid=element_blank()) +\n  guides(color = guide_legend(override.aes = list(size=8, alpha=1)))\n\n\n\n\n\n\n\n\nFrom our visualizations, clusters 2, 3, 4, and 9 all appear to be related cell types - ventricular cardiomyocytes.\nWe can also see across our four plots that we have clear evidence of both technical (batch) and biological (donor) variation!\n\n\n5. Quantifying a batch effect\nNow that we have evidence of a batch effect, let’s quantify it! We’ll make use of the kBET (k-nearest neighbor batch effect test) package from the Theis lab.\n\n#kBET - k-nearest neighbour batch effect test\ndata &lt;- reducedDim(sasc)\nbatch &lt;- sasc$batch\n\nsubset_size &lt;- 0.1 #subsample to 10% of the data for speed\nsubset_id &lt;- sample.int(\n  n = length(batch), \n  size = floor(subset_size * length(batch)), \n  replace=FALSE\n)\n\nset.seed(1000)\nbatch.estimate &lt;- kBET(\n  data[subset_id,], \n  batch[subset_id]\n) #this may take a few minutes\n\n\n\n\n\n\n\n\n\nbatch.estimate$summary\n\n      kBET.expected kBET.observed kBET.signif\nmean    0.002482853     0.9888066           0\n2.5%    0.000000000     0.9753086           0\n50%     0.002743484     0.9917695           0\n97.5%   0.006207133     1.0000000           0\n\n\nBased on our rejection rate plot, it really does seem that we have a batch effect in our data. We can simulate a random batch assignment in our data and look at the same plot to convince ourselves of this observation.\n\nset.seed(1000)\nrandombatch &lt;- sample(sasc$batch, dim(sasc)[2])\nbatch.estimate.fake &lt;- kBET(\n  data[subset_id,], \n  randombatch[subset_id]\n) #this may take a few minutes\n\n\n\n\n\n\n\n\nAlright, so we clearly do have a batch effect!\n\n\n6. Batch correction\nSince we’ve proven that we have a batch effect in our data, let’s perform batch correction using Monocle 3. We make use of the align_cds function, a wrapper built around the reducedMNN function from the batchelor package developed by the Marioni Lab at the University of Cambridge.\n\n### Batch correction built-in to Monocle3\nset.seed(1000)\nbc_cds &lt;- align_cds(\n  sasc, \n  alignment_group = \"batch\", \n  k = 50\n) #this may take a minute\n\nAligning cells from different batches using Batchelor.\nPlease remember to cite:\n     Haghverdi L, Lun ATL, Morgan MD, Marioni JC (2018). 'Batch effects in single-cell RNA-sequencing data are corrected by matching mutual nearest neighbors.' Nat. Biotechnol., 36(5), 421-427. doi: 10.1038/nbt.4091\n\nbc_cds\n\nclass: cell_data_set \ndim: 21286 24216 \nmetadata(2): cds_version citations\nassays(1): counts\nrownames(21286): ENSG00000238009 ENSG00000241860 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(16): barcode n.umi ... UMAP1 UMAP2\nreducedDimNames(3): PCA UMAP Aligned\nmainExpName: NULL\naltExpNames(0):\n\n\nBased on our ‘aligned’ PCA, we can then call reduce_dimensions to generate a corresponding UMAP.\n\n# We can run reduce_dimensions to generate a UMAP from the 'aligned' PCA\nset.seed(1000)\nbc_cds &lt;- reduce_dimension(\n  bc_cds, \n  reduction_method = \"UMAP\", \n  preprocess_method = \"Aligned\"\n)\n\nsasc$aligned_UMAP1 &lt;- reducedDim(bc_cds, \"UMAP\")[,1] #save these in our original cds\nsasc$aligned_UMAP2 &lt;- reducedDim(bc_cds, \"UMAP\")[,2] #save these in our original cds\n\nplot_cells(bc_cds, color_cells_by = \"batch\")\n\nNo trajectory to plot. Has learn_graph() been called yet?\n\n\n\n\n\n\n\n\n\nFinally, we use kBET again to quantitatively verify that we have removed our batch effect from our data.\n\n#Use kBET to quantitatively ask if it removes the batch effect\ndata &lt;- reducedDim(bc_cds, \"UMAP\") #note that we are running this on the UMAP\nbatch &lt;- bc_cds$batch\nsubset_size &lt;- 0.1 #subsample to 10% of the data\nsubset_id &lt;- sample.int(\n  n = length(batch), \n  size = floor(subset_size * length(batch)), \n  replace = FALSE\n)\n\nset.seed(1000)\nbatch.estimate &lt;- kBET(\n  data[subset_id,], \n  batch[subset_id]\n) #this may take a few minutes\n\n\n\n\n\n\n\nbatch.estimate$summary\n\n      kBET.expected kBET.observed kBET.signif\nmean     0.02403292     0.8678189           0\n2.5%     0.01574074     0.8291152           0\n50%      0.02331962     0.8683128           0\n97.5%    0.03364198     0.9012346           0\n\n\nWhile the UMAP looks much better, the kBET metric is telling us that there is still a batch effect. If we run kBET on the ‘aligned’ PCs, the rejection rate is still close to 1. Is it possible we introduced artifacts through our batch correction? Let’s try the Harmony package from the Raychaudhuri Lab at Harvard and see if anything is different.\n\n### Batch correction with Harmony\nset.seed(1000)\nharm_cds &lt;- RunHarmony(sasc, 'batch') #this may take a few minutes\n\nTransposing data matrix\n\n\nInitializing state using k-means centroids initialization\n\n\nHarmony 1/10\n\n\nHarmony 2/10\n\n\nHarmony 3/10\n\n\nHarmony 4/10\n\n\nHarmony converged after 4 iterations\n\nharm_cds #note the \"HARMONY\" in reducedDim\n\nclass: cell_data_set \ndim: 21286 24216 \nmetadata(2): cds_version citations\nassays(1): counts\nrownames(21286): ENSG00000238009 ENSG00000241860 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(18): barcode n.umi ... aligned_UMAP1 aligned_UMAP2\nreducedDimNames(3): PCA UMAP HARMONY\nmainExpName: NULL\naltExpNames(0):\n\nsasc\n\nclass: cell_data_set \ndim: 21286 24216 \nmetadata(2): cds_version citations\nassays(1): counts\nrownames(21286): ENSG00000238009 ENSG00000241860 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(18): barcode n.umi ... aligned_UMAP1 aligned_UMAP2\nreducedDimNames(2): PCA UMAP\nmainExpName: NULL\naltExpNames(0):\n\n\n\n# Under the hood, Monocle 3 is using the uwot package to generate UMAPs\nharmony_umap &lt;- umap(reducedDim(harm_cds, \"HARMONY\"), seed=1000) #this may take a minute\nsasc$harmony_UMAP1 &lt;- harmony_umap[,1] #save these to our original cds\nsasc$harmony_UMAP2 &lt;- harmony_umap[,2] #save these to our original cds\n\n\n#Plot\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = harmony_UMAP1, y = harmony_UMAP2, color = batch)) +\n  geom_point(size=0.5, alpha=0.5) +\n  theme_bw() + \n  scale_color_viridis(discrete=T, begin=0.1, end=0.9, option=\"A\") +\n  theme(legend.position=\"bottom\", aspect.ratio = 1, panel.grid=element_blank()) +\n  guides(color = guide_legend(override.aes = list(size = 8, alpha = 1)))\n\n\n\n\n\n\n\n\n\n#Use kBET to quantitatively ask\ndata &lt;- harmony_umap #note, we could alternatively run kBET at the level of the corrected PCs \nbatch &lt;- harm_cds$batch\nsubset_size &lt;- 0.1 #subsample to 10% of the data\nsubset_id &lt;- sample.int(\n  n = length(batch),\n  size = floor(subset_size * length(batch)), \n  replace = FALSE)\n\nset.seed(1000)\nbatch.estimate &lt;- kBET(data[subset_id,], batch[subset_id]) #this may take a few minutes\n\n\n\n\n\n\n\nbatch.estimate$summary\n\n      kBET.expected kBET.observed kBET.signif\nmean    0.003004115     0.8504115           0\n2.5%    0.000000000     0.8106996           0\n50%     0.002743484     0.8518519           0\n97.5%   0.008230453     0.8973251           0\n\n\nOnce again, our UMAP looks better, but our kBET metric suggests that it is still far from perfect.\nSo what benefit does batch correction offer? This is debatable, but certainly one thing it can do is help in identifying cell types. In our case, our toy dataset already had annotated cell types, but if we didn’t know these ahead of time, batch correction could help us identify them.\n\n#cluster cells that have been aligned and plot these on our original UMAP\nbc_cds &lt;- cluster_cells(\n  bc_cds, \n  k = 40, \n  cluster_method = \"leiden\", \n  random_seed = 1000\n) #this may take a few minutes\n\n\nsasc$aligned_clusters &lt;- clusters(bc_cds) #save to original cds object\nsasc$aligned_partitions &lt;- partitions(bc_cds) #save to original cds object\n\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = UMAP1, y = UMAP2, color = k40_leiden_clusters)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  theme_bw() + \n  scale_color_manual(values = colpal) +\n  theme(legend.position = \"bottom\", aspect.ratio = 1, panel.grid = element_blank()) +\n  guides(color = guide_legend(override.aes = list(size = 8, alpha = 1)))\n\n\n\n\n\n\n\n\n\ndata.frame(colData(sasc)) %&gt;%\n  group_by(cell_type) %&gt;%\n  count(aligned_partitions) %&gt;%\n  spread(aligned_partitions, n)\n\n# A tibble: 12 × 6\n# Groups:   cell_type [12]\n   cell_type                   `1`   `2`   `3`   `4`   `5`\n   &lt;fct&gt;                     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 Adipocyte                     1    NA    NA    63    NA\n 2 Atrial Cardiomyocyte          5    NA    NA    NA    NA\n 3 Cytoplasmic Cardiomyocyte  1368    16     3     2     3\n 4 Endothelium                2309     9   238    NA    31\n 5 Fibroblast                 2666     6     4    NA     1\n 6 Lymphocyte                  192    NA    NA    NA    NA\n 7 Macrophage                 1752     4    11    NA    NA\n 8 Neuronal                    445     4     2    NA    NA\n 9 Pericyte                    280   880     2     1    NA\n10 Unknown                    2981    71    34    15    17\n11 Vascular Smooth Muscle      116    87     1    NA    NA\n12 Ventricular Cardiomyocyte 10556    15     1    23     1\n\n\nEven with our imperfect batch correction methods, our analysis has started to show us that some of our original clusters are related (ventricular cardiomyocytes).\n\n\n7. Differential gene expression\nWe are going to focus specifically on the ventricular cardiomyocytes for the rest of this code example. We start by subsetting our data down to ventricular cardiomyocytes that belong to clusters 2, 3, 4, and 9.\n\n#combine batch and donor as a new column \nsasc$id &lt;- paste(sasc$batch, sasc$donor, sep=\"_\")\n\n#subset ventricular cardiomyocyte data only \ncds_vent &lt;- sasc[,sasc$cell_type == \"Ventricular Cardiomyocyte\" &\n                   sasc$k40_leiden_clusters %in% c(\"2\", \"3\", \"4\", \"9\")]\n\nDifferential expression analysis can take a long time, so we will run the following code on a subset of pre-defined genes.\n\ngene_list &lt;- c(\"LINC00486\", \"TTN\", \"LINC-PINT\", \"TAS2R14\", \"MT-CO1\",\n               \"MT-ND4\", \"FN1\", \"LAMA2\", \"XIST\", \"PDK4\", \"ZBTB16\",\n               \"PPP1R3E\", \"TMTC1\", \"NT5DC3\", \"RBX1\", \"MRPL45\", \"ESR2\",\n               \"TUBGCP4\", \"MYH7\", \"MYL2\", \"MB\", \"ACTC1\", \"TPM1\", \"MYH6\")\n\ncds_subset &lt;- cds_vent[rowData(cds_vent)$gene_short_name %in% gene_list,]\n\nWe now plot expression levels of these genes, split by donor.\n\nplot_genes_violin(cds_subset, group_cells_by = \"donor\", ncol = 4) +\n  theme(axis.text.x=element_text(angle = 45, hjust = 1))\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\nWarning: Removed 108135 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 108135 rows containing non-finite outside the scale range\n(`stat_summary()`).\n\n\n\n\n\n\n\n\n\nIt is clear that some of our genes are differentially expressed across our two donors. How do we tell what contribution comes from donors and what comes from assay batch? Let’s build models for our data and compare.\nIn the donor model, we assume there are no batch effects and the only contributing variable is donor.\n\ndonor_model &lt;- fit_models(\n  cds_subset,\n  model_formula_str = \"~donor\",\n  expression_family=\"negbinomial\"\n)\n\n\ncoefficient_table(donor_model) %&gt;% \n  filter(term == \"donor1\") %&gt;%\n  filter(q_value &lt; 0.05) %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) %&gt;%\n  arrange(gene_short_name)\n\n# A tibble: 15 × 5\n   id              gene_short_name term     q_value estimate\n   &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 ENSG00000159251 ACTC1           donor1 5.46e-125   0.671 \n 2 ENSG00000115414 FN1             donor1 0           2.40  \n 3 ENSG00000196569 LAMA2           donor1 2.56e- 88  -0.245 \n 4 ENSG00000231721 LINC-PINT       donor1 1.67e-258  -1.91  \n 5 ENSG00000230876 LINC00486       donor1 4.42e-217   1.14  \n 6 ENSG00000198125 MB              donor1 1.38e- 55   0.463 \n 7 ENSG00000198804 MT-CO1          donor1 2.12e-214   1.23  \n 8 ENSG00000198886 MT-ND4          donor1 1.25e-170   1.13  \n 9 ENSG00000092054 MYH7            donor1 1.46e-107  -0.460 \n10 ENSG00000111245 MYL2            donor1 1.76e- 22   0.299 \n11 ENSG00000004799 PDK4            donor1 5.25e- 29  -0.165 \n12 ENSG00000212127 TAS2R14         donor1 6.18e-155  -3.80  \n13 ENSG00000155657 TTN             donor1 2.41e- 24  -0.0999\n14 ENSG00000229807 XIST            donor1 1.59e-167  -5.55  \n15 ENSG00000109906 ZBTB16          donor1 3.44e-178  -0.579 \n\n\nIn our second model, we include batch effects as a predictor variable.\n\n#controlling for batch effects\ndonor_batch_model &lt;- fit_models(\n  cds_subset,\n  model_formula_str = \"~donor + batch\",\n  expression_family=\"negbinomial\"\n)\n\n\ncoefficient_table(donor_batch_model) %&gt;% \n  filter(term == \"donor1\") %&gt;%\n  filter(q_value &lt; 0.05) %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) %&gt;%\n  arrange(gene_short_name)\n\n# A tibble: 15 × 5\n   id              gene_short_name term     q_value estimate\n   &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 ENSG00000159251 ACTC1           donor1 6.11e-  6   0.165 \n 2 ENSG00000115414 FN1             donor1 2.00e-200   2.37  \n 3 ENSG00000196569 LAMA2           donor1 3.44e- 72  -0.275 \n 4 ENSG00000231721 LINC-PINT       donor1 3.45e-  2   0.228 \n 5 ENSG00000198125 MB              donor1 4.23e-  2  -0.0950\n 6 ENSG00000198804 MT-CO1          donor1 4.17e- 43   0.605 \n 7 ENSG00000198886 MT-ND4          donor1 1.48e- 21   0.422 \n 8 ENSG00000197616 MYH6            donor1 6.76e- 23   0.303 \n 9 ENSG00000092054 MYH7            donor1 7.85e- 16  -0.215 \n10 ENSG00000111245 MYL2            donor1 1.12e- 13   0.299 \n11 ENSG00000004799 PDK4            donor1 5.56e-243  -0.534 \n12 ENSG00000212127 TAS2R14         donor1 9.17e-  3  -0.527 \n13 ENSG00000155657 TTN             donor1 2.00e- 36   0.142 \n14 ENSG00000229807 XIST            donor1 1.15e-167  -5.56  \n15 ENSG00000109906 ZBTB16          donor1 3.01e-  3  -0.0961\n\n\nComparing these two models, we can see that some of our genes are significantly influenced by the introduction of batch as a predictor.\n\n# Comparing models of gene expression\ncompare_models(donor_batch_model, donor_model) %&gt;% \n  select(gene_short_name, q_value) %&gt;% \n  data.frame()\n\n   gene_short_name       q_value\n1        LINC00486  0.000000e+00\n2              TTN 4.329415e-286\n3              FN1  1.000000e+00\n4            LAMA2  1.020045e-02\n5             PDK4  0.000000e+00\n6        LINC-PINT  0.000000e+00\n7           ZBTB16 6.655693e-187\n8          TAS2R14  0.000000e+00\n9            TMTC1  2.872632e-05\n10          NT5DC3  5.740679e-02\n11            MYL2  1.000000e+00\n12         PPP1R3E  4.636225e-01\n13            MYH6  1.999277e-62\n14            MYH7  5.726891e-61\n15            ESR2  2.311010e-02\n16           ACTC1 4.529192e-170\n17         TUBGCP4  5.352325e-01\n18            TPM1  9.684985e-01\n19          MRPL45  1.000000e+00\n20              MB 4.086623e-205\n21            RBX1  1.000000e+00\n22            XIST  1.000000e+00\n23          MT-CO1 7.757731e-279\n24          MT-ND4  0.000000e+00\n\n\nLet’s take a closer look at two example genes: LINC00486 and TTN, both of which have highly significant q-values (close to 0).\n\n# Get normalized counts\ncntmtx &lt;- normalized_counts(cds_subset)\ncds_subset$LINC00486 &lt;- cntmtx[\"ENSG00000230876\",]\ncds_subset$TTN &lt;- cntmtx[\"ENSG00000155657\",]\n\n\n#the case of LINC00486\na &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = donor, y = LINC00486)) + \n  geom_violin(aes(fill = donor)) +\n  geom_boxplot(width = 0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nb &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = batch, y = LINC00486)) + \n  geom_violin(aes(fill = \"salmon\")) +\n  geom_boxplot(width=0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nc &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = id, y = LINC00486, fill = donor)) + \n  geom_violin() +\n  geom_boxplot(width=0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nplot_grid(a + theme(legend.position = \"none\"),\n          b + theme(legend.position = \"none\") + ylab(\"\"),\n          c + theme(legend.position = \"none\") + ylab(\"\"), \n          labels=c(\"A\", \"B\", \"C\"),\n          nrow = 1)\n\n\n\n\n\n\n\n\nFrom our model, we can see that the batch variable was the primary contributor to differences in LINC00486 expression across our data.\n\ncoefficient_table(donor_model) %&gt;% \n  filter(gene_short_name == \"LINC00486\" & term ==\"donor1\") %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) \n\n# A tibble: 1 × 5\n  id              gene_short_name term     q_value estimate\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 ENSG00000230876 LINC00486       donor1 4.42e-217     1.14\n\ncoefficient_table(donor_batch_model) %&gt;% \n  filter(gene_short_name == \"LINC00486\" & term %in% c(\"donor1\", \"batchScale\")) %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) \n\n# A tibble: 2 × 5\n  id              gene_short_name term       q_value estimate\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 ENSG00000230876 LINC00486       donor1           1   0.0133\n2 ENSG00000230876 LINC00486       batchScale       0   3.64  \n\n\n\n#the case of TNN\nx &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = donor, y = TTN)) + \n  geom_violin(aes(fill = donor)) +\n  geom_boxplot(width = 0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\ny &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = batch, y = TTN)) + \n  geom_violin(aes(fill = \"salmon\")) +\n  geom_boxplot(width = 0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nz &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = id, y = TTN, fill = donor)) + \n  geom_violin() +\n  geom_boxplot(width = 0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nplot_grid(\n  x + theme(legend.position=\"none\"),\n  y + theme(legend.position=\"none\") + ylab(\"\"),\n  z + theme(legend.position=\"none\") + ylab(\"\"), \n  labels=c(\"A\", \"B\", \"C\"),\n  nrow = 1)\n\n\n\n\n\n\n\n\nOn the other hand, in the case of TTN, we see that both donor and batch affected its expression.\n\ncoefficient_table(donor_model) %&gt;% \n  filter(gene_short_name == \"TTN\" & term ==\"donor1\") %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) \n\n# A tibble: 1 × 5\n  id              gene_short_name term    q_value estimate\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 ENSG00000155657 TTN             donor1 2.41e-24  -0.0999\n\ncoefficient_table(donor_batch_model) %&gt;% \n  filter(gene_short_name == \"TTN\" & term %in% c(\"donor1\", \"batchScale\")) %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) \n\n# A tibble: 2 × 5\n  id              gene_short_name term        q_value estimate\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 ENSG00000155657 TTN             donor1     2.00e-36    0.142\n2 ENSG00000155657 TTN             batchScale 0          -0.405\n\n\n\n\n8. Key takeaways and suggestions\n\nDo not blindly apply batch correction! You risk introducing more artifacts than you remove.\nIn general, it is better to use batch correction methods for data exploration and visualization rather than analysis.\nIf you have a batch effect, model it in your downstream analysis (i.e include it as a covariate) instead of modifying your data directly.\n\n\n\n9. Conclusions\nThis concludes the content that was covered in the SASC User Group workshop on batch correction. I’d like to give a huge thank you again to Mary and the team at the BBI for organizing this tutorial and sharing it freely with the public! I look forward to covering more from these meetings in the future, as well as sharing my own thoughts and exploration of single cell data down the road."
  },
  {
    "objectID": "posts/terminologies/index.html",
    "href": "posts/terminologies/index.html",
    "title": "Biomedical Data Science sub-disciplines (v0)",
    "section": "",
    "text": "Welcome back to another week of [VS]Codes! In today’s post, I will be summarizing my personal definitions and interpretations of the various subdisciplines in biomedical data science. Given my prior background and experience in this subject area, I figured that writing this post would be an easy task! However, referencing the literature further made me realize that the boundaries across the subfields of health technology are fairly nebulous. Different departments, organizations, and individuals all have their own opinions of what their work involves. Dr. Bill Hersh from Oregon Health and Science University has a great paper (all the way back from 2009!) on the need to converge on consistent terminology in the field of biomedical informatics: you can give it a read here.\nGiven the lack of clarity in the subfields of biomedical data science, my goal today will be to provide my personal interpretation of the breakdown of relevant fields, informed by some of the definitions I’ve collated from external references. This description is absolutely meant to be a first draft of sorts (hence the v0 in the title)… I aim to refine these terms further based on additional input down the road."
  },
  {
    "objectID": "posts/terminologies/index.html#information-and-data",
    "href": "posts/terminologies/index.html#information-and-data",
    "title": "Biomedical Data Science sub-disciplines (v0)",
    "section": "Information and Data",
    "text": "Information and Data\nInformation Technology (IT)\nInformation technology (IT) refers to the use of computer systems to manage, process, protect, and exchange information. The overarching goal among its specializations is to use technology systems to solve problems and handle information. [5]\nInformation Science\nInformation science is the theoretical study of how information is created, organized, managed, stored, retrieved, and used. It is an interdisciplinary field that combines aspects of computer science and information management. [6]\nInformatics\nInformatics refers to the use and implementation of technology systems to analyze and manage information. Its primary focus is its application to specific external domains “for the good of people, organizations, and society”. [7, 8]\nData Science\nData science refers the study of data to extract meaningful insights and acquire knowledge. It is an interdisciplinary field that combines principles from a broad range of fields, including mathematics, statistics, artificial intelligence, and computer engineering, to collect, process, and analyze large amounts of data. There is a stronger focus on predictive modeling and algorithmic design. [9, 10, 11]\nData Analytics\nData analysis involves the analysis and examination of large amounts of data to better understand trends in the system being studied. There is a stronger focus on developing tables, visualizations, and overarching products to comprehend the data. [12]\nBased on the above definitions, I visualize the interplay of information and data as follows:\n\nInformation technology serves as the overarching guidebook for computational work. Within IT, information science dictates the way that we collect, store, organize, and manage our data. And finally, under both information technology and information science, data science, data analytics, and informatics intersect with one another, allowing for the development of predictive models and the analysis of data for applications to external domains."
  },
  {
    "objectID": "posts/terminologies/index.html#biomedical-informatics-and-computational-biology",
    "href": "posts/terminologies/index.html#biomedical-informatics-and-computational-biology",
    "title": "Biomedical Data Science sub-disciplines (v0)",
    "section": "Biomedical Informatics and Computational Biology",
    "text": "Biomedical Informatics and Computational Biology\nBiomedicine\nBiomedicine and the biomedical sciences refer to a set of sciences that apply understandings of biology and the natural sciences to develop knowledge, interventions, or technology that are of use in healthcare or public health. [13]\n\nBiomedical Informatics\nBiomedical Informatics\nBiomedical informatics is an interdisciplinary field seeking to study and advance the use of biomedical data to improve individual health, public health, and healthcare. It investigates, simulates, experiments with and translates a wide swath of biological systems to connect basic and clinical research with practical application for the overall betterment of healthcare. [17, 18, 19]\nBioinformatics\nBioinformatics, the application of biomedical informatics in cellular and molecular biology (often with a focus on genomics), is a scientific subdiscipline that involves using computer technology to collect, store, analyze, and disseminate biological data and information, such as DNA and amino acid sequences or annotations about those sequences, to increase our understanding of health and disease. [19, 20]\nTranslational Informatics\nTranslational informatics, the application of biomedical informatics to human health, is focused on the study and application of existing biomedical data to bridge new ways to improve diagnosis, staging, prognosis, and treatment of human disease. [19, 21]\nClinical Informatics\nClinical informatics, the subdiscipline of biomedical informatics related to patient data (typically from electronic medical records), focuses on the application of informatics to specific clinical subdisciplines, such as healthcare, nursing, dentistry, and pathology. [19, 22]\nThe subfields of biomedical informatics can be represented in the following hierarchy:\n\nBioinformatics is focused on processing the core data of biological systems, translational informatics is focused on the translation from biology to medicine, and clinical informatics is focused on the analysis of medical and healthcare data.\n\n\nComputational Biology\nComputational Biology\nComputational biology refers to the use of mathematics, statistics, and algorithms to understand biological systems based on data from experimental measurements. Examples of biological questions that may be tackled include what:\n\nbiological tasks are carried out by particular nucleic acid or peptide sequences\nwhich gene (or genes) when expressed produce a particular phenotype or behavior\nwhat sequence of changes in gene or protein expression or localization lead to a particular disease\nhow do changes in cell organization influence cell behavior. [23]\n\nComputational Genomics\nGenomics is a subfield of biomedicine focused on studying the entire set of DNA of an organism. Genomics research involves identifying and characterizing all the genes and functional elements in an organism’s genome as well as how they interact. [14, 15] Computational genomics refers to the use of computational and statistical analysis to decipher biology from genome sequences and related data, including both DNA and RNA sequence as well as other “post-genomic” data. [24]\nThe following figure shows some of the subfields of biology that can be addressed through computation. Computational genomics could be utilized in all three cases.\n\n\n\nDifferentiating biomedical informatics and computational biology\nBiomedical informatics and computational biology are very similar terms to one another - both involve the interdisciplinary application of information technology to biomedicine. The key differentiators I see between the two terms are the order of prioritization in disciplines and the nature of the data under consideration.\nIn biomedical informatics, the focus is on the development computational infrastructure and analysis to handle large-scale biomedical data.\nOn the other hand, computational biology starts with the focus on a biological experiment. From a biological question, corresponding data are generated, and computational analyses are applied.\nIndeed, I see the second word in each phrase as the “order of operations”: biomedical informatics is centered on the informatics, and computational biology is centered on the biology."
  },
  {
    "objectID": "posts/terminologies/index.html#health-technology-and-biotechnology",
    "href": "posts/terminologies/index.html#health-technology-and-biotechnology",
    "title": "Biomedical Data Science sub-disciplines (v0)",
    "section": "Health Technology and Biotechnology",
    "text": "Health Technology and Biotechnology\nHealth Technology\nHealth technology, or “health tech,” refers to the use of technologies developed for the purpose of improving any and all aspects of the healthcare system. It is focused primarily on the development of healthcare products and services. [25]\nHealth Information Technology\nHealth information technology (health IT) involves the processing, storage, and exchange of health information in an electronic environment. Applications include enhancing the quality of healthcare, preventing medical errors, reducing healthcare costs, and expanding access to healthcare. [26]\nBiotechnology\nBiotechnology (biotech) involves the use of living organisms and/or biological systems to develop or create different products [27].\nBioengineering\nBioengineering involves the application of engineering principles in combination with living organisms and/or biological systems to develop or create different products. These solutions may take the form of devices or computer programs (e.g., simulation of biomedical processes). However, the focus is on the biomedical problem to be solved, not data, information or knowledge. [28]\nDigital Health\nDigital health refers to the use of information and communications technologies in medicine and other health professions to manage illnesses and health risks and to promote wellness. [29]\nI see health technology and biotechnology as synonymous fields to one another, with health tech focused more on advancing human healthcare and biotech focused more on the use of biological systems. The incorporation of IT into health technology yields digital health and health IT, while the incorporation of engineering into biotechnology yields bioengineering."
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello world!",
    "section": "",
    "text": "Hello world, and welcome to my official blog! I’ve had numerous thoughts swirling around in my head over the past several years, but have had trouble formally committing to bringing them out into the world… however, encouragement from colleagues and recent reading (i.e. Dorie Clark’s The Long Game) have convinced me to finally bring my virtual pen to paper. In particular, I found myself tremendously inspired by the Fred Hutch DaSL Culture and Work Style document and the mantra of “ship as soon as you can.” The following graphic does a great job at reminding us how to avoid the pit of perfectionism!\n\nMy hopes for this blog are to (a) build a community, (b) share my knowledge, and (c) learn alongside my readers. I aim to write weekly (if not more frequent) posts on a variety of subjects, including:\n\nmy personal story\nadvice to others in my field\ntechnical tutorials\nsubject matter overviews, and\nmiscellaneous detours\n\nI have no expectations that my content will be perfect, but then again, learning from mistakes is one of the best parts of life :)\nGiven my background and expertise, my general focus will be on the world of biomedical data science and health technology, but the sky is the limit for the topics we may cover!\nThanks for tuning in - I’m delighted to have you along for the ride…"
  },
  {
    "objectID": "posts/professional-journey/index.html",
    "href": "posts/professional-journey/index.html",
    "title": "My professional journey",
    "section": "",
    "text": "For my first “real” post, I figured it would be a helpful exercise to go through my professional journey and track how I ended up where I currently am. This description is meant to be more of an overview, and I intend to provide more details on individual portions of it in the future.\n\nHaving grown up in the Silicon Valley, I always had a front-row seat to the power and potential of technology to improve people’s lives. All of the biggest tech companies had their headquarters within driving distance from my home, and every day, I could see how they had impacted not only myself but also everyone around me: computers, cellphones, social media, education, automobiles, entertainment… everything was shaped by information technology. Both of my parents were in the software industry too, and seeing the productive, fulfilling jobs that they were able to have made me certain that I wanted to involve technology in my future career. Learning how to code from my mother in high school made me feel like I was being imparted with some special kind of magic - entering the world of software engineering truly felt right at my fingertips.\nAt the same time, growing up in the Silicon Valley felt like growing up in a bubble. I yearned to explore the link between information technology and its downstream applications beyond my baseline understanding of how to write code. So, I sought out more. In high school, I took a breadth of science classes, and I found myself inspired by the concept of “interdisciplinarity”. Instead of being drawn to “information technology,” I was drawn closer and closer to the world of “information science” and its applications to multiple disciplines, including biology and medicine. My drive for interdisciplinary experiences and my desire to explore a world of technological applications outside of the Bay Area led me to Duke University for my undergraduate education - in fact, one of the mantras of the university was “creative thinking across intellectual boundaries”. At Duke, I completed double majors in Computer Science and Statistics. Duke’s affiliated medical campus also gave me chances to explore interdisciplinary applications in the world of biomedical informatics, and I pursued multiple research opportunities, including an Honor’s thesis for my Statistics degree under the supervision of Dr. Li Ma. I also completed a minor in Computational Biology, taking classes such as Computational Genomics with Dr. Alexander Hartemink and Computational Structural Biology with Dr. Bruce Donald. Lastly, I was lucky to have great summer internship mentors (including Li-Yuan Chern at Pharmacyclics and Drs. Zichen Wang and Avi Ma’ayan at the Icahn School of Medicine), who kept me motivated and inspired to stick to my path and pursue an advanced interdisciplinary career.\nBy the end of my undergrad, I knew that I wasn’t going to enter the traditional computer science recruitment cycle for software engineering roles - I instead applied to doctoral programs in biomedical informatics and computational biology that would allow me to build my knowledge base further and prepare me to become a leader in impactful projects that made a clear benefit in people’s lives. I was lucky to earn an admission with the Genomics and Computational Biology program at the University of Pennsylvania Perelman School of Medicine. Again, great mentors from my classes and research rotations (including Drs. Ryan Urbanowicz and Marylyn Ritchie to name a few) helped advance my training and made me a better researcher and scientist day by day. I am most indebted to my PhD advisor, Dr. Dokyoon Kim, for his support and mentorship throughout my PhD and subsequent post-doctoral position. With his leadership style and work ethic, he was a true role model for me throughout my graduate degree. I also fell in love with the combination of technical research and scientific storytelling that came out of my dissertation (to be discussed in a later post). During my PhD, I was able to complete a Master’s degree in Statistics and Data Science from the Wharton School of Business under the supervision of Dr. Anderson Zhang, as well as a summer internship as a User Experience Researcher in Health AI/ML under the supervision of Dr. Mandi Hall with the Health Futures team at Microsoft Research. All of these opportunities helped me to refine a set of motivators for my long-term career:\n\nimpact\nconnection\npassion\nleadership\n\nThese values aided me tremendously in my search for my first job upon the completion of my PhD. Today, I work as a clinical data scientist in the Translational Analytics and Informatics group at the Fred Hutchinson Cancer Center’s Data Science Lab (DaSL) in Seattle, WA (also to be discussed more in a future post). I am tremendously grateful to be a part of a supportive, driven community of fellow data scientists and researchers as we develop the clinical data infrastructure at Fred Hutch, and I look forward to sharing more about my work and career as the years progress!"
  },
  {
    "objectID": "posts/phd-context/index.html",
    "href": "posts/phd-context/index.html",
    "title": "My doctoral research: the background",
    "section": "",
    "text": "In the next few posts, I will be providing an overview of my PhD in Biomedical Informatics and Computational Genomics that I completed under the mentorship of Dr. Dokyoon Kim at the University of Pennsylvania Perelman School of Medicine. You can listen to a full presentation of my thesis defense here, and you can read the full text of my dissertation here. Note that all figures featured in this blog post were created using BioRender.com. Today’s post will focus on the background context that motivated my research. Without further ado, let’s get started!\n\n\nHuman biology is a complicated field. Millions of cells, cellular components, molecules, and chemicals interact with one another moment by moment to keep us alive each and every day. And while biologists and clinicians have defined a variety of classifications and ontologies to organize and understand these systems, the intricacies of the field continue to require extensive research and investment. Achieving a better understanding of human biology necessitates a unified language.\nAt the core of human biology and its defined ontologies lies the field of genetics. Genetics refers to the study of heritable traits (also known as phenotypes). If we define genetics as the language of biology, then the vocabulary of genetics would be the gene - a unit of heritable information that influences how we develop and operate. Extending the analogy further, the letters of genetics would be the nucleotide, a set of four molecules whose arrangements define different genes. Indeed, differences in individual nucleotides, also known as single-nucleotide variants/polymorphisms or “SNVs/SNPs”, are responsible for genetic variation across individuals and can lead to differing phenotypic outcomes. Genetic variation is a significant component of the diversity of life.\nGiven the complex interactions that occur across biomolecules in our bodies, it becomes apparent that networks of interacting genes drive our ability to live. Human diseases can be thought of as disruptions to these networks. The field of medicine aims to prevent, alleviate, and cure disease through the maintenance of health and the development of novel therapeutics.\nUnfortunately, for the most part, medicine today still operates from a “good enough” perspective - many patients are treated with the same medications without regard for or understanding of differences in their individual backgrounds or health profiles. Indeed, much more can be done to enhance the accuracy and efficacy of treatment.\nThe field of precision medicine uses large-scale multimodal/multiomic data to individualize patient care and gain a comprehensive understanding of human health. The goal of precision medicine is to achieve more accurate and precise disease prediction, prevention, treatment, and therapeutics. The field of genomics, involving the study of genetics from a “big data” lens, offers a significant opportunity to advance precision medicine research.\nGenomics refers to the study of an individual’s entire set of genes (a.k.a. their genome). We can work with genomic data from large-scale biomedical data, including both electronic health records (EHRs) and patient biobanks. EHRs, also known as electronic medical records (EMRs), refer to large clinical databases of patient medical history and clinical data. Biobanks, on the other hand, refer to biomedical databases with large quantities of patient biological samples, often including access to their genetic information. Combining these two data sources into a merged EHR-linked biobank provides an extra level of power in the study of genomics and medicine. EHR-linked biobanks offer the ability to identify and evaluate statistically significant genetic contributors to human disease. For instance, a genome-wide or phenome-wide association study (GWAS/PheWAS) applied to an EHR-linked biobank can identify associations between a variety of diseases and SNPs.\nMany research efforts in the field of precision medicine have used the results of PheWASs to identify genetic contributors to diseases. With such discoveries, patient genetic profiles can be built into diagnosis/treatment pipelines, allowing for the personalization of patient care.\nNotably, so far, most precision medicine research efforts that have made their way into the clinic have focused on one disease at a time. However, complex diseases rarely impact patients one-at-a-time. Shared SNPs and genes can contribute to the onset of multiple diseases in a single patient over time. These disease “multimorbidities” can lead to increase healthcare costs, health burdens, and risk of death. Thus, it becomes clear that we must evaluate the genetics of not only individual diseases but also cross-phenotype associations if we wish to gain a deeper understanding of overall patient health.\nGiven the significance of cross-phenotype associations, the field of “network medicine” offers a helpful framework to investigate the associations between diseases. Thus, the objective of my dissertation was to apply a “network medicine” approach to investigate genetic contributors to disease multimorbidities:\n\n\n\nFig 1. An overview of the process of using PheWAS results from an EHR-linked biobank for network medicine\n\n\nI broke this objective down into three chapters:\n\nCreation: construct and analyze a network of diseases derived from an EHR-linked biobank for the evaluation of genetic similarity between phenotypes\nComparison. generate and compare different disease networks generated from different populations and from genetic components.\nTranslation. extend the conclusions drawn from disease network analysis and comparison to downstream precision medicine applications.\n\n\n\n\nFig 2. The three sub-chapters of my PhD dissertation\n\n\nIn the coming week(s), I will go in-depth into the published manuscripts and preprints that correspond to these chapters, as well as my overall takeaways from my PhD research! Till next time~"
  }
]