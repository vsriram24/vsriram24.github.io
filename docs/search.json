[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vivek Sriram, PhD, MA",
    "section": "",
    "text": "Welcome to my website! I am a biomedical data scientist and health machine learning researcher based in Seattle, WA.\nMy general research skills and interests include:\n\ntranslational bioinformatics and personalized medicine\nbig data analysis and visualization\ndeep learning and generative AI\nnatural language processing\nnetwork science\nhuman-centered design\n\nI am always open to speaking, teaching, collaboration, consulting, and outreach opportunities. Feel free to reach out to me with questions or requests at vivek.sriram@gmail.com."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I consider myself a computational scientist with a passion for people-focused projects. I currently work as a clinical data scientist for the Office of the Chief Data Officer at the Fred Hutch Cancer Center in Seattle, WA. My role centers on the development and productization of statistical models and machine learning systems for electronic phenotyping and cohort identification across the institution. I also serve as a guest lecturer in the Department of Biostatistics, Epidemiology, and Informatics at the University of Pennsylvania and a collaborator with the Health Futures Group at Microsoft Research.\nPrior to my position at Fred Hutch, I completed my Ph.D. and postdoctoral training in Biomedical Informatics and Computational Genomics at the University of Pennsylvania Perelman School of Medicine. There, I worked with Dr. Dokyoon Kim’s Integrative ’Omics and Biomedical Informatics Lab, developing machine learning and graph-based methods to identify genetic contributors to disease comorbidities from large-scale multimodal biomedical data (You can listen to a recording of my public thesis defense here!)\nIn addition to my doctoral degree, I hold a M.A. in Statistics and Data Science from the Wharton School of Business, as well as a B.Sc. with honors in Statistics, a B.Sc. in Computer Science, and a minor in Computational Biology from Duke University."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "[VS]Codes",
    "section": "",
    "text": "Welcome to [VS]Codes, a biomedical data science and clinical informatics blog mixed with professional advice and miscellaneous detours, written by Vivek Sriram. To subscribe to my blog and get updates on my new posts directly in your inbox, click here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn overview of learning health systems\n\n\n\n\n\n\nOverviews\n\n\n\nA data-driven paradigm for healthcare\n\n\n\n\n\nNov 18, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nWhy I think the pursuit of “work-life balance” is overrated\n\n\n\n\n\n\nAdvice\n\n\nMiscellaneous\n\n\nPersonal\n\n\n\nReframing a professional paradigm\n\n\n\n\n\nNov 4, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nMy blog / newsletter recommendations\n\n\n\n\n\n\nPersonal\n\n\n\nRead up!\n\n\n\n\n\nSep 16, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nAn intro to Convolutional Neural Networks\n\n\n\n\n\n\nOverviews\n\n\n\nA summary from Fei-Fei Li’s Stanford CS231n: ‘Deep Learning for Computer Vision’ course\n\n\n\n\n\nSep 9, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nposit::conf 2024\n\n\n\n\n\n\nConferences\n\n\n\nA few talk summaries and my personal takeaways\n\n\n\n\n\nSep 2, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nAn intro to NLP for oncology\n\n\n\n\n\n\nOverviews\n\n\n\nA summary of Yim et al’s JAMA Oncology paper ‘Natural Language Processing in Oncology: A Review’\n\n\n\n\n\nAug 19, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nFactor analysis for multiomics data with MOFA2\n\n\n\n\n\n\nOverviews\n\n\nTutorials\n\n\n\nSome background information and a tutorial by Alex Gurbych from blackthorn.ai\n\n\n\n\n\nAug 12, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nObliteride 2024\n\n\n\n\n\n\nMiscellaneous\n\n\n\nPutting cancer behind us\n\n\n\n\n\nAug 5, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to PyTorch\n\n\n\n\n\n\nTutorials\n\n\n\nUsing basic PyTorch to process data and train a neural network\n\n\n\n\n\nJul 29, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nCascadia R Conference 2024\n\n\n\n\n\n\nConferences\n\n\n\nA few talk summaries and my personal takeaways\n\n\n\n\n\nJul 22, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nMy podcast recommendations\n\n\n\n\n\n\nPersonal\n\n\n\nListen up!\n\n\n\n\n\nJul 15, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nA Python introduction to neural networks and backpropagation\n\n\n\n\n\n\nTutorials\n\n\n\nAndrej Karpathy’s ‘Neural Networks: Zero to Hero’ video #1\n\n\n\n\n\nJul 8, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nMy doctoral research: takeaways and advice\n\n\n\n\n\n\nPersonal\n\n\nAdvice\n\n\n\nWe’re all in this together\n\n\n\n\n\nJun 24, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nMy doctoral research: the content\n\n\n\n\n\n\nPersonal\n\n\nOverviews\n\n\n\nConte(n)t is everything\n\n\n\n\n\nJun 17, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nMy doctoral research: the background\n\n\n\n\n\n\nPersonal\n\n\nOverviews\n\n\n\nContext is everything\n\n\n\n\n\nJun 10, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nBiomedical Data Science sub-disciplines (v0)\n\n\n\n\n\n\nOverviews\n\n\n\nWhat’s in a word?\n\n\n\n\n\nJun 3, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nCorrecting batch effects in single cell RNA-seq data with Monocle 3\n\n\n\n\n\n\nTutorials\n\n\n\nTakeaways from the SASC User Group Workshop #2\n\n\n\n\n\nMay 27, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nMy professional journey\n\n\n\n\n\n\nPersonal\n\n\n\nLife is a highway\n\n\n\n\n\nMay 15, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nHello world!\n\n\n\n\n\n\nPersonal\n\n\n\nThe start of a new chapter\n\n\n\n\n\nMay 14, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Sriram V§, Nam Y§, Shivakumar M, Verma A, Jung S-H, Lee SM*, Kim D*. A Network-Based Analysis of Disease Complication Associations for Obstetric Disorders in the UK Biobank. Journal of Personalized Medicine. 2021; 11(12):1382. https://doi.org/10.3390/jpm11121382\nNam Y§, Jung S-H§, Verma A, Sriram V, Wong H-H, Yun J-S, Kim D*. netCRS: Network-based comorbidity risk score for prediction of myocardial infarction using biobank-scaled PheWAS data. Pacific Symposium on Biocomputing 2022. https://doi.org/10.1142/9789811250477_0030\nSriram V§, Shivakumar M§, Jung S-H, Nam Y, Bang L, Verma A, Lee S, Choe EK, Kim D, NETMAGE: A human disease phenotype map generator for the network-based visualization of phenome-wide association study results, GigaScience, Volume 11, 2022, giac002, https://doi.org/10.1093/gigascience/giac002\nLee S§, Nam Y§, Choi ES, Jung YM, Sriram V, Leiby J, Koo JN, Oh IH, Kim BJ, Kim SM, Kim SY, Kim GM, Joo SK, Shin S, Norwitz E, Park CW, Jun JK, Kim W, Kim D*, Park JS*. Development of early prediction model for pregnancy-associated hypertension with graph-based semi-supervised learning. Scientific Reports, 12(1), 15793, 2022. https://doi.org/10.1038/s41598-022-15391-4\nNam Y§, Jung SH§, Yun JS, Verma A, Sriram V, Shin H, Won H*, Kim D*. Discovering comorbid diseases using an inter-disease interactivity network based on biobank-scale PheWAS data. Bioinformatics 39(1), 2023. https://doi.org/10.1093/bioinformatics/btac822\nWoerner J§, Sriram V§, Nam Y, Verma A, Kim D*. Uncovering genetic associations in the human diseasome using an endophenotype-augmented disease network. Bioinformatics 40(3), 2024. https://doi.org/10.1093/bioinformatics/btae126.\nSriram V, Conard AM, Rosenberg I, Kim D, Hall AK*. Accelerating precision medicine: a proposed framework for large-scale multiomic data integrity, interoperability, analysis, and collaboration in biomedical discovery. medRXiv. https://doi.org/10.1101/2024.03.15.24304358. Preprint.\nNam Y§, Sriram V§, Shivakumar M, Verma A, Yun JS, Kim D*. An enhanced disease network with robust cross-phenotype relationships via variant frequency-inverse phenotype frequency. Preprint.\nSriram V, Woerner J, Ahn YY*, Kim D*. The interplay of sex and genotype in disease associations: a comprehensive network analysis in the UK Biobank. Preprint."
  },
  {
    "objectID": "posts/240514_hello-world/index.html",
    "href": "posts/240514_hello-world/index.html",
    "title": "Hello world!",
    "section": "",
    "text": "Hello world, and welcome to my official blog! I’ve had numerous thoughts swirling around in my head over the past several years, but have had trouble formally committing to bringing them out into the world… however, encouragement from colleagues and recent reading (i.e. Dorie Clark’s The Long Game) have convinced me to finally bring my virtual pen to paper. In particular, I found myself tremendously inspired by the Fred Hutch DaSL Culture and Work Style document and the mantra of “ship as soon as you can.” The following graphic does a great job at reminding us how to avoid the pit of perfectionism!\n\nMy hopes for this blog are to (a) build a community, (b) share my knowledge, and (c) learn alongside my readers. I aim to write weekly (if not more frequent) posts on a variety of subjects, including:\n\nmy personal story\nadvice to others in my field\ntechnical tutorials\nsubject matter overviews, and\nmiscellaneous detours\n\nI have no expectations that my content will be perfect, but then again, learning from mistakes is one of the best parts of life :)\nGiven my background and expertise, my general focus will be on the world of biomedical data science and health technology, but the sky is the limit for the topics we may cover!\nThanks for tuning in - I’m delighted to have you along for the ride…"
  },
  {
    "objectID": "posts/240515_professional-journey/index.html",
    "href": "posts/240515_professional-journey/index.html",
    "title": "My professional journey",
    "section": "",
    "text": "For my first “real” post, I figured it would be a helpful exercise to go through my professional journey and track how I ended up where I currently am. This description is meant to be more of an overview, and I intend to provide more details on individual portions of it in the future.\n\nHaving grown up in the Silicon Valley, I always had a front-row seat to the power and potential of technology to improve people’s lives. All of the biggest tech companies had their headquarters within driving distance from my home, and every day, I could see how they had impacted not only myself but also everyone around me: computers, cellphones, social media, education, automobiles, entertainment… everything was shaped by information technology. Both of my parents were in the software industry too, and seeing the productive, fulfilling jobs that they were able to have made me certain that I wanted to involve technology in my future career. Learning how to code from my mother in high school made me feel like I was being imparted with some special kind of magic - entering the world of software engineering truly felt right at my fingertips.\nAt the same time, growing up in the Silicon Valley felt like growing up in a bubble. I yearned to explore the link between information technology and its downstream applications beyond my baseline understanding of how to write code. So, I sought out more. In high school, I took a breadth of science classes, and I found myself inspired by the concept of “interdisciplinarity”. Instead of being drawn to “information technology,” I was drawn closer and closer to the world of “information science” and its applications to multiple disciplines, including biology and medicine. My drive for interdisciplinary experiences and my desire to explore a world of technological applications outside of the Bay Area led me to Duke University for my undergraduate education - in fact, one of the mantras of the university was “creative thinking across intellectual boundaries”. At Duke, I completed double majors in Computer Science and Statistics. Duke’s affiliated medical campus also gave me chances to explore interdisciplinary applications in the world of biomedical informatics, and I pursued multiple research opportunities, including an Honor’s thesis for my Statistics degree under the supervision of Dr. Li Ma. I also completed a minor in Computational Biology, taking classes such as Computational Genomics with Dr. Alexander Hartemink and Computational Structural Biology with Dr. Bruce Donald. Lastly, I was lucky to have great summer internship mentors (including Li-Yuan Chern at Pharmacyclics and Drs. Zichen Wang and Avi Ma’ayan at the Icahn School of Medicine), who kept me motivated and inspired to stick to my path and pursue an advanced interdisciplinary career.\nBy the end of my undergrad, I knew that I wasn’t going to enter the traditional computer science recruitment cycle for software engineering roles - I instead applied to doctoral programs in biomedical informatics and computational biology that would allow me to build my knowledge base further and prepare me to become a leader in impactful projects that made a clear benefit in people’s lives. I was lucky to earn an admission with the Genomics and Computational Biology program at the University of Pennsylvania Perelman School of Medicine. Again, great mentors from my classes and research rotations (including Drs. Ryan Urbanowicz and Marylyn Ritchie to name a few) helped advance my training and made me a better researcher and scientist day by day. I am most indebted to my PhD advisor, Dr. Dokyoon Kim, for his support and mentorship throughout my PhD and subsequent post-doctoral position. With his leadership style and work ethic, he was a true role model for me throughout my graduate degree. I also fell in love with the combination of technical research and scientific storytelling that came out of my dissertation (to be discussed in a later post). During my PhD, I was able to complete a Master’s degree in Statistics and Data Science from the Wharton School of Business under the supervision of Dr. Anderson Zhang, as well as a summer internship as a User Experience Researcher in Health AI/ML under the supervision of Dr. Mandi Hall with the Health Futures team at Microsoft Research. All of these opportunities helped me to refine a set of motivators for my long-term career:\n\nimpact\nconnection\npassion\nleadership\n\nThese values aided me tremendously in my search for my first job upon the completion of my PhD. Today, I work as a clinical data scientist in the Translational Analytics and Informatics group at the Fred Hutchinson Cancer Center’s Data Science Lab (DaSL) in Seattle, WA (also to be discussed more in a future post). I am tremendously grateful to be a part of a supportive, driven community of fellow data scientists and researchers as we develop the clinical data infrastructure at Fred Hutch, and I look forward to sharing more about my work and career as the years progress!"
  },
  {
    "objectID": "posts/sasc-workshop2/index.html",
    "href": "posts/sasc-workshop2/index.html",
    "title": "Correcting batch effects in single cell RNA-seq data with Monocle 3",
    "section": "",
    "text": "In this week’s blog, I’ll be summarizing takeaways and a code example from the Seattle Area Single Cell (SASC) User Group’s second workshop of the year, which was held on May 16th, 2024. Slides from the workshop can be found here.\nThe SASC User Group, directed by Dr. Mary O’Neill at the Brotman Baty Institute, is designed to create connections and foster community among single-cell researchers at Fred Hutch, UW Medicine, Seattle Children’s, as well as other Seattle-area researchers working with single-cell data. If you are interested in joining the group, you can subscribe to their listserv here. The group holds quarterly meetings rotating across the three campuses, each with a different focus. May’s workshop was dedicated to applying batch correction methods using Monocle 3 to analyze single-cell RNA-seq (scRNA-seq) data.\n\nAll credit for the data and code in this workshop goes to Mary and the folks at the BBI who helped organize this community. I have simply summarized their content and added a few clarifiers in various sections! I claim no significant knowledge myself of working with single cell data - in the future, I hope to release a post that highlights some more of the biological context highlighted through this workshop. You can follow along with the original tutorial and code example at the SASC GitHub page here.\nAnd so, with context out of the way, let’s get started!\n\n\n1. Background\nMonocle 3 is “an analysis toolkit for single-cell RNA-seq data”, developed by the Trapnell Lab at the University of Washington Department of Genome Sciences.\nWhen analyzing any form of data, especially single cell data, it is important to keep the right sources of variation (see Aquino, Bisiaux, Li et al., Nature 2024). Batch effects refer to technical, non-biological factors that cause variation in data, and must be appropriately addressed to avoid confounding in results.\nThe best way to get around batch effects is to avoid introducing them in the first place! Nothing can salvage a poor study design. In a similar vein, it is important to determine whether or not there are actually batch effects in the first place that are influencing your data. Sometimes, batch corrections can introduce more artifacts than they alleviate. So, when applying batch correction methods, apply them thoughtfully. Know what they are doing, what to use them for, and where they can lead you astray.\nA variety of batch correction methods exist for scRNA-seq data (see Antonsson, Melsted, bioRXiv 2024). Generally, single cell analysis falls into two camps:\n\nBatch correction is only for visualization. The batch category is used as a covariate in downstream analysis\nBatch correction is incorporated into the data processing pipeline. Batch corrected data are used in downstream analysis.\n\nFor our code example, the data we are using represent a subset of heart data generated by the BBI after processing through Scale Biosciences’ and Parse Biosciences’ respective single-cell sequencing assays. The same samples are used in both assays - each sample had two different donors. Data were mixed together, and then genetic demultiplexing was performed.\nWe start off by calling our required packages for analysis. In this case, we are using R version 4.4.0. Refer to the following links (or the SASC GitHub page) for help with installing required packages:\n\nBioconductor Installation\nMonocle 3 Installation\nkBET Installation\nHarmony Installation\n\n\nsetwd(\"~/Documents/Developer/vsriram24.github.io/posts/sasc-workshop2\")\n\n#load required packages\nlibrary(monocle3)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(randomcoloR)\nlibrary(kBET)\nlibrary(cowplot)\nlibrary(harmony)\nlibrary(uwot)\nlibrary(batchelor)\n\n\n\n2. Reading in the data\nWith our environment ready to go, we start by reading in our input scRNA-seq data. This dataset is in the form of an S4 object, the standard format for representing sequencing data in Monocle 3. Other packages such as Seurat will have their own file formats to represent data.\n\n#Read in the cell data set containing a random sub-sampling of 35K barcodes from\nsasc &lt;- readRDS(\"BBI_heart_hs_mix_36601humangenes_35000barcodes.RDS\")\n\nWe can use the detect_genes function to count how many cells in our data are expressed above a minimum threshold.\n\nsasc &lt;- detect_genes(sasc)\n\nexpressed &lt;- data.frame(rowData(sasc)) %&gt;% \n  arrange(desc(num_cells_expressed))\n\nWe then use the n.umi attribute from the output of detect_genes to see how many unique molecular identifiers (UMIs) are in our data.\n\nsummary(sasc$n.umi)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    100     306     837    2084    2311  179927 \n\n\nLooking at the donor attribute of our data, and we see that there are numerous doublet (appearing for both donors) as well as unassigned (appearing for neither donor) UMIs in our data that may belong to either of our donors.\n\ntable(sasc$donor)\n\n\n         0          1    doublet unassigned \n     16410      14938        274       3378 \n\n\nWe are also able to see the breakdown of UMIs processed by our Parse and Scale scRNA-seq assays respectively.\n\ntable(sasc$batch)\n\n\nParse Scale \n16263 18737 \n\n\n\n\n3. Quality control\nNow that we have a breakdown of our data, we can perform quality control.\nWe start by calculating the mitochondrial DNA content in our scRNA-seq data. The presence of mitochondrial DNA (mtDNA) in our data represents low quality calls.\n\n# Search for genes with \"MT\" in their name.\nfData(sasc)$MT &lt;- grepl(\n  \"MT-\", \n  rowData(sasc)$gene_short_name\n)\n\ntable(fData(sasc)$MT)\n\n\nFALSE  TRUE \n36588    13 \n\n\nBased upon the mitochondrial DNA content we calculated, we can evaluate the percentage of mitochondrial reads in our data.\n\npData(sasc)$MT_reads &lt;- Matrix::colSums(exprs(sasc)[fData(sasc)$MT,])\npData(sasc)$MT_perc &lt;- pData(sasc)$MT_reads/Matrix::colSums(exprs(sasc))*100\n\nsummary(sasc$MT_perc)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.1303  0.9560  2.8839  3.2258 61.3861       1 \n\n\nWith our calculated mitochondrial percentages, we can start to visualize our data.\nLet’s look at a plot of genes by UMIs, colored by mitochondrial percentage.\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = n.umi, y = num_genes_expressed)) +\n  facet_wrap(~batch, nrow = 1) +\n  geom_point(size = 0.5, alpha = 0.3, aes(color = MT_perc)) +\n  theme_light() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14),\n        axis.title = element_text(size = 16),\n        axis.text.y = element_text(size = 14),\n        aspect.ratio = 1) +\n  scale_color_viridis() +\n  xlab(\"UMIs\") +\n  ylab(\"Number of Genes Captured\") +\n  scale_y_log10() +\n  scale_x_log10() +\n  geom_abline(slope = 1, color = \"grey\") +\n  geom_hline(yintercept = 200, linetype = \"dotted\", color = \"red\") +\n  geom_vline(xintercept = 300, linetype = \"dotted\", color = \"red\")\n\n\n\n\n\n\n\n\nWe can see that in spite of the fact that our two samples were identical, different mitochondrial content are exhibited across our assays. In particular, we see a lot of noise for the UMIs that correspond to under 200 genes.\nNow let’s look at mitochondrial percentage by donor type.\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = donor, y = MT_perc)) +\n  facet_wrap(~batch, nrow = 1, drop = TRUE, scales = \"free_x\") +\n  geom_violin(aes(fill = batch)) +\n  geom_boxplot(notch = T, fill = \"white\", width = 0.25, alpha = 0.3, outlier.shape = NA) +\n  theme_light() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14),\n        axis.title = element_text(size = 16),\n        axis.text.y = element_text(size = 14)) +\n  xlab(\"\") +\n  ylab(\"MT %\") +\n  theme(legend.position = \"none\")\n\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\n\n\n\n\n\n\n\n\n\nWe again see differences across our assays here in terms of MT percentage - in particular, it seems like we would want to have mitochondrial percentage no greater than 10% in our data.\nNow let’s look at UMIs by donor type.\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = donor, y = n.umi)) +\n  facet_wrap(~batch, nrow = 1, drop = TRUE, scales = \"free_x\") +\n  geom_violin(aes(fill = batch)) +\n  geom_boxplot(notch = T, fill = \"white\", width = 0.25, alpha = 0.3, outlier.shape = NA) +\n  theme_light() +\n  geom_hline(yintercept = 300, linetype = \"dashed\", color = \"blue\") + #change to thresholds\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14),\n        axis.title = element_text(size=16),\n        axis.text.y = element_text(size = 14)) +\n  scale_y_log10() +\n  xlab(\"\") +\n  ylab(\"UMIs\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nHere, it seems like data are hard to differentiate under a certain number of UMIs (in this case, 300).\nSo, let’s establish a QC filter for our data: 300 UMIs, 200 genes, and over 10% mitochondrial content.\n\n#Let's remove everything under 300 UMIs, 200 genes, and over 10% mitochondrial percentage\nsasc$qcflag &lt;- ifelse(\n  sasc$n.umi &gt;= 300 & \n  sasc$num_genes_expressed &gt;= 200 & \n  sasc$MT_perc &lt; 10, \n  \"PASS\", \n  \"FAIL\"\n)\n\ntable(sasc$qcflag)\n\n\n FAIL  PASS \n10180 24820 \n\nsasc &lt;- sasc[,sasc$qcflag == \"PASS\"] #filter out failing barcodes\nsasc\n\nclass: cell_data_set \ndim: 36601 24820 \nmetadata(1): cds_version\nassays(1): counts\nrownames(36601): ENSG00000243485 ENSG00000237613 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24820): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(12): barcode n.umi ... MT_perc qcflag\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nLet’s also remove UMIs that were mapped to the wrong genome, nonassignable to a donor, or deemed a doublet.\n\nsasc &lt;- sasc[,sasc$organism == \"human\" & sasc$donor %in% c(0, 1)]\nsasc\n\nclass: cell_data_set \ndim: 36601 24216 \nmetadata(1): cds_version\nassays(1): counts\nrownames(36601): ENSG00000243485 ENSG00000237613 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(12): barcode n.umi ... MT_perc qcflag\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nFinally, we have our semi-cleaned data!\n\ntable(sasc$batch)\n\n\nParse Scale \n11474 12742 \n\ntable(sasc$batch, sasc$donor)\n\n       \n           0    1 doublet unassigned\n  Parse 8637 2837       0          0\n  Scale 4506 8236       0          0\n\n\n\n\n4. Pre-processing and data visualization\nAfter completing quality control of our data, we can start to pre-process it.\n\n# remove non-expressed/non-captured genes \nhist(fData(sasc)$num_cells_expressed)\n\n\n\n\n\n\n\n\nWe can also conduct feature selection for genes if we want. This step is not necessary in every case, but it can help with reducing computational loads. Here, we filter out genes that are not expressed in at least 25 cells.\n\ntable(fData(sasc)$num_cells_expressed &gt; 25)\n\n\nFALSE  TRUE \n15315 21286 \n\n# filter out genes not expressed in at least 25 cells\nsasc &lt;- sasc[fData(sasc)$num_cells_expressed &gt; 25, ]\nsasc \n\nclass: cell_data_set \ndim: 21286 24216 \nmetadata(1): cds_version\nassays(1): counts\nrownames(21286): ENSG00000238009 ENSG00000241860 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(12): barcode n.umi ... MT_perc qcflag\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nNow we apply the estimate_size_factors function from Monocle 3 to evaluate the relative bias in each cell.\n\nsasc &lt;- estimate_size_factors(sasc)\n\nAfter estimating size factors, we can run preprocess_cds, a standardized workflow in the Monocle 3 package that normalizes the data by log and size factor to address depth differences, and then calculates a lower dimensional space that will be used as the input for further dimensionality reduction.\n\nset.seed(1000)\nsasc &lt;- preprocess_cds(sasc) #this may take a few minutes\n\nWe can then call reduce_dimensions from Monocle 3 on our data to get down to the most relevant components.\n\nset.seed(1000)\nsasc &lt;- reduce_dimension(sasc) #this may take a few minutes\n\nNo preprocess_method specified, using preprocess_method = 'PCA'\n\n\nNow let’s plot the cells in our data (first colored by batch and then by donor)\n\nplot_cells(sasc, color_cells_by = \"batch\")\n\nNo trajectory to plot. Has learn_graph() been called yet?\n\n\n\n\n\n\n\n\nplot_cells(sasc, color_cells_by = \"donor\")\n\nNo trajectory to plot. Has learn_graph() been called yet?\n\n\n\n\n\n\n\n\n\nWe now have our dimensionality-reduced data! Let’s use k-means clustering to categorize our cells into clusters.\nOne can (and should) spend a lot of time tweaking their clustering parameters. In this situation, we’ll go with a k of 40 for Leiden clustering. It is advisable to try several k-values and/or resolutions during this exploratory data analysis.\n\nsasc &lt;- cluster_cells(\n  sasc, \n  k=40, \n  cluster_method=\"leiden\", \n  random_seed=1000\n) #this may take a few minutes\n\n# add cluster information for each cell\ncolData(sasc)$k40_leiden_clusters = clusters(sasc) \n\nplot_cells(sasc)\n\n\n\n\n\n\n\n\nLet’s get the top marker genes based on our Leiden clustering:\n\ntop_marker_genes &lt;- top_markers(\n  sasc, \n  group_cells_by=\"k40_leiden_clusters\"\n)\n\nkeep &lt;- top_marker_genes %&gt;%\n  filter(fraction_expressing &gt;= 0.30) %&gt;%\n  group_by(cell_group) %&gt;%\n  top_n(3, marker_score) %&gt;%\n  pull(gene_short_name) %&gt;%\n  unique()\n\n\nplot_genes_by_group(\n  sasc,\n  c(keep),\n  group_cells_by = \"k40_leiden_clusters\", #\"partition\", \"cluster\"\n  ordering_type = \"maximal_on_diag\",\n  max.size = 3\n)\n\n\n\n\n\n\n\n\nLet’s do some more data visualization here. We’ll move out of Monocle 3 and into ggplot2 to improve our flexibility with plotting.\n\n#add UMAP coordinates to the colData for easy plotting\nsasc$UMAP1 &lt;- reducedDim(sasc, \"UMAP\")[,1]\nsasc$UMAP2 &lt;- reducedDim(sasc, \"UMAP\")[,2]\n\n\n#generate a distinguishable color scheme\nset.seed(1000)\ncolpal &lt;- randomcoloR::distinctColorPalette(k=12)\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x=UMAP1, y=UMAP2, color=cell_type)) +\n  facet_wrap(~batch+donor) +\n  geom_point(size=0.5, alpha=0.5) +\n  theme_bw() +\n  scale_color_manual(values=colpal) +\n  theme(legend.position=\"bottom\", aspect.ratio = 1, panel.grid=element_blank()) +\n  guides(color = guide_legend(override.aes = list(size=8, alpha=1)))\n\n\n\n\n\n\n\n\nFrom our visualizations, clusters 2, 3, 4, and 9 all appear to be related cell types - ventricular cardiomyocytes.\nWe can also see across our four plots that we have clear evidence of both technical (batch) and biological (donor) variation!\n\n\n5. Quantifying a batch effect\nNow that we have evidence of a batch effect, let’s quantify it! We’ll make use of the kBET (k-nearest neighbor batch effect test) package from the Theis lab.\n\n#kBET - k-nearest neighbour batch effect test\ndata &lt;- reducedDim(sasc)\nbatch &lt;- sasc$batch\n\nsubset_size &lt;- 0.1 #subsample to 10% of the data for speed\nsubset_id &lt;- sample.int(\n  n = length(batch), \n  size = floor(subset_size * length(batch)), \n  replace=FALSE\n)\n\nset.seed(1000)\nbatch.estimate &lt;- kBET(\n  data[subset_id,], \n  batch[subset_id]\n) #this may take a few minutes\n\n\n\n\n\n\n\n\n\nbatch.estimate$summary\n\n      kBET.expected kBET.observed kBET.signif\nmean    0.002482853     0.9888066           0\n2.5%    0.000000000     0.9753086           0\n50%     0.002743484     0.9917695           0\n97.5%   0.006207133     1.0000000           0\n\n\nBased on our rejection rate plot, it really does seem that we have a batch effect in our data. We can simulate a random batch assignment in our data and look at the same plot to convince ourselves of this observation.\n\nset.seed(1000)\nrandombatch &lt;- sample(sasc$batch, dim(sasc)[2])\nbatch.estimate.fake &lt;- kBET(\n  data[subset_id,], \n  randombatch[subset_id]\n) #this may take a few minutes\n\n\n\n\n\n\n\n\nAlright, so we clearly do have a batch effect!\n\n\n6. Batch correction\nSince we’ve proven that we have a batch effect in our data, let’s perform batch correction using Monocle 3. We make use of the align_cds function, a wrapper built around the reducedMNN function from the batchelor package developed by the Marioni Lab at the University of Cambridge.\n\n### Batch correction built-in to Monocle3\nset.seed(1000)\nbc_cds &lt;- align_cds(\n  sasc, \n  alignment_group = \"batch\", \n  k = 50\n) #this may take a minute\n\nAligning cells from different batches using Batchelor.\nPlease remember to cite:\n     Haghverdi L, Lun ATL, Morgan MD, Marioni JC (2018). 'Batch effects in single-cell RNA-sequencing data are corrected by matching mutual nearest neighbors.' Nat. Biotechnol., 36(5), 421-427. doi: 10.1038/nbt.4091\n\nbc_cds\n\nclass: cell_data_set \ndim: 21286 24216 \nmetadata(2): cds_version citations\nassays(1): counts\nrownames(21286): ENSG00000238009 ENSG00000241860 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(16): barcode n.umi ... UMAP1 UMAP2\nreducedDimNames(3): PCA UMAP Aligned\nmainExpName: NULL\naltExpNames(0):\n\n\nBased on our ‘aligned’ PCA, we can then call reduce_dimensions to generate a corresponding UMAP.\n\n# We can run reduce_dimensions to generate a UMAP from the 'aligned' PCA\nset.seed(1000)\nbc_cds &lt;- reduce_dimension(\n  bc_cds, \n  reduction_method = \"UMAP\", \n  preprocess_method = \"Aligned\"\n)\n\nsasc$aligned_UMAP1 &lt;- reducedDim(bc_cds, \"UMAP\")[,1] #save these in our original cds\nsasc$aligned_UMAP2 &lt;- reducedDim(bc_cds, \"UMAP\")[,2] #save these in our original cds\n\nplot_cells(bc_cds, color_cells_by = \"batch\")\n\nNo trajectory to plot. Has learn_graph() been called yet?\n\n\n\n\n\n\n\n\n\nFinally, we use kBET again to quantitatively verify that we have removed our batch effect from our data.\n\n#Use kBET to quantitatively ask if it removes the batch effect\ndata &lt;- reducedDim(bc_cds, \"UMAP\") #note that we are running this on the UMAP\nbatch &lt;- bc_cds$batch\nsubset_size &lt;- 0.1 #subsample to 10% of the data\nsubset_id &lt;- sample.int(\n  n = length(batch), \n  size = floor(subset_size * length(batch)), \n  replace = FALSE\n)\n\nset.seed(1000)\nbatch.estimate &lt;- kBET(\n  data[subset_id,], \n  batch[subset_id]\n) #this may take a few minutes\n\n\n\n\n\n\n\nbatch.estimate$summary\n\n      kBET.expected kBET.observed kBET.signif\nmean     0.02403292     0.8678189           0\n2.5%     0.01574074     0.8291152           0\n50%      0.02331962     0.8683128           0\n97.5%    0.03364198     0.9012346           0\n\n\nWhile the UMAP looks much better, the kBET metric is telling us that there is still a batch effect. If we run kBET on the ‘aligned’ PCs, the rejection rate is still close to 1. Is it possible we introduced artifacts through our batch correction? Let’s try the Harmony package from the Raychaudhuri Lab at Harvard and see if anything is different.\n\n### Batch correction with Harmony\nset.seed(1000)\nharm_cds &lt;- RunHarmony(sasc, 'batch') #this may take a few minutes\n\nTransposing data matrix\n\n\nInitializing state using k-means centroids initialization\n\n\nHarmony 1/10\n\n\nHarmony 2/10\n\n\nHarmony 3/10\n\n\nHarmony 4/10\n\n\nHarmony converged after 4 iterations\n\nharm_cds #note the \"HARMONY\" in reducedDim\n\nclass: cell_data_set \ndim: 21286 24216 \nmetadata(2): cds_version citations\nassays(1): counts\nrownames(21286): ENSG00000238009 ENSG00000241860 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(18): barcode n.umi ... aligned_UMAP1 aligned_UMAP2\nreducedDimNames(3): PCA UMAP HARMONY\nmainExpName: NULL\naltExpNames(0):\n\nsasc\n\nclass: cell_data_set \ndim: 21286 24216 \nmetadata(2): cds_version citations\nassays(1): counts\nrownames(21286): ENSG00000238009 ENSG00000241860 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(18): barcode n.umi ... aligned_UMAP1 aligned_UMAP2\nreducedDimNames(2): PCA UMAP\nmainExpName: NULL\naltExpNames(0):\n\n\n\n# Under the hood, Monocle 3 is using the uwot package to generate UMAPs\nharmony_umap &lt;- umap(reducedDim(harm_cds, \"HARMONY\"), seed=1000) #this may take a minute\nsasc$harmony_UMAP1 &lt;- harmony_umap[,1] #save these to our original cds\nsasc$harmony_UMAP2 &lt;- harmony_umap[,2] #save these to our original cds\n\n\n#Plot\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = harmony_UMAP1, y = harmony_UMAP2, color = batch)) +\n  geom_point(size=0.5, alpha=0.5) +\n  theme_bw() + \n  scale_color_viridis(discrete=T, begin=0.1, end=0.9, option=\"A\") +\n  theme(legend.position=\"bottom\", aspect.ratio = 1, panel.grid=element_blank()) +\n  guides(color = guide_legend(override.aes = list(size = 8, alpha = 1)))\n\n\n\n\n\n\n\n\n\n#Use kBET to quantitatively ask\ndata &lt;- harmony_umap #note, we could alternatively run kBET at the level of the corrected PCs \nbatch &lt;- harm_cds$batch\nsubset_size &lt;- 0.1 #subsample to 10% of the data\nsubset_id &lt;- sample.int(\n  n = length(batch),\n  size = floor(subset_size * length(batch)), \n  replace = FALSE)\n\nset.seed(1000)\nbatch.estimate &lt;- kBET(data[subset_id,], batch[subset_id]) #this may take a few minutes\n\n\n\n\n\n\n\nbatch.estimate$summary\n\n      kBET.expected kBET.observed kBET.signif\nmean    0.003004115     0.8504115           0\n2.5%    0.000000000     0.8106996           0\n50%     0.002743484     0.8518519           0\n97.5%   0.008230453     0.8973251           0\n\n\nOnce again, our UMAP looks better, but our kBET metric suggests that it is still far from perfect.\nSo what benefit does batch correction offer? This is debatable, but certainly one thing it can do is help in identifying cell types. In our case, our toy dataset already had annotated cell types, but if we didn’t know these ahead of time, batch correction could help us identify them.\n\n#cluster cells that have been aligned and plot these on our original UMAP\nbc_cds &lt;- cluster_cells(\n  bc_cds, \n  k = 40, \n  cluster_method = \"leiden\", \n  random_seed = 1000\n) #this may take a few minutes\n\n\nsasc$aligned_clusters &lt;- clusters(bc_cds) #save to original cds object\nsasc$aligned_partitions &lt;- partitions(bc_cds) #save to original cds object\n\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = UMAP1, y = UMAP2, color = k40_leiden_clusters)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  theme_bw() + \n  scale_color_manual(values = colpal) +\n  theme(legend.position = \"bottom\", aspect.ratio = 1, panel.grid = element_blank()) +\n  guides(color = guide_legend(override.aes = list(size = 8, alpha = 1)))\n\n\n\n\n\n\n\n\n\ndata.frame(colData(sasc)) %&gt;%\n  group_by(cell_type) %&gt;%\n  count(aligned_partitions) %&gt;%\n  spread(aligned_partitions, n)\n\n# A tibble: 12 × 6\n# Groups:   cell_type [12]\n   cell_type                   `1`   `2`   `3`   `4`   `5`\n   &lt;fct&gt;                     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 Adipocyte                     1    NA    NA    63    NA\n 2 Atrial Cardiomyocyte          5    NA    NA    NA    NA\n 3 Cytoplasmic Cardiomyocyte  1368    16     3     2     3\n 4 Endothelium                2309     9   238    NA    31\n 5 Fibroblast                 2666     6     4    NA     1\n 6 Lymphocyte                  192    NA    NA    NA    NA\n 7 Macrophage                 1752     4    11    NA    NA\n 8 Neuronal                    445     4     2    NA    NA\n 9 Pericyte                    280   880     2     1    NA\n10 Unknown                    2981    71    34    15    17\n11 Vascular Smooth Muscle      116    87     1    NA    NA\n12 Ventricular Cardiomyocyte 10556    15     1    23     1\n\n\nEven with our imperfect batch correction methods, our analysis has started to show us that some of our original clusters are related (ventricular cardiomyocytes).\n\n\n7. Differential gene expression\nWe are going to focus specifically on the ventricular cardiomyocytes for the rest of this code example. We start by subsetting our data down to ventricular cardiomyocytes that belong to clusters 2, 3, 4, and 9.\n\n#combine batch and donor as a new column \nsasc$id &lt;- paste(sasc$batch, sasc$donor, sep=\"_\")\n\n#subset ventricular cardiomyocyte data only \ncds_vent &lt;- sasc[,sasc$cell_type == \"Ventricular Cardiomyocyte\" &\n                   sasc$k40_leiden_clusters %in% c(\"2\", \"3\", \"4\", \"9\")]\n\nDifferential expression analysis can take a long time, so we will run the following code on a subset of pre-defined genes.\n\ngene_list &lt;- c(\"LINC00486\", \"TTN\", \"LINC-PINT\", \"TAS2R14\", \"MT-CO1\",\n               \"MT-ND4\", \"FN1\", \"LAMA2\", \"XIST\", \"PDK4\", \"ZBTB16\",\n               \"PPP1R3E\", \"TMTC1\", \"NT5DC3\", \"RBX1\", \"MRPL45\", \"ESR2\",\n               \"TUBGCP4\", \"MYH7\", \"MYL2\", \"MB\", \"ACTC1\", \"TPM1\", \"MYH6\")\n\ncds_subset &lt;- cds_vent[rowData(cds_vent)$gene_short_name %in% gene_list,]\n\nWe now plot expression levels of these genes, split by donor.\n\nplot_genes_violin(cds_subset, group_cells_by = \"donor\", ncol = 4) +\n  theme(axis.text.x=element_text(angle = 45, hjust = 1))\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\nWarning: Removed 108135 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 108135 rows containing non-finite outside the scale range\n(`stat_summary()`).\n\n\n\n\n\n\n\n\n\nIt is clear that some of our genes are differentially expressed across our two donors. How do we tell what contribution comes from donors and what comes from assay batch? Let’s build models for our data and compare.\nIn the donor model, we assume there are no batch effects and the only contributing variable is donor.\n\ndonor_model &lt;- fit_models(\n  cds_subset,\n  model_formula_str = \"~donor\",\n  expression_family=\"negbinomial\"\n)\n\n\ncoefficient_table(donor_model) %&gt;% \n  filter(term == \"donor1\") %&gt;%\n  filter(q_value &lt; 0.05) %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) %&gt;%\n  arrange(gene_short_name)\n\n# A tibble: 15 × 5\n   id              gene_short_name term     q_value estimate\n   &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 ENSG00000159251 ACTC1           donor1 5.46e-125   0.671 \n 2 ENSG00000115414 FN1             donor1 0           2.40  \n 3 ENSG00000196569 LAMA2           donor1 2.56e- 88  -0.245 \n 4 ENSG00000231721 LINC-PINT       donor1 1.67e-258  -1.91  \n 5 ENSG00000230876 LINC00486       donor1 4.42e-217   1.14  \n 6 ENSG00000198125 MB              donor1 1.38e- 55   0.463 \n 7 ENSG00000198804 MT-CO1          donor1 2.12e-214   1.23  \n 8 ENSG00000198886 MT-ND4          donor1 1.25e-170   1.13  \n 9 ENSG00000092054 MYH7            donor1 1.46e-107  -0.460 \n10 ENSG00000111245 MYL2            donor1 1.76e- 22   0.299 \n11 ENSG00000004799 PDK4            donor1 5.25e- 29  -0.165 \n12 ENSG00000212127 TAS2R14         donor1 6.18e-155  -3.80  \n13 ENSG00000155657 TTN             donor1 2.41e- 24  -0.0999\n14 ENSG00000229807 XIST            donor1 1.59e-167  -5.55  \n15 ENSG00000109906 ZBTB16          donor1 3.44e-178  -0.579 \n\n\nIn our second model, we include batch effects as a predictor variable.\n\n#controlling for batch effects\ndonor_batch_model &lt;- fit_models(\n  cds_subset,\n  model_formula_str = \"~donor + batch\",\n  expression_family=\"negbinomial\"\n)\n\n\ncoefficient_table(donor_batch_model) %&gt;% \n  filter(term == \"donor1\") %&gt;%\n  filter(q_value &lt; 0.05) %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) %&gt;%\n  arrange(gene_short_name)\n\n# A tibble: 15 × 5\n   id              gene_short_name term     q_value estimate\n   &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 ENSG00000159251 ACTC1           donor1 6.11e-  6   0.165 \n 2 ENSG00000115414 FN1             donor1 2.00e-200   2.37  \n 3 ENSG00000196569 LAMA2           donor1 3.44e- 72  -0.275 \n 4 ENSG00000231721 LINC-PINT       donor1 3.45e-  2   0.228 \n 5 ENSG00000198125 MB              donor1 4.23e-  2  -0.0950\n 6 ENSG00000198804 MT-CO1          donor1 4.17e- 43   0.605 \n 7 ENSG00000198886 MT-ND4          donor1 1.48e- 21   0.422 \n 8 ENSG00000197616 MYH6            donor1 6.76e- 23   0.303 \n 9 ENSG00000092054 MYH7            donor1 7.85e- 16  -0.215 \n10 ENSG00000111245 MYL2            donor1 1.12e- 13   0.299 \n11 ENSG00000004799 PDK4            donor1 5.56e-243  -0.534 \n12 ENSG00000212127 TAS2R14         donor1 9.17e-  3  -0.527 \n13 ENSG00000155657 TTN             donor1 2.00e- 36   0.142 \n14 ENSG00000229807 XIST            donor1 1.15e-167  -5.56  \n15 ENSG00000109906 ZBTB16          donor1 3.01e-  3  -0.0961\n\n\nComparing these two models, we can see that some of our genes are significantly influenced by the introduction of batch as a predictor.\n\n# Comparing models of gene expression\ncompare_models(donor_batch_model, donor_model) %&gt;% \n  select(gene_short_name, q_value) %&gt;% \n  data.frame()\n\n   gene_short_name       q_value\n1        LINC00486  0.000000e+00\n2              TTN 4.329415e-286\n3              FN1  1.000000e+00\n4            LAMA2  1.020045e-02\n5             PDK4  0.000000e+00\n6        LINC-PINT  0.000000e+00\n7           ZBTB16 6.655693e-187\n8          TAS2R14  0.000000e+00\n9            TMTC1  2.872632e-05\n10          NT5DC3  5.740679e-02\n11            MYL2  1.000000e+00\n12         PPP1R3E  4.636225e-01\n13            MYH6  1.999277e-62\n14            MYH7  5.726891e-61\n15            ESR2  2.311010e-02\n16           ACTC1 4.529192e-170\n17         TUBGCP4  5.352325e-01\n18            TPM1  9.684985e-01\n19          MRPL45  1.000000e+00\n20              MB 4.086623e-205\n21            RBX1  1.000000e+00\n22            XIST  1.000000e+00\n23          MT-CO1 7.757731e-279\n24          MT-ND4  0.000000e+00\n\n\nLet’s take a closer look at two example genes: LINC00486 and TTN, both of which have highly significant q-values (close to 0).\n\n# Get normalized counts\ncntmtx &lt;- normalized_counts(cds_subset)\ncds_subset$LINC00486 &lt;- cntmtx[\"ENSG00000230876\",]\ncds_subset$TTN &lt;- cntmtx[\"ENSG00000155657\",]\n\n\n#the case of LINC00486\na &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = donor, y = LINC00486)) + \n  geom_violin(aes(fill = donor)) +\n  geom_boxplot(width = 0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nb &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = batch, y = LINC00486)) + \n  geom_violin(aes(fill = \"salmon\")) +\n  geom_boxplot(width=0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nc &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = id, y = LINC00486, fill = donor)) + \n  geom_violin() +\n  geom_boxplot(width=0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nplot_grid(a + theme(legend.position = \"none\"),\n          b + theme(legend.position = \"none\") + ylab(\"\"),\n          c + theme(legend.position = \"none\") + ylab(\"\"), \n          labels=c(\"A\", \"B\", \"C\"),\n          nrow = 1)\n\n\n\n\n\n\n\n\nFrom our model, we can see that the batch variable was the primary contributor to differences in LINC00486 expression across our data.\n\ncoefficient_table(donor_model) %&gt;% \n  filter(gene_short_name == \"LINC00486\" & term ==\"donor1\") %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) \n\n# A tibble: 1 × 5\n  id              gene_short_name term     q_value estimate\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 ENSG00000230876 LINC00486       donor1 4.42e-217     1.14\n\ncoefficient_table(donor_batch_model) %&gt;% \n  filter(gene_short_name == \"LINC00486\" & term %in% c(\"donor1\", \"batchScale\")) %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) \n\n# A tibble: 2 × 5\n  id              gene_short_name term       q_value estimate\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 ENSG00000230876 LINC00486       donor1           1   0.0133\n2 ENSG00000230876 LINC00486       batchScale       0   3.64  \n\n\n\n#the case of TNN\nx &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = donor, y = TTN)) + \n  geom_violin(aes(fill = donor)) +\n  geom_boxplot(width = 0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\ny &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = batch, y = TTN)) + \n  geom_violin(aes(fill = \"salmon\")) +\n  geom_boxplot(width = 0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nz &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = id, y = TTN, fill = donor)) + \n  geom_violin() +\n  geom_boxplot(width = 0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nplot_grid(\n  x + theme(legend.position=\"none\"),\n  y + theme(legend.position=\"none\") + ylab(\"\"),\n  z + theme(legend.position=\"none\") + ylab(\"\"), \n  labels=c(\"A\", \"B\", \"C\"),\n  nrow = 1)\n\n\n\n\n\n\n\n\nOn the other hand, in the case of TTN, we see that both donor and batch affected its expression.\n\ncoefficient_table(donor_model) %&gt;% \n  filter(gene_short_name == \"TTN\" & term ==\"donor1\") %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) \n\n# A tibble: 1 × 5\n  id              gene_short_name term    q_value estimate\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 ENSG00000155657 TTN             donor1 2.41e-24  -0.0999\n\ncoefficient_table(donor_batch_model) %&gt;% \n  filter(gene_short_name == \"TTN\" & term %in% c(\"donor1\", \"batchScale\")) %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) \n\n# A tibble: 2 × 5\n  id              gene_short_name term        q_value estimate\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 ENSG00000155657 TTN             donor1     2.00e-36    0.142\n2 ENSG00000155657 TTN             batchScale 0          -0.405\n\n\n\n\n8. Key takeaways and suggestions\n\nDo not blindly apply batch correction! You risk introducing more artifacts than you remove.\nIn general, it is better to use batch correction methods for data exploration and visualization rather than analysis.\nIf you have a batch effect, model it in your downstream analysis (i.e include it as a covariate) instead of modifying your data directly.\n\n\n\n9. Conclusions\nThis concludes the content that was covered in the SASC User Group workshop on batch correction. I’d like to give a huge thank you again to Mary and the team at the BBI for organizing this tutorial and sharing it freely with the public! I look forward to covering more from these meetings in the future, as well as sharing my own thoughts and exploration of single cell data down the road."
  },
  {
    "objectID": "posts/terminologies/index.html",
    "href": "posts/terminologies/index.html",
    "title": "Biomedical Data Science sub-disciplines (v0)",
    "section": "",
    "text": "Welcome to another week of [VS]Codes! In today’s post, I will be summarizing my personal definitions and interpretations of the various subdisciplines in biomedical data science. Given my prior background and experience in this subject area, I figured that writing this post would be an easy task! However, referencing the literature further made me realize that the boundaries across the subfields of health technology are fairly nebulous. Different departments, organizations, and individuals all have their own opinions of what their work involves. Dr. Bill Hersh from Oregon Health and Science University has a great paper (all the way back from 2009!) on the need to converge on consistent terminology in the field of biomedical informatics: you can give it a read here.\nGiven the lack of clarity in the subfields of biomedical data science, my goal today will be to provide my personal interpretation of the breakdown of relevant fields, informed by some of the definitions I’ve collated from external references. This description is absolutely meant to be a first draft of sorts (hence the v0 in the title)… I aim to refine these terms further based on additional input down the road."
  },
  {
    "objectID": "posts/terminologies/index.html#information-and-data",
    "href": "posts/terminologies/index.html#information-and-data",
    "title": "Biomedical Data Science sub-disciplines (v0)",
    "section": "Information and Data",
    "text": "Information and Data\nInformation Technology (IT)\nInformation technology (IT) refers to the use of computer systems to manage, process, protect, and exchange information. The overarching goal among its specializations is to use technology systems to solve problems and handle information. [5]\nInformation Science\nInformation science is the theoretical study of how information is created, organized, managed, stored, retrieved, and used. It is an interdisciplinary field that combines aspects of computer science and information management. [6]\nInformatics\nInformatics refers to the use and implementation of technology systems to analyze and manage information. Its primary focus is its application to specific external domains “for the good of people, organizations, and society”. [7, 8]\nData Science\nData science refers the study of data to extract meaningful insights and acquire knowledge. It is an interdisciplinary field that combines principles from a broad range of fields, including mathematics, statistics, artificial intelligence, and computer engineering, to collect, process, and analyze large amounts of data. There is a stronger focus on predictive modeling and algorithmic design. [9, 10, 11]\nData Analytics\nData analysis involves the analysis and examination of large amounts of data to better understand trends in the system being studied. There is a stronger focus on developing tables, visualizations, and overarching products to comprehend the data. [12]\nBased on the above definitions, I visualize the interplay of information and data as follows:\n\nInformation technology serves as the overarching guidebook for computational work. Within IT, information science dictates the way that we collect, store, organize, and manage our data. And finally, under both information technology and information science, data science, data analytics, and informatics intersect with one another, allowing for the development of predictive models and the analysis of data for applications to external domains."
  },
  {
    "objectID": "posts/terminologies/index.html#biomedical-informatics-and-computational-biology",
    "href": "posts/terminologies/index.html#biomedical-informatics-and-computational-biology",
    "title": "Biomedical Data Science sub-disciplines (v0)",
    "section": "Biomedical Informatics and Computational Biology",
    "text": "Biomedical Informatics and Computational Biology\nBiomedicine\nBiomedicine and the biomedical sciences refer to a set of sciences that apply understandings of biology and the natural sciences to develop knowledge, interventions, or technology that are of use in healthcare or public health. [13]\n\nBiomedical Informatics\nBiomedical Informatics\nBiomedical informatics is an interdisciplinary field seeking to study and advance the use of biomedical data to improve individual health, public health, and healthcare. It investigates, simulates, experiments with and translates a wide swath of biological systems to connect basic and clinical research with practical application for the overall betterment of healthcare. [17, 18, 19]\nBioinformatics\nBioinformatics, the application of biomedical informatics in cellular and molecular biology (often with a focus on genomics), is a scientific subdiscipline that involves using computer technology to collect, store, analyze, and disseminate biological data and information, such as DNA and amino acid sequences or annotations about those sequences, to increase our understanding of health and disease. [19, 20]\nTranslational Informatics\nTranslational informatics, the application of biomedical informatics to human health, is focused on the study and application of existing biomedical data to bridge new ways to improve diagnosis, staging, prognosis, and treatment of human disease. [19, 21]\nClinical Informatics\nClinical informatics, the subdiscipline of biomedical informatics related to patient data (typically from electronic medical records), focuses on the application of informatics to specific clinical subdisciplines, such as healthcare, nursing, dentistry, and pathology. [19, 22]\nThe subfields of biomedical informatics can be represented in the following hierarchy:\n\nBioinformatics is focused on processing the core data of biological systems, translational informatics is focused on the translation from biology to medicine, and clinical informatics is focused on the analysis of medical and healthcare data.\n\n\nComputational Biology\nComputational Biology\nComputational biology refers to the use of mathematics, statistics, and algorithms to understand biological systems based on data from experimental measurements. Examples of biological questions that may be tackled include what:\n\nbiological tasks are carried out by particular nucleic acid or peptide sequences\nwhich gene (or genes) when expressed produce a particular phenotype or behavior\nwhat sequence of changes in gene or protein expression or localization lead to a particular disease\nhow do changes in cell organization influence cell behavior. [23]\n\nComputational Genomics\nGenomics is a subfield of biomedicine focused on studying the entire set of DNA of an organism. Genomics research involves identifying and characterizing all the genes and functional elements in an organism’s genome as well as how they interact. [14, 15] Computational genomics refers to the use of computational and statistical analysis to decipher biology from genome sequences and related data, including both DNA and RNA sequence as well as other “post-genomic” data. [24]\nThe following figure shows some of the subfields of biology that can be addressed through computation. Computational genomics could be utilized in all three cases.\n\n\n\nDifferentiating biomedical informatics and computational biology\nBiomedical informatics and computational biology are very similar terms to one another - both involve the interdisciplinary application of information technology to biomedicine. The key differentiators I see between the two terms are the order of prioritization in disciplines and the nature of the data under consideration.\nIn biomedical informatics, the focus is on the development computational infrastructure and analysis to handle large-scale biomedical data.\nOn the other hand, computational biology starts with the focus on a biological experiment. From a biological question, corresponding data are generated, and computational analyses are applied.\nIndeed, I see the second word in each phrase as the “order of operations”: biomedical informatics is centered on the informatics, and computational biology is centered on the biology."
  },
  {
    "objectID": "posts/terminologies/index.html#health-technology-and-biotechnology",
    "href": "posts/terminologies/index.html#health-technology-and-biotechnology",
    "title": "Biomedical Data Science sub-disciplines (v0)",
    "section": "Health Technology and Biotechnology",
    "text": "Health Technology and Biotechnology\nHealth Technology\nHealth technology, or “health tech,” refers to the use of technologies developed for the purpose of improving any and all aspects of the healthcare system. It is focused primarily on the development of healthcare products and services. [25]\nHealth Information Technology\nHealth information technology (health IT) involves the processing, storage, and exchange of health information in an electronic environment. Applications include enhancing the quality of healthcare, preventing medical errors, reducing healthcare costs, and expanding access to healthcare. [26]\nBiotechnology\nBiotechnology (biotech) involves the use of living organisms and/or biological systems to develop or create different products [27].\nBioengineering\nBioengineering involves the application of engineering principles in combination with living organisms and/or biological systems to develop or create different products. These solutions may take the form of devices or computer programs (e.g., simulation of biomedical processes). However, the focus is on the biomedical problem to be solved, not data, information or knowledge. [28]\nDigital Health\nDigital health refers to the use of information and communications technologies in medicine and other health professions to manage illnesses and health risks and to promote wellness. [29]\nI see health technology and biotechnology as synonymous fields to one another, with health tech focused more on advancing human healthcare and biotech focused more on the use of biological systems. The incorporation of IT into health technology yields digital health and health IT, while the incorporation of engineering into biotechnology yields bioengineering."
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello world!",
    "section": "",
    "text": "Hello world, and welcome to my official blog! I’ve had numerous thoughts swirling around in my head over the past several years, but have had trouble formally committing to bringing them out into the world… however, encouragement from colleagues and recent reading (i.e. Dorie Clark’s The Long Game) have convinced me to finally bring my virtual pen to paper. In particular, I found myself tremendously inspired by the Fred Hutch DaSL Culture and Work Style document and the mantra of “ship as soon as you can.” The following graphic does a great job at reminding us how to avoid the pit of perfectionism!\n\nMy hopes for this blog are to (a) build a community, (b) share my knowledge, and (c) learn alongside my readers. I aim to write weekly (if not more frequent) posts on a variety of subjects, including:\n\nmy personal story\nadvice to others in my field\ntechnical tutorials\nsubject matter overviews, and\nmiscellaneous detours\n\nI have no expectations that my content will be perfect, but then again, learning from mistakes is one of the best parts of life :)\nGiven my background and expertise, my general focus will be on the world of biomedical data science and health technology, but the sky is the limit for the topics we may cover!\nThanks for tuning in - I’m delighted to have you along for the ride…"
  },
  {
    "objectID": "posts/professional-journey/index.html",
    "href": "posts/professional-journey/index.html",
    "title": "My professional journey",
    "section": "",
    "text": "For my first “real” post, I figured it would be a helpful exercise to go through my professional journey and track how I ended up where I currently am. This description is meant to be more of an overview, and I intend to provide more details on individual portions of it in the future.\n\nHaving grown up in the Silicon Valley, I always had a front-row seat to the power and potential of technology to improve people’s lives. All of the biggest tech companies had their headquarters within driving distance from my home, and every day, I could see how they had impacted not only myself but also everyone around me: computers, cellphones, social media, education, automobiles, entertainment… everything was shaped by information technology. Both of my parents were in the software industry too, and seeing the productive, fulfilling jobs that they were able to have made me certain that I wanted to involve technology in my future career. Learning how to code from my mother in high school made me feel like I was being imparted with some special kind of magic - entering the world of software engineering truly felt right at my fingertips.\nAt the same time, growing up in the Silicon Valley felt like growing up in a bubble. I yearned to explore the link between information technology and its downstream applications beyond my baseline understanding of how to write code. So, I sought out more. In high school, I took a breadth of science classes, and I found myself inspired by the concept of “interdisciplinarity”. Instead of being drawn to “information technology,” I was drawn closer and closer to the world of “information science” and its applications to multiple disciplines, including biology and medicine. My drive for interdisciplinary experiences and my desire to explore a world of technological applications outside of the Bay Area led me to Duke University for my undergraduate education - in fact, one of the mantras of the university was “creative thinking across intellectual boundaries”. At Duke, I completed double majors in Computer Science and Statistics. Duke’s affiliated medical campus also gave me chances to explore interdisciplinary applications in the world of biomedical informatics, and I pursued multiple research opportunities, including an Honor’s thesis for my Statistics degree under the supervision of Dr. Li Ma. I also completed a minor in Computational Biology, taking classes such as Computational Genomics with Dr. Alexander Hartemink and Computational Structural Biology with Dr. Bruce Donald. Lastly, I was lucky to have great summer internship mentors (including Li-Yuan Chern at Pharmacyclics and Drs. Zichen Wang and Avi Ma’ayan at the Icahn School of Medicine), who kept me motivated and inspired to stick to my path and pursue an advanced interdisciplinary career.\nBy the end of my undergrad, I knew that I wasn’t going to enter the traditional computer science recruitment cycle for software engineering roles - I instead applied to doctoral programs in biomedical informatics and computational biology that would allow me to build my knowledge base further and prepare me to become a leader in impactful projects that made a clear benefit in people’s lives. I was lucky to earn an admission with the Genomics and Computational Biology program at the University of Pennsylvania Perelman School of Medicine. Again, great mentors from my classes and research rotations (including Drs. Ryan Urbanowicz and Marylyn Ritchie to name a few) helped advance my training and made me a better researcher and scientist day by day. I am most indebted to my PhD advisor, Dr. Dokyoon Kim, for his support and mentorship throughout my PhD and subsequent post-doctoral position. With his leadership style and work ethic, he was a true role model for me throughout my graduate degree. I also fell in love with the combination of technical research and scientific storytelling that came out of my dissertation (to be discussed in a later post). During my PhD, I was able to complete a Master’s degree in Statistics and Data Science from the Wharton School of Business under the supervision of Dr. Anderson Zhang, as well as a summer internship as a User Experience Researcher in Health AI/ML under the supervision of Dr. Mandi Hall with the Health Futures team at Microsoft Research. All of these opportunities helped me to refine a set of motivators for my long-term career:\n\nimpact\nconnection\npassion\nleadership\n\nThese values aided me tremendously in my search for my first job upon the completion of my PhD. Today, I work as a clinical data scientist in the Translational Analytics and Informatics group at the Fred Hutchinson Cancer Center’s Data Science Lab (DaSL) in Seattle, WA (also to be discussed more in a future post). I am tremendously grateful to be a part of a supportive, driven community of fellow data scientists and researchers as we develop the clinical data infrastructure at Fred Hutch, and I look forward to sharing more about my work and career as the years progress!"
  },
  {
    "objectID": "posts/phd-context/index.html",
    "href": "posts/phd-context/index.html",
    "title": "My doctoral research: the background",
    "section": "",
    "text": "In the next few posts, I will be providing an overview of my PhD in Biomedical Informatics and Computational Genomics that I completed under the mentorship of Dr. Dokyoon Kim at the University of Pennsylvania Perelman School of Medicine. You can listen to a full presentation of my thesis defense here, and you can read the full text of my dissertation here. Note that all figures featured in this blog post were created using BioRender.com. Today’s post will focus on the background context that motivated my research. Without further ado, let’s get started!\n\n\nHuman biology is a complicated field. Millions of cells, cellular components, molecules, and chemicals interact with one another moment by moment to keep us alive each and every day. And while biologists and clinicians have defined a variety of classifications and ontologies to organize and understand these systems, the intricacies of the field continue to require extensive research and investment. Achieving a better understanding of human biology necessitates a unified language.\nAt the core of human biology and its defined ontologies lies the field of genetics. Genetics refers to the study of heritable traits (also known as phenotypes). If we define genetics as the language of biology, then the vocabulary of genetics would be the gene - a unit of heritable information that influences how we develop and operate. Extending the analogy further, the letters of genetics would be the nucleotide, a set of four molecules whose arrangements define different genes. Indeed, differences in individual nucleotides, also known as single-nucleotide variants/polymorphisms or “SNVs/SNPs”, are responsible for genetic variation across individuals and can lead to differing phenotypic outcomes. Genetic variation is a significant component of the diversity of life.\nGiven the complex interactions that occur across biomolecules in our bodies, it becomes apparent that networks of interacting genes drive our ability to live. Human diseases can be thought of as disruptions to these networks. The field of medicine aims to prevent, alleviate, and cure disease through the maintenance of health and the development of novel therapeutics.\nUnfortunately, for the most part, medicine today still operates from a “good enough” perspective - many patients are treated with the same medications without regard for or understanding of differences in their individual backgrounds or health profiles. Indeed, much more can be done to enhance the accuracy and efficacy of treatment.\nThe field of precision medicine uses large-scale multimodal/multiomic data to individualize patient care and gain a comprehensive understanding of human health. The goal of precision medicine is to achieve more accurate and precise disease prediction, prevention, treatment, and therapeutics. The field of genomics, involving the study of genetics from a “big data” lens, offers a significant opportunity to advance precision medicine research.\nGenomics refers to the study of an individual’s entire set of genes (a.k.a. their genome). We can work with genomic data from large-scale biomedical data, including both electronic health records (EHRs) and patient biobanks. EHRs, also known as electronic medical records (EMRs), refer to large clinical databases of patient medical history and clinical data. Biobanks, on the other hand, refer to biomedical databases with large quantities of patient biological samples, often including access to their genetic information. Combining these two data sources into a merged EHR-linked biobank provides an extra level of power in the study of genomics and medicine. EHR-linked biobanks offer the ability to identify and evaluate statistically significant genetic contributors to human disease. For instance, a genome-wide or phenome-wide association study (GWAS/PheWAS) applied to an EHR-linked biobank can identify associations between a variety of diseases and SNPs.\nMany research efforts in the field of precision medicine have used the results of PheWASs to identify genetic contributors to diseases. With such discoveries, patient genetic profiles can be built into diagnosis/treatment pipelines, allowing for the personalization of patient care.\nNotably, so far, most precision medicine research efforts that have made their way into the clinic have focused on one disease at a time. However, complex diseases rarely impact patients one-at-a-time. Shared SNPs and genes can contribute to the onset of multiple diseases in a single patient over time. These disease “multimorbidities” can lead to increase healthcare costs, health burdens, and risk of death. Thus, it becomes clear that we must evaluate the genetics of not only individual diseases but also cross-phenotype associations if we wish to gain a deeper understanding of overall patient health.\nGiven the significance of cross-phenotype associations, the field of “network medicine” offers a helpful framework to investigate the associations between diseases. Thus, the objective of my dissertation was to apply a “network medicine” approach to investigate genetic contributors to disease multimorbidities:\n\n\n\nFig 1. An overview of the process of using PheWAS results from an EHR-linked biobank for network medicine\n\n\nI broke this objective down into three chapters:\n\nCreation: construct and analyze a network of diseases derived from an EHR-linked biobank for the evaluation of genetic similarity between phenotypes\nComparison. generate and compare different disease networks generated from different populations and from genetic components.\nTranslation. extend the conclusions drawn from disease network analysis and comparison to downstream precision medicine applications.\n\n\n\n\nFig 2. The three sub-chapters of my PhD dissertation\n\n\nIn the coming week(s), I will go in-depth into the published manuscripts and preprints that correspond to these chapters, as well as my overall takeaways from my PhD research! Till next time~"
  },
  {
    "objectID": "posts/phd-papers/index.html",
    "href": "posts/phd-papers/index.html",
    "title": "My doctoral research: the content",
    "section": "",
    "text": "In last week’s post, I provided an overview of the context for my PhD research in Biomedical Informatics and Computational Genomics that I completed under the mentorship of Dr. Dokyoon Kim at the University of Pennsylvania Perelman School of Medicine. Today’s post will focus on some of the actual content that came out of my research. You can listen to a full presentation of my thesis defense here, and you can read the full text of my dissertation here. Note that all figures featured in this blog post were created using BioRender.com.\n\n\nAs discussed last week, the objective of my dissertation was to apply a network medicine approach to investigate genetic contributors to disease multimorbidities.\n\n\n\nFig 1. An overview of the process of using PheWAS results from an EHR-linked biobank for network medicine\n\n\nI broke this objective down into three chapters:\n\nCreation: construct and analyze a network of diseases derived from an EHR-linked biobank for the evaluation of genetic similarity between phenotypes\nComparison. generate and compare different disease networks generated from different populations and from genetic components.\nTranslation. extend the conclusions drawn from disease network analysis and comparison to downstream precision medicine applications.\n\n\n\n\nFig 2. The three sub-chapters of my PhD dissertation\n\n\nIn today’s post, I will provide an example manuscript from each of these chapters to provide more insight into some of the work that I did.\nChapter 1. Creation\nExample manuscript - NETMAGE: A human disease phenotype map generator for the network-based visualization of phenome-wide association study results\n\nDisease-disease networks (DDNs), graphs where nodes represent diseases and edges represent associations between diseases, can provide an intuitive way of understanding the relationships between diseases.\nUsing summary statistics from a phenome-wide association study (PheWAS), we can generate a corresponding DDN where edges represent shared genetic variants (e.g. SNPs) between diseases.\nSuch a network can help us analyze genetic associations across the “diseasome,” the landscape of all human diseases, and identify potential genetic influences for disease multimorbidities.\nTo improve the ease of network-based analysis of shared genetic components across diseases, we developed the humaN disEase phenoType MAp GEnerator (NETMAGE), a web-based tool that produces interactive DDN visualizations from PheWAS summary statistics. You can try out the tool we developed at the following link: https://hdpm.biomedinfolab.com/netmage/\n\nUsers can search their generated maps by various attributes and select nodes to view related diseases, associated variants, and various network statistics.\n\nAs a test case, we used NETMAGE to construct an example network from UK BioBank (UKBB) PheWAS summary statistic data. You can explore this network at the following link: https://hdpm.biomedinfolab.com/ddn/ukbb\n\nOur map correctly displayed previously identified disease comorbidities from the UKBB and identified concentrations of hub diseases in the endocrine/metabolic and circulatory disease categories.\n\nBy examining the associations between diseases in our map, we can identify potential genetic explanations for the relationships between diseases and better understand the underlying architecture of the human diseasome.\nYou can read the published manuscript for this project here.\n\nChapter 2. Comparison\nExample manuscript - The interplay of sex and genotype in disease associations: a comprehensive network analysis in the UK Biobank\n\nGiven that many individual diseases exhibit sex-specific differences in their genetic influences (also known as “genotype-by-sex” or “GxS” effects), we aimed to determine whether disease multimorbidities are also influenced by GxS interactions.\nThrough the comparison of sex-stratified DDNs, we investigated differences across the sexes in patterns of shared genetic architecture between diseases.\nUsing sex-stratified phenome-wide association study summary data from the UK Biobank, we built male- and female-specific DDNs for 103 different diseases.\nWe compared our networks using the network comparison methods highlighted in figure 3:\n\n\n\n\nFig 3. Overview of network comparison methods for comparing sex-stratified DDNs\n\n\n\nComparing the two graphs reveals that the diseasomes of males and females are similar to one another in terms of network topology.\n\n\n\n\nTable 1. Network statistics for our two sex-stratified DDNs\n\n\n\nSome diseases, however, seem to exhibit sex-specific influence in cross-phenotype associations. For instance, autoimmune and inflammatory disorders including multiple sclerosis and osteoarthritis are centrally involved only in the female-specific DDN, while cardiometabolic diseases and skin cancer are more prominent only in the male-specific DDN.\n\n\n\n\nTable 2. Most central diseases in our sex-stratified DDNs, based on centrality measures including degree, weighted degree, and betweenness centrality.\n\n\n\nNotably, discrepancies in embedding distances and clustering patterns across the networks imply a more expansive genetic influence on multimorbidity risk for females than males.\n\n\n\n\nFig 4. Heatmaps of edge sets across disease categories for our two sex-stratified DDNs. Brighter colors indicate more edges shared between disease categories.\n\n\n\nIn summary, our analysis affirms the presence of GxS interactions in cross-phenotype associations, emphasizing the continued need for investigation of the role of sex in disease onset and its importance in biomedical discovery and precision medicine research.\nThis manuscript is currently under review for publication.\n\nChapter 3. Translation\nExample chapter - An enhanced disease network with robust cross-phenotype relationships via variant frequency-inverse phenotype frequency.\n\nDDNs constructed from PheWAS data offer a unique ability to observe and evaluate associations between diseases from large-scale biomedical data.\nThese publications all follow a similar approach when constructing a DDN:\n\n(a) a single statistical significance level (p-value) is selected to determine associations between diseases and SNPs.\n(b) the links between diseases and SNPs are compressed into links between diseases to generate the DDN\n(c) a similarity metric such as cosine similarity is used to determine how similar two diseases are based on the number of shared associated SNPs\n\nThis process for constructing DDN seems straightforward, but it has the following limitations (see Figure 5):\n\n(a) the entire structure of the DDN can vary depending on the selection of significance level threshold in the PheWAS-driven complex relationship.\n(b) the effect of individual SNPs on the interactions across more than 2 diseases is masked\n(c) the chosen similarity metric can mask the exact amount of similarity between diseases\n\n\n\n\n\nFig 5. Overview of current approaches for constructing a DDN and their limitations.\n\n\n\nTo address the discussed limitations of previous approaches to developing DDNs, we proposed a new method inspired by natural language processing to generate networks from PheWAS data\nTaking inspiration from the NLP method “term frequency - inverse document frequency” (TF-IDF), we propose a new method we call “variant frequency - inverse phenotype frequency” (VF-IPF), which will weight the contributions of SNPs to disease associations. The outcome of this method presents itself as follows:\n\nIf a SNP is significant for only a few diseases, it is upweighted.\nIf a SNP is significant for many diseases, it is downweighted (similar to searching for the word “the” in a manuscript)\nIf a SNP is not significant for diseases, it is downweighted.\n\n\n\n\n\nFig 6. An overview of the VF-IPF algorithm\n\n\n\nTo test how the proposed method affects the way we represent cross-phenotype associations, we constructed an enhanced disease-disease network (eDDN) using UK biobank PheWAS summary statistics and tested the eDDN with three downstream tasks (see Figure 7), including:\n\nco-occurrence disease prediction when index disease of interest is given,\nnovel disease connection prediction, and\ntherapeutic drug prediction based on disease similarity.\n\n\n\n\n\n\nFig 7. Downstream tasks for the eDDN\n\n\n\nComparing our eDDN’s effectivness at predicting known disease comorbidities compared to other DDNs, we see that our eDDN has the highest AUC (i.e. it has the best performance).\n\n\n\n\nFig 8. The eDDN can predict disease comorbidities better than standard DDNs\n\n\n\nFurthermore, we see the utility of the eDDN in evaluating potential options for drug repurposing in the treatment of rheumatoid arthritis.\n\n\n\n\nFig 9. The eDDN can help with drug repurposing applications, suggesting alternative pre-existing treatments for rheumatoid arthritis.\n\n\n\nIn summary, we find that our proposed eDDN more effectively captures genetic associations between diseases from PheWAS data compared to previous approaches.\nThis manuscript is currently under review for publication.\n\n\nToday’s post was meant to give a sample of some of my work during my PhD. To read more about my currently published manuscripts, you can refer to my Google Scholar profile here.\nIn next week’s post, I will conclude this series on my PhD work with my personal takeaways from my program as well as tips for current, incoming, and aspiring PhD students, including selecting a program, selecting a thesis advisor, picking projects, and more! Until then~"
  },
  {
    "objectID": "posts/phd-takeaways/index.html",
    "href": "posts/phd-takeaways/index.html",
    "title": "My doctoral research: takeaways and advice",
    "section": "",
    "text": "In the last few posts, I have provided an overview of my PhD in Biomedical Informatics and Computational Genomics that I completed under the mentorship of Dr. Dokyoon Kim at the University of Pennsylvania Perelman School of Medicine. You can listen to a full presentation of my thesis defense here, and you can read the full text of my dissertation here. In today’s post, I will conclude this series on my doctoral research with my personal takeaways and tips for picking, pursuing, and finishing a PhD program."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#why-did-i-choose-to-pursue-a-phd-after-my-undergrad",
    "href": "posts/phd-takeaways/index.html#why-did-i-choose-to-pursue-a-phd-after-my-undergrad",
    "title": "My doctoral research: takeaways and advice",
    "section": "Why did I choose to pursue a PhD after my undergrad?",
    "text": "Why did I choose to pursue a PhD after my undergrad?\nAs I explained in my previous post describing my professional journey, I’ve always wanted to be a leader in impactful projects that made a clear benefit in people’s lives. When I was an undergrad, I began to research job opportunities that I found aligned with my interests. Looking at the qualifications required for these roles, and based on further advice from colleagues and mentors in my summer internships, I realized that in order for me to be able to become a leader in an interdisicplinary field in the future, I would have to pursue education beyond my undergraduate degree. Without a Master’s or PhD, I knew that I would eventually hit a wall in my career progression.\nI knew earning a graduate degree in the future after working for some time was a completely valid option. But, I also felt that knowing my own personality, it would be harder to bring myself back to school after a few years - I would feel the pay differential more keenly, and I would have to retrain myself to become a student. So, I decided to apply for graduate programs during my senior year of college.\nI considered both Master’s and PhD programs, but in weighing the opportunity costs for the computational fields I was considering, I ended up focusing my attention on doctorate degrees. I knew that a PhD would help me transition more clearly into cutting-edge research-based career opportunities. I also ideally wanted to avoid having to pay for a Master’s degree. A PhD program, on the other hand, would support me with a stipend for the duration of the program. Finally, I knew that with a computationally-focused PhD, it would be very reasonable to aim to graduate in about five years, which would be shorter than the average timeline for a PhD in the United States.\nI applied only for PhD programs where I knew I would feel content about spending 5+ years of my time. I also applied to one Master’s program in case I didn’t get in to any PhD programs. If nothing worked out for me, my back-up plan would have been to try to find a 1-year Master’s program attached to my undergrad or look for a short-term research position so that I could gain more experience and reapply fully for Master’s programs in the following year. Luckily, PhD admissions worked out, and I found a program that fit with everything I was looking for!"
  },
  {
    "objectID": "posts/phd-takeaways/index.html#why-did-i-apply-for-bioinformatics-programs",
    "href": "posts/phd-takeaways/index.html#why-did-i-apply-for-bioinformatics-programs",
    "title": "My doctoral research: takeaways and advice",
    "section": "Why did I apply for bioinformatics programs?",
    "text": "Why did I apply for bioinformatics programs?\nI chose to apply for bioinformatics / computational biology programs based on both my interest in the field as well as my likelihood of getting into such a department. I had always been motivated by the concepts of interdisclipinary research involving informational technology. So, I knew that I wanted to pursue a graduate degree related to data science, computer science, or statistics. At the same time, focusing on the area of biomedical research felt like an untapped market to me - there was so much data to work with and so much opportunity to advance the field. My time at Duke also gave me extensive exposure to research in the biomedical informatics domain. As a result, I felt that I would be a competitive applicant for cutting-edge programs in biomedical informatics and computational biology compared to other disciplines. I also felt that even if I wanted to pivot to a career that didn’t involve biomedical applications in the future, having a computational PhD would be sufficient to qualify me for such roles."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#why-did-i-pick-penn-genomics-and-computational-biology",
    "href": "posts/phd-takeaways/index.html#why-did-i-pick-penn-genomics-and-computational-biology",
    "title": "My doctoral research: takeaways and advice",
    "section": "Why did I pick Penn Genomics and Computational Biology?",
    "text": "Why did I pick Penn Genomics and Computational Biology?\nComing from a more computational background, I wanted a program that could help me catch up in topics in which I was lacking while also advancing further in my training for subjects where I already had the expertise. Penn GCB offered a very customized approach for selecting coursework, allowing me to take more foundational classes in genetics and molecular biology while pursuing more advanced curriculum in statistics and computer science.\nPenn also had multiple professors under whom I could see myself working, as well as access to interesting medical data due to its association with the University of Pennsylvania Health System.\nI also considered the happiness of students currently in the program and the livability of Philadelphia. It was apparent to me after my interview weekend that students were able to have fulfilling lives outside of their research, and that Philadelphia would be an exciting (and affordable) city for me to spend my twenties!\nLastly, I appreciated that Penn offered the opportunity to pursue a free Master’s degree in Statistics and Data Science from the Wharton School of Business concurrently with my PhD. Given my undergraduate degree in Statistics, I felt that this would be something I could more easily pursue, and that it would also give me a leg up in terms of foundational knowledge and branding in the future if I chose to pivot away from biomedical research.\n\n\n\nA view of my thinking face for all of my major life decisions during my PhD."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#how-did-i-choose-my-principal-investigator-pi",
    "href": "posts/phd-takeaways/index.html#how-did-i-choose-my-principal-investigator-pi",
    "title": "My doctoral research: takeaways and advice",
    "section": "How did I choose my Principal Investigator (PI)?",
    "text": "How did I choose my Principal Investigator (PI)?\nThe highest priority for me in picking my PhD lab was finding a group whose research spoke to me. After this, there were a few pieces that led me to settle on Dr. Dokyoon Kim.\nComing directly out of my undergrad, I knew that I would need a lot of support from my PI. So, I wanted to work with a more junior professor who would have the time to help me when I needed it. I also liked the idea of being one of the first students in a new lab and marking my own path. Based on my rotation, I could tell that my PI was highly attentive, and I had plenty of face-time with him each week, as well as support from post-docs and engineers in the lab whenever I needed it.\nFurther, I appreciated that my PI had a program set up with internationally-based clinicians to stay in Philadelphia and work in the lab each year. This gave countless opportunities to learn from and collaborate with people in the medical field as well as gain deeper insight into the impact that our projects could have downstream.\nLastly, it just so happened that my rotation with my PI coincided with March 2020, the start of the full impact of the COVID-19 pandemic on the U.S. In the face of entirely remote work for an unknown amount of time, it was immediately apparent to me that my PI would be great about supporting me virtually for however long we were required to work from home."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#how-did-i-choose-my-dissertation-topic",
    "href": "posts/phd-takeaways/index.html#how-did-i-choose-my-dissertation-topic",
    "title": "My doctoral research: takeaways and advice",
    "section": "How did I choose my dissertation topic?",
    "text": "How did I choose my dissertation topic?\nTo read more about the context and content of my dissertation, you can read my previous posts here and here.\nPersonally, I’ve always found that I excel the most when I am passionate about the projects I’m pursuing. It is important for me to not only see the motivators of my work, but also its downstream impact. As a result, I wanted to pursue a dissertation that felt intuitive and important.\nComing from a computational background, I did not have a disease area of interest that I had to focus on. Indeed, the biological question at hand was less important to me in my initial choice of project than its impact.\nLastly, I wanted the opportunity to familiarize myself with new types of data, to develop new computational methods and tools that could be used by the biomedical research community, and to see the translational impact of my work on people’s lives.\nMy rotation project (you can read the published manuscript for this project here) gave me an incredible view into the potential of my research trajectory at the start of my PhD. I loved how intuitive the baseline concepts of network medicine were, and I could see how it had the potential to bring together scientific storytelling aspects of data visualization with advanced technical research in graph-based machine learning. Ultimately, this dissertation topic felt like something that I could truly take full ownership of."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#my-personal-phd-pros",
    "href": "posts/phd-takeaways/index.html#my-personal-phd-pros",
    "title": "My doctoral research: takeaways and advice",
    "section": "My personal PhD pros",
    "text": "My personal PhD pros\n\nI gained strong interdisciplinary expertise in my subject matter:\n\nBiomedical informatics, computational genomics, translational science\nData science, software development, statistics, machine learning\n\nI learned how to conduct independent research and lead the direction of projects\nI gained valuable experience in mentoring and teaching others\nI was able to network with many amazing colleagues both in academia and industry in my discipline\nI was able to keep making income throughout the duration of my degree\nAfter my degree, I am taken more seriously by people in my field whom I meet for the first time\nI feel tremendously prepared to take on leadership roles in exciting interdisciplinary research areas in the future"
  },
  {
    "objectID": "posts/phd-takeaways/index.html#my-personal-phd-cons",
    "href": "posts/phd-takeaways/index.html#my-personal-phd-cons",
    "title": "My doctoral research: takeaways and advice",
    "section": "My personal PhD cons",
    "text": "My personal PhD cons\n\nHaving a PhD will make the jobs that you seek more niche\n\nWhen you pursue a PhD for career advancement, you’re typically seeking a career that is beyond the norm\nBachelor’s-level (and to an extent, Master’s-level) jobs are less individually unique from one another, but they are more broadly available (e.g. software engineer)\nFinding the right type of opportunity for a PhD-level individual requires patience\n\nPursuing a PhD is a stressful experience!\n\nIt can be hard to set boundaries between your work and your personal life. There are no clear deadlines for your projects either… your work will expand to fill your time unless you set your own pace\nA lot of luck is involved in how quickly you can make progress. There are so many factors out of your control that can affect the success of your experiments and your publications\n\nThere is a financial cost to pursuing a PhD\n\nIf you can be accepted to a PhD program, then you can be accepted to a much higher-paying job in industry. It is a very personal decision regarding whether or not this drop in salary is worth it"
  },
  {
    "objectID": "posts/phd-takeaways/index.html#choosing-to-do-a-phd",
    "href": "posts/phd-takeaways/index.html#choosing-to-do-a-phd",
    "title": "My doctoral research: takeaways and advice",
    "section": "Choosing to do a PhD",
    "text": "Choosing to do a PhD\nThe biggest piece of advice I have here is… make sure you’re doing a PhD for the right reason.\n\nIt’s important to think about why you want a graduate degree in the first place… Becoming a professor? Seeking those longer-term “extraordinary” opportunities in industry? Pure academic curiosity?\n\nI know people in all three of these camps, and I think they’re all very justified reasons. There are plenty more reasons to do a PhD outside of these - everyone has their own individual biases that draw them to the experience.\n\nThere are also plenty of reasons NOT to do a PhD.\n\nThe worst reason to pursue a PhD is for the “prestige.”\n\nIf you don’t find yourself intrinsically excited about the work you’re doing at the end of the day, then stop wasting your time! It’s not worth spending so much time on something just because you want other people to think more of you.\nA caveat - this lack of intrinsic excitement is different from joining a PhD program and then experiencing lulls in your research where you’re frustrated with your progress. This latter occurrence is totally normal and quite common! At the end of the day, when the experiments work out, you’ll remember why you started your program in the first place.\n\nThe second worst reason to pursue a PhD is that you don’t know what else to do with your time.\n\nGetting a PhD is not a passive experience. You cannot just “let it happen to you.” You have to be active about seeking out opportunities, making connections with others, and progressing on your work in order to succeed.\n\nIf you find yourself in either of these camps, then I can guarantee that you will be miserable and that you will burn out.\n\nIf you want to gain more knowledge, there are plenty of more lucrative / less time-intensive ways to do so than pursuing a PhD:\n\nPursue a different type of graduate degree\nFind a job relevant to your career\nJoin a technical bootcamp\nTake individual classes / online courses\nPursue independent projects (maybe even start a blog! ;))\n\nThere will always be a tradeoff when you decide to pursue a PhD. Some doors will open and other doors will close. Think about what’s best for YOU in your life and for your career.\n\n\n\n\nMy transition from first-year student to graduate. A pandemic and a doctorate degree will make you older AND wiser!"
  },
  {
    "objectID": "posts/phd-takeaways/index.html#selecting-a-program",
    "href": "posts/phd-takeaways/index.html#selecting-a-program",
    "title": "My doctoral research: takeaways and advice",
    "section": "Selecting a program",
    "text": "Selecting a program\nI’ve discussed earlier why I personally chose to attend Penn GCB, but here are some good questions to ask yourself when selecting a PhD program to attend:\n\nDo you like the work that you’ll be doing?\n\nThis is the whole point of a PhD - to do cutting-edge work. You need to like the field you’re in and the opportunities that will be available to you\n\nIs there more than one professor with whom you could see yourself working?\n\nEven if you have a professor who has committed to taking you on, this point is important to consider. Professors are people too, and they move around universities all the time. Make sure you’re not joining a program just because of a single person - otherwise, if they leave or if you position doesn’t work out, you may find yourself scrambling to find a new professor in the middle of your PhD who may not even align with your research interests.\n\nWill you have the right level of support for your background?\n\nSome people will come with a lot of experience and need less guidance when they start their PhDs. Others will come with minimal experience and need more hand-holding.\n\nI was in this second camp - I needed lots of hand-holding for biomedical concepts, and I wanted more independence in my explorations of computer science and statistics\n\nMake sure that the program can help you up-skill as needed (i.e. through coursework, registering for conferences / workshops, connecting you with the right mentors, etc.)\n\nHow is work-life harmony handled in this program?\n\nI cannot stress this enough - you are more than your work. You will need to work hard in a PhD, but you cannot let it absorb your entire life. You will burn out if you do. A PhD is a marathon and not a sprint.\nYou won’t necessarily need an active student community, but your peers are the only people who will truly understand the day-to-day of what being a PhD student means. You’ll find that being a part of such a community can be tremendously rewarding, and that your colleagues will be a huge help in your times of need.\nYou can gauge the status of how well work boundaries are set by your program through the students you meet during interview/admit visits. Obviously a lot of your work-life balance will come down to the lab you join. But in general, are the students happy with their choice to join this program? Do they have time for things outside of their work?\n\nDo you like where will you be living?\n\nDo you like the location of the program? Can you see yourself spending 5+ years there?\nIs the place you’re living affordable given the stipend that the program offers? How are housing/rental costs in the area?\nIs it easy to get to work? If not, how often will you expect to be coming in to campus?\n\nHow does the program support career progression and what are the types of opportunities that may be available to you after graduation?\n\nWhat kinds of support systems does the program have for career development?\n\nIs there support for internships? Fellowships / grants? Travel opportunities to conferences? Mentorship / teaching opportunities? The ability to earn additional certificates / degrees?\n\nWhat do alumni from the program usually do after graduation? Where (physically) do they end up? How much did the program and/or their research focus help with finding a job?\nIn these situations, when you have an admission to a program and you’re trying to decide on it, it’s great to speak to not only current students but also alumni to get a sense for the pros and cons of the program. You may not know the right questions to ask during these informational interviews, but if the program has a good community, then they’ll be happy to help you out regardless."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#selecting-a-thesis-advisor",
    "href": "posts/phd-takeaways/index.html#selecting-a-thesis-advisor",
    "title": "My doctoral research: takeaways and advice",
    "section": "Selecting a thesis advisor",
    "text": "Selecting a thesis advisor\n\nYour PhD advisor doesn’t have to be your best friend, but you should ideally have a friendly relationship with them :) Are they a nice person to work with? Do they have your best interests at heart? Or are they more concerned with using your time and work to advance the standing of their lab?\nDo you like the work that you’d be doing with this PI? It’s important that you don’t pick a lab just because you like the mentor’s personality.\nPick a PI who will lift you up rather than hold you down\n\nYou want to be challenged, but you don’t want to make life harder for yourself. Your PI is directly responsible for:\n\nthe type of research you explore\nwhen you graduate\nhow much work-life balance you have\n\nAt the end of the day, a great PI should always be your advocate!\n\nRegarding co-mentorships…\n\nCo-mentorships across two groups can be great if you have the right projects in mind and need both labs’ expertise.\nHowever, these can also go very poorly if your work aligns more with one group than the other. There’s a high likelihood that you will fall between the cracks and be stuck in your PhD for much longer than you need to be.\n\nSometimes it’ll still work out, especially if both PI’s already collaborate. But in these cases, I personally think it seems unnecessary to have both professors be your mentor. You can always have one of these professors serve on your thesis committee instead.\n\nHere’s my personal opinion… If you are coming in with less experience, I would advise picking a single PI. If you are coming in with more experience and know exactly what type of dissertation you want to work on, then you can consider multiple PIs."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#picking-research-projects",
    "href": "posts/phd-takeaways/index.html#picking-research-projects",
    "title": "My doctoral research: takeaways and advice",
    "section": "Picking research projects",
    "text": "Picking research projects\n\nHere’s a big “duh” piece of advice – pick a research topic you’re excited about! Why would you spend 5+ years of your life on something that doesn’t get you excited?\nAnother point - prioritize skill-building when you can, but don’t prioritize it over progress on your dissertation.\n\nYou can up-skill in specific areas as much as you want after your PhD. If you are distracted by “side-quests,” you will take longer to graduate when you could have instead finished your degree earlier and kept progressing in your career.\n\nThink about the trade-off between your academic passions and the logistics of your work. Try to find the optimal balance across academic curiosity, skill-building, and time required for the research project.\n\nData generation will always take longer than expected. The easiest way to cut down on the time needed for your PhD is to work on projects where the data are already generated :)"
  },
  {
    "objectID": "posts/phd-takeaways/index.html#wrapping-up-your-phd",
    "href": "posts/phd-takeaways/index.html#wrapping-up-your-phd",
    "title": "My doctoral research: takeaways and advice",
    "section": "Wrapping up your PhD",
    "text": "Wrapping up your PhD\n\nIn my personal opinion, ending a PhD is an exercise in self-confidence and believing in oneself. Ultimately, completing your PhD means knowing how to advocate for yourself.\n\nThis can be easier or harder depending on your PI and your thesis committee. Some PIs / committees will be on the same page as you. They may even say themselves that it’s time for you to defend.\nOthers will not tell you they think you are ready to leave. It is up to you to justify in your committee meetings why you feel qualified to defend and graduate.\n\nHere’s my biggest indicator for when it’s time to graduate - when you no longer feel that you NEED guidance from your superiors.\n\nYou don’t need to feel that you have nothing more to learn. In fact, pursuing a PhD will teach you that you always have more to learn!\nYou don’t even have to have fulfilled all the goals of your dissertation… the aims that you come up with at the start of your disseration are somewhat arbitrary benchmarks.\nInstead… Are you able to devise a full research project concept and methodology? Can you procure the right data and follow through on the analysis? Are you able to communicate your results in a cogent, impactful manner?\n\nI hit a point toward the end of my PhD where I felt I could still keep learning and exploring, but I was coming up with all of the directions of the exploration myself. In other words, I was an independent researcher! This was my cue to wrap up and defend.\n\n\nWith that, we’ve reached the conclusion of my series on my doctoral research! If you’ve read this far, I hope you found the information I shared to be useful. The process of picking, starting, and completing a PhD is a tremendous challenge, and if you’re struggling at any point with any of the concepts I’ve covered today, feel free to reach out to me on LinkedIn or shoot me an email at vivek.sriram@gmail.com! I am always happy to chat and offer my two cents.\nAnd a last reminder… as I’ve said earlier in this post - you are not your work! Regardless of the stressors and major decisions that surround you, never forget to remember what matters most at the end of the day: your personal happiness and well-being. Make sure to take time to enjoy the little things in life, like this squirrel :)\n\n\nImage References:\n\nParamount Plus\nMemebase.com\nBusiness Insider"
  },
  {
    "objectID": "posts/karpathy-1/index.html",
    "href": "posts/karpathy-1/index.html",
    "title": "A Python introduction to neural networks and backpropagation",
    "section": "",
    "text": "This is a walkthrough of Andrej Karpathy’s video “The spelled-out intro to neural networks and backpropagation: building micrograd”. This video is the first in his YouTube series, “Neural Networks: Zero to Hero.”\n\n\n\nA screen-grab of Andrej’s video\n\n\nNeural networks are mathematical models used to represent nodes and the signals they send to one another through their links. Neural networks replicate the structure of the brain, where interconnected neurons send messages to each other through electric signals across the synpases that bridge them together. While individual nodes can perform only simple operations, many nodes connected together in a network can perform complex computational tasks.\nThe general structure of a neural network for machine learning includes three types of nodes, grouped into different “layers” within the neural network. The first set of nodes are the input nodes, corresponding to input (or training data). The second set of nodes refer to intermediate (hidden) nodes. The final type of node is the output node, corresponding to the result of processing the input data through the hidden nodes. This output layer typically comes in the form of a loss function, characterizing the difference between the output of the model and the expected output. The goal of optimizing a neural network is to minimize this output loss to best match the behavior of the input data.\n\n\n\nThe basic format of a neural network\n\n\nBackpropagation is an algorithm for supervised learning of neural networks using gradient descent. This method will calculate the gradient of each intermediate node in the network with respect to the loss function, allowing us to iteratively tune their weights to minimize the overall loss.\nIn his video tutorial, Andrej shows how to construct a neural network from scratch and perform backpropagation on it to optimize the weights of the network. The code presented in this example is a direct copy of the code walked through in the video, streamlined a bit for interpretation. Writing this blog post helped me solidify my understanding of the material (and also helped me practice writing Python code in Quarto :) ). I would highly recommend following along with this tutorial and further videos for a hands-on, ground-up exploration of neural networks and language models! I aim to work through his other tutorials in the future as well.\nWith background out of the way, let’s get started~\n```{r}\nlibrary(reticulate)\nuse_python('/opt/anaconda3/bin/python')\n```\n\n# Import required packages\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n\nDefining functions and manually calculating derivatives\nWe can start by thinking about a simple mathematical expression to give us some intuition behind the workings of individual neurons.\nLet’s define a scalar value function f(x) that takes scalar input and returns scalar output. We can apply this function to a single value or a range of values.\n\n# e.g. scalar value function that takes scalar input and returns scalar output\ndef f(x):\n    return 3*x**2 - 4*x + 5\n\n# e.g. single value\nf(3.0)\n\n# e.g. range of values\nxs = np.arange(-5, 5, 0.25)\nys = f(xs)\nys\n\narray([100.    ,  91.6875,  83.75  ,  76.1875,  69.    ,  62.1875,\n        55.75  ,  49.6875,  44.    ,  38.6875,  33.75  ,  29.1875,\n        25.    ,  21.1875,  17.75  ,  14.6875,  12.    ,   9.6875,\n         7.75  ,   6.1875,   5.    ,   4.1875,   3.75  ,   3.6875,\n         4.    ,   4.6875,   5.75  ,   7.1875,   9.    ,  11.1875,\n        13.75  ,  16.6875,  20.    ,  23.6875,  27.75  ,  32.1875,\n        37.    ,  42.1875,  47.75  ,  53.6875])\n\n\nWe can plot the output of our function to see the association between our input and output as well.\n\nplt.plot(xs, ys)\n\n\n\n\n\n\n\n\nDetermining the derivative of f would let us identify inflection points in our data. Let’s calculate the derivative of f at 3 (i.e. f’(3)) numerically using the fundamental law of calculus:\n\\[\n\\lim_{h\\to\\infty} \\frac{f(x+h)-f(x)}{h}\n\\]\n\nh = 0.0000000001\nx = 3.0\n\n(f(x+h) - f(x))/h\n\n14.000001158365194\n\n\nNote that if our h is too small for Python, we will end up with a floating point error. With some trial and error for different values of h, we can see f’(3) = 14\nNow let’s make a function that is a little more complicated: \\[\nd(a, b, c) = a*b + c\n\\]\n\na = 2.0\nb = -3.0\nc = 10.0\nd1 = a*b + c\n\nAgain, we can calculate the derivative of d. This time, since we have three inputs, we have to pick a variable with respect to which we calculate the derivative. Let’s numerically calculate the derivative of d with respect to a.\n\nh = 0.0000001\n\n#derivative wrt a\na += h\n\nd2 = a*b + c\n\nprint('d1', d1)\nprint('d2', d2)\nprint('slope', (d2-d1)/h)\n\nd1 4.0\nd2 3.9999997\nslope -2.9999999995311555\n\n\nWe can do the same with respect to b as well.\n\n#derivative wrt b\nb += h\na = 2.0\n\nd2 = a*b + c\n\nprint('d1', d1)\nprint('d2', d2)\nprint('slope', (d2-d1)/h)\n\nd1 4.0\nd2 4.0000002\nslope 1.9999999967268423\n\n\nWe now have some intuition for how functions and derivatives work.\n\n\nThe ‘Value’ Class\nLet’s define a class “Value” to store the individual values that come together to make a function / mathematical expression. Each ‘Value’ can be thought of as a node in a neural network.\n\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None #default: nothing\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    # Nicer looking way to see what the value actually is instead of an object\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __radd__(self, other): # other * self\n        return self + other\n        \n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other): # other * self\n        return self * other\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n        out = Value(t, (self, ), 'tanh')\n        def _backward():\n            self.grad +=  (1 - t**2) * out.grad\n        out._backward = _backward\n        return out\n\n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self, ), 'exp')\n\n        def _backward():\n            self.grad += out.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data**other, (self,), f'**{other}')\n\n        def _backward():\n            self.grad += other * (self.data**(other-1)) * out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __truediv__(self, other): #self / other\n        return self * other**-1\n\n    def __neg__(self): # -self\n        return self * -1\n\n    def __sub__(self, other): # self - other\n        return self + (-other)\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        \n        build_topo(self)\n        \n        # call _backward() in the right topological order\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\nWe can see how to perform mathematical operations using our Value class:\n\na = Value(2.0)\nb = Value(4.0)\na-b\n\nValue(data=-2.0)\n\n\nNow let’s define an example function L that makes use of our Value class:\n\\[\nL(a, b, c, f) = (a*b + c)*f\n\\]\n\na = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label = 'e'\nd = e + c; d.label = 'd'\nf = Value(-2.0, label = 'f')\nL = d * f; L.label = 'L'\nL\n\nValue(data=-8.0)\n\n\nBased on the code in our Value class, we are able to see for each node which nodes came before it and what the operation was to generate the current node.\n\nd._prev\nd._op\n\n'+'\n\n\nWe can also define a function ‘draw_dot’ to be able to visualize the components of our function. Here, we build out a graph using the GraphViz API. We then iterate over all nodes and create corresponding nodes and edges (including values and operations as different node types in our network).\n\nfrom graphviz import Digraph\n\ndef trace(root):\n    # builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) #LR = left to right\n\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        # for any value in the graph, create a rectangular ('record') node for it\n        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape = 'record')\n        if n._op:\n            # if this value is a result of some operation, create an op node for it\n            dot.node(name = uid + n._op, label = n._op)\n            # and connect this node to it\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        # connect n1 to the op node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n\n\ndraw_dot(L)\n\n\n\n\n\n\n\n\n\n\nManual backpropagation example\nWith our basic function L now represented as a network of values and operations, let’s perform manual backpropagation.\nWe’ll start from L and work backwards, taking the derivative with respect to L at each intermediate value. This exercise is equivalent to determining the derivative of an output L with respect to the internal weights of a neural network.\n\n# Let's calculate gradient of L wrt a manually using the fundamental theorem of calculus\n# (f(x+h) - f(x))/h\ndef lol():\n    h = 0.001\n    \n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label = 'f')\n    L = d * f; L.label = 'L'\n    L1 = L.data\n\n    a = Value(2.0 + h, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    e = a*b; e.label = 'e'\n    d = e + c; d.label = 'd'\n    f = Value(-2.0, label = 'f')\n    L = d * f; L.label = 'L'\n    L2 = L.data\n\n    print((L2-L1)/h)\n\n\n#dL/da\nlol()\n\n6.000000000000227\n\n\nWe can go through this entire network structure and set the gradients for each node with respect to L.\n\n#We know dL/dL = 1\nL.grad = 1\n\n\n#L = d*f\n#So dL/df = d\n#and dL/dd = f\n\nf.grad = 4.0 # this is just the value of d\nd.grad = -2.0 # this is just the value of f\n\n\n# what is dL/dc?\n# We can use dL/dd and dd/dc and apply the chain rule\n# dL / dc = (dL/dd) * (dd/dc) = -2*1 = -2\n# dL/de is the same, -2\nc.grad = -2.0 # this is just the value of d\ne.grad = -2.0 # this is just the value of f\n\n\n# dL/da = dL/de * de/da = -2*b = -2*-3 = 6\n# dL/db = dL/de * de/db = -2*a = -2*2 = -4\na.grad = 6.0 # this is just the value of d\nb.grad = -4.0 # this is just the value of f\n\n\ndraw_dot(L)\n\n\n\n\n\n\n\n\nHere is our key takeaway from this example:\nBackpropagation is just the recursive application of the chain rule backwards through the computational graph of your neural network.\n\n\nIntroducing an activation function.\nIn our previous example, we had an output L that could take on any value. Now let’s make use of the hyperbolic tangent (\\(tanh\\)) activation function to limit our output to a range of -1 to 1.\n\\(Tanh\\) looks as follows:\n\n#Squashing/activation function - tan(h)\nplt.plot(np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2)));\nplt.grid();\n\n\n\n\n\n\n\n\nLet’s define a new function \\(o = tanh(x1*w1 + x2*w2 + b)\\)\n\n# inputs x1, x2\nx1 = Value(2.0, label = 'x1')\nx2 = Value(0.0, label = 'x2')\n\n# weights w1, w2\nw1 = Value(-3.0, label = 'w1')\nw2 = Value(1.0, label = 'w2')\n\n# bias of the neuron (crazy bias makes clean output in this example)\nb = Value (6.881373587019542, label = 'b')\n\n#x1*w1 + x2*w2 + b\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\nx1w1x2w2 = x1w1 + x2w2;\nx1w1x2w2.label = 'x1*w1 + x2*w2'\n\n# n is our cell body activation without the activation function\nn = x1w1x2w2 + b;\nn.label = 'n'\n\n# Apply activation function (defined in Value class earlier)\no = n.tanh(); o.label = 'o'\n\nHere is the network that represents the function we just defined:\n\ndraw_dot(o)\n\n\n\n\n\n\n\n\nWe care most about the derivative of o with respect to the weights w1 and w2. In a normal neural network, we would have many more input and intermediate nodes (not just the two as in this example). We will calculate the gradients for this network by hand.\n\no.grad = 1.0\n\n# o = tanh(n)\n# do/dn = 1-tanh^2(n) = 1 - o^2\nn.grad = 1-o.data**2\n\n# do/db = do/dn * dn/db = (1-o^2)*1 = 1-o^2\n# d(x1w1x2w2)/db = do/dn * dn/d(x1w1x2w2) = (1-o^2)*1 = 1-o^2\nx1w1x2w2.grad = 1-o.data**2\nb.grad = 1-o.data**2\n\n# same logic of back-propagation wrt '+'\nx1w1.grad = 1-o.data**2\nx2w2.grad = 1-o.data**2\n\n#do/dx2 = w2 * do/d(x2w2)\nx2.grad = w2.data * x2w2.grad\n#do/dw2 = x2 * do/d/(x2w2)\nw2.grad = x2.data * x2w2.grad\n\n# same logic as for x2/w2\nx1.grad = w1.data * x1w1.grad\nw1.grad = x1.data * x1w1.grad\n\n\ndraw_dot(o)\n\n\n\n\n\n\n\n\nSo, because w1’s gradient is positive, if we want this neuron’s output to increase, then we should increase w1. w2 doesn’t affect the output of this function because its gradient is 0.\n\n\nAutomating backpropagation\nLet’s stop doing this back-propagation manually! Take a look at the logic for _backward and backwardin the Value class to see how we handle this (we apply a topological sort to our data in the backward function). We also ensure that we never call _backward on a node before we’ve called it on its children. Lastly, we make sure that we accumulate gradients in the backward function.\n\no.grad = 1.0\n\no._backward()\nn._backward()\nb._backward()\nx1w1x2w2._backward()\nx2w2._backward()\nx1w1._backward()\n\n\ndraw_dot(o)\n\n\n\n\n\n\n\n\n\no.backward()\ndraw_dot(o)\n\n\n\n\n\n\n\n\n\na = Value(3.0, label = 'a')\nb = a+a; b.label = 'b'\nb.backward()\ndraw_dot(b)\n\n\n\n\n\n\n\n\nEverything works! Yay!\n\n\nBreaking up tanh into its individual components\nInstead of using a \\(tanh\\) function in our Value class, we can break it up into exponent and division functions to see an example of a more complicated network.\n\n# inputs x1, x2\nx1 = Value(2.0, label = 'x1')\nx2 = Value(0.0, label = 'x2')\n\n# weights w1, w2\nw1 = Value(-3.0, label = 'w1')\nw2 = Value(1.0, label = 'w2')\n\n# bias of the neuron\nb = Value (6.881373587019542, label = 'b')\n\n#x1*w1 + x2*w2 + b\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\nx1w1x2w2 = x1w1 + x2w2;\nx1w1x2w2.label = 'x1*w1 + x2*w2'\n\n# n is our cell body activation without the activation function\nn = x1w1x2w2 + b;\nn.label = 'n'\n\n# Apply activation function (defined in Value class earlier)\ne = (2*n).exp()\no = (e-1)/(e+1)\no.label = 'o'\no.backward()\n\n\ndraw_dot(o)\n\n\n\n\n\n\n\n\nAs we can see, even after breaking our tanh function into its individual components, our forward and backward passes are still correct! Note that the level at which you perform your individual operations is entirely up to you (e.g. tanh vs. its individual components). All that matters is that you have input and output and that you can do forward/backward passing of your operations.\n\n\nBackpropagation with PyTorch\nNow that we’ve developed backpropagation manually, let’s see how it can be performed in PyTorch. With PyTorch, everything is based around tensors rather than scalars.\n\nimport torch\n\n# Cast to double to get 64bit precision\nx1 = torch.Tensor([2.0]).double()\n# by default, pytorch will say leaf nodes don't have gradients to improve efficiency\nx1.requires_grad = True\n\nx2 = torch.Tensor([0.0]).double()\nx2.requires_grad = True\n\nw1 = torch.Tensor([-3.0]).double()\nw1.requires_grad = True\n\nw2 = torch.Tensor([1.0]).double()\nw2.requires_grad = True\n\nb = torch.Tensor([6.8813735870195432]).double()\nb.requires_grad = True\n\nn = x1*w1 + x2*w2 + b\no = torch.tanh(n)\n\n# PyTorch tensors have data and grad elements\nprint(o.data.item())\n# PyTorch has a backward function too\no.backward()\n\nprint('---')\nprint('x2', x2.grad.item())\nprint('w2', w2.grad.item())\nprint('x1', x1.grad.item())\nprint('w1', w1.grad.item())\n\n0.7071066904050358\n---\nx2 0.5000001283844369\nw2 0.0\nx1 -1.5000003851533106\nw1 1.0000002567688737\n\n\nPyTorch makes all of our calculations much more efficient. We can do all of these operations in parallel with very large tensors and not just scalar values.\n\n\nA simple neural network\nWe’ve had enough fun with “neural network adjacent” mathematical expressions and their corresponding computational topologies.\nLet’s implement a simple neural network. We will base this off of a multilayer perceptron (MLP). We can define a Neuron class, Layer class, and MLP class for our network.\nA typical neural network neuron looks like the following:\n\n\n\nA typical neuron in a neural network\n\n\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n        self.b = Value(random.uniform(-1,1))\n        \n    # Python goes to __call__ when you use the class as a function\n    def __call__(self, x):\n        # w.x + b\n        # start with self.b, add the dot product of w and x\n        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n        out = act.tanh()\n        return out\n\n    def parameters(self):\n        return self.w + [self.b]\n\n\nclass Layer:\n    # nout is the size of the output of the layer\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        outs = [n(x) for n in self.neurons]\n        return outs[0] if len(outs) == 1 else outs\n\n    def parameters(self):\n        params = []\n        for neuron in self.neurons:\n            ps = neuron.parameters()\n            params.extend(ps)\n        return params\n        \n        # Same as:\n        # return [p for neuron in self.neurons for p in neuron.parameters()]\n\n\nclass MLP:\n    # nouts is the list of layer sizes we want\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\nBased upon our defined classes, let’s initialize our MLP.\n\nx = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\nn(x)\n\nValue(data=0.4944649312890593)\n\n\n\ndraw_dot(n(x))\n\n\n\n\n\n\n\n\nWow, our function is much crazier than our initial examples! Obviously we’re never going to manually backpropagate such an example… let’s have PyTorch do it for us.\nWe start by defining some sample input data and our desired targets. We then use our baseline MLP to calculate model outputs from the input data.\n\n# Example data\nxs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0],\n    [1.0, 1.0, -1.0]\n]\n\nys = [1.0, -1.0, -1.0, 1.0] #desired targets\n\n# Apply our MLP to predict y from x\nypred = [n(x) for x in xs]\nypred\n\n[Value(data=0.4944649312890593),\n Value(data=0.40977958134154474),\n Value(data=-0.4050151100259451),\n Value(data=0.3923524132012742)]\n\n\nWe can compare our model outputs to the expected outputs using a loss function such as mean squared error (MSE).\n\n# loss will measure how good our neural net is\n# let's do mean squared error\nloss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\nloss\n\nValue(data=2.96628678270387)\n\n\nNow let’s backpropagate (automatically this time)!\n\nloss.backward()\n\nIf the gradient of a weight is positive, then decreasing the weight will decrease the overall loss. Similarly, if the gradient is negative, then increasing the weight will decrease the loss.\n\n# If this gradient is positive, then decreasing this weight will decrease our loss\n# If this is negative, then increasing this weight will decrease our loss\nn.layers[0].neurons[0].w[0].grad\n\n-1.2640701291368466\n\n\n\nn.layers[0].neurons[0].w[0].data\n\n-0.9567063145203327\n\n\nFor every parameter in our neural network, let’s change the weights slightly to reduce the overall loss. We increase the weight for negative gradients and decrease the weight for positive gradients.\n\n# for every parameter in our neural net, let's change the weights slightly to reduce the loss\n# increase for negative grad, decrease for positive grad\nfor p in n.parameters():\n    p.data += -0.01*p.grad\n\nOur overall loss should have gone down a bit now. Let’s recalculate it.\n\nypred = [n(x) for x in xs]\nloss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\nloss\n\nValue(data=2.6726876872891854)\n\n\n\n# Propagate\nloss.backward()\n\n\nypred\n\n[Value(data=0.4993403271200778),\n Value(data=0.3198034181833698),\n Value(data=-0.45493607758790167),\n Value(data=0.38108818311718595)]\n\n\nNice, we’re able to train our data better now. Let’s formalize this process of updating gradients in a loop. This is the same thing as “stochastic gradient descent”.\n\n# Reset the neural net\nx = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\nn(x)\n\nValue(data=0.34934876482956906)\n\n\n\n# Initialize input data and desired targets\nxs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0],\n    [1.0, 1.0, -1.0]\n]\n\nys = [1.0, -1.0, -1.0, 1.0]\n\n\n# 20 iterations\nfor k in range(20):\n    # forward pass\n    ypred = [n(x) for x in xs]\n    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n\n    # backward pass\n    for p in n.parameters():\n        p.grad = 0.0\n    loss.backward()\n\n    # update\n    # \"stochastic gradient descent\"\n    for p in n.parameters():\n        p.data += -0.05 * p.grad\n\n    print(k, loss.data)\n\n0 8.493929671336291\n1 6.992068662805386\n2 5.769077247651545\n3 4.4735019295768295\n4 3.989548688723282\n5 3.7147609186944743\n6 3.405667943207894\n7 2.8620742530053116\n8 1.7745806845463523\n9 0.7698861574003086\n10 0.4457449169022083\n11 0.30780024751796564\n12 0.23273878356815986\n13 0.18590290331973824\n14 0.15408948967655905\n15 0.13116783996889828\n16 0.11392081179247492\n17 0.10050481438071963\n18 0.08979065153648244\n19 0.08104964070586138\n\n\n\nypred\n\n[Value(data=0.9034116586940597),\n Value(data=-0.9567970932148172),\n Value(data=-0.8069423147548922),\n Value(data=0.8194935678632469)]\n\n\nTa-da! We now understand the intuition behind developing simple neural networks and performing backpropagation to improve their predictive performance!\n\n\nTakeaways and summary\nNeural nets are simple mathematical expressions that take input data and weights. Working with neural networks involves a forward pass of input data followed by the application of a loss function.\nThe goal of a neural network for machine learning is to minimize the output loss to get the model to better predict desired targets. Backpropagation can be applied from the loss function to determine the gradients of the intermediate weights of the network. We can then tune the weights of these nodes against the gradient (i.e. gradient descent) to improve the predictive performance of the model.\nSimulating a blob of neural tissue in this manner can handle all sorts of interesting problems. Generative Pre-trained Transformers (GPTs) uses massive amounts of text from the internet and then predict the next words in a sentence based on context. These are really just fancy neural networks with hundreds of billions of parameters. Different models may use different loss functions and different methods for gradient descent, but the underlying concepts are all consistent.\nThis concludes my walkthrough of Andrej’s first neural networks video tutorial. Until next time, [VS]Coders!"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching and Mentorship",
    "section": "",
    "text": "Teaching\n\nTeaching Assistant – “Introduction to Python”, 2024 to Present\n\nFred Hutch Cancer Center Data Science Lab, Seattle, WA\n\nTeaching Assistant – “Introduction to R”, 2024 to Present\n\nFred Hutch Cancer Center Data Science Lab, Seattle, WA\n\nGuest Lecturer – “Databases and Data Integration”, 2024 to Present\n\nUniversity of Pennsylvania Department of Biostatistics, Epidemiology, and Informatics, Philadelphia, PA\n\nVolunteer teacher and instructor, 2019 to 2023\n\nScience Education Academy outreach program, Philadelphia, PA\n\nGraduate Teaching Assistant – “Databases and Data Integration”, 2022 to 2023\n\nUniversity of Pennsylvania Department of Biostatistics, Epidemiology, and Informatics, Philadelphia, PA\n\nGraduate Tutor – “Machine Learning”, 2022\n\nUniversity of Pennsylvania Department of Computer and Information Science, Philadelphia, PA\n\nGraduate Mentor and Lecturer - “Summer Computational Biology Preview”, Summer 2021\n\nUniversity of Pennsylvania Genomics and Computational Biology Graduate Group, Philadelphia, PA\n\nGraduate Teaching Assistant - Python Bootcamp, Summer 2020\n\nUniversity of Pennsylvania Genomics and Computational Biology Graduate Group, Philadelphia, PA\n\nUndergraduate Teaching Assistant – “Intro to Computational Genomics”, 2017 to 2018\n\nDuke University Department of Computer Science, Durham, NC\n\n\nMentorship\n\nUPenn Biomedical Graduate Studies Career Paths Mentor (2024 - Present)\nAHLI Machine Learning for Health (ML4H) Mentor (2024 - Present)\nRotation Advisor for Alexis Garófalo, UPenn GCB PhD Student (2022)\nRotation Advisor for Jakob Woerner, UPenn GCB PhD Student (2021)\nRotation Advisor for Anni Moore, UPenn GCB PhD Student (2021)"
  },
  {
    "objectID": "conferences.html",
    "href": "conferences.html",
    "title": "Presentations and Talks",
    "section": "",
    "text": "Talks\n\n2023 - Public Thesis Defense, University of Pennsylvania Genomics and Computational Biology Graduate Group\n\n“Dissecting the genetic architecture of disease multimorbidities through the graph-based analysis of EHR-linked biobanks”\n\n2023 - Invited Speaker, University of Pennsylvania Institution for Biomedical Informatics Annual Retreat\n\n“From network medicine to precision medicine: AI methods for the investigation of genetic contributors to disease multimorbidities”\n\n\nPresentations\n\n2023 - Poster Presenter, American Society for Human Genetics Annual Conference\n\n“The interplay of sex and genotype in disease associations: a comprehensive network analysis in the UK Biobank”\n\n2022 - Poster Presenter (Reviewer’s Choice), American Society for Human Genetics Annual Conference\n\n“A genetic correlation disease-disease network (gcDDN) for the improved identification of novel phenotype relationships”\n\n2021 - Poster Presenter, Mid-Atlantic Bioinformatics Conference\n\n“Polygenic Score Pipeline (PiPn): a Toolkit for Automated Polygenic Score Analysis using the PGS Catalog”\n\n2021 - Poster Presenter, American Society for Human Genetics Annual Conference\n\n“The identification of comorbidity risk via disease-disease network: an application to pre-eclamptic women in the UK Biobank”\n\n2020 - Poster Presenter, American Society for Human Genetics Annual Conference\n\n“NETMAGE: a human disease phenotype map generator for the visualization of phenome-wide association study results”"
  },
  {
    "objectID": "posts/podcasts/index.html",
    "href": "posts/podcasts/index.html",
    "title": "My podcast recommendations",
    "section": "",
    "text": "Welcome back to another edition of [VS]Codes! In this week’s post, I’ll be summarizing a list of my top podcasts, including content that covers the biomedical informatics and digital health space, statistics and data science, and a couple of miscellaneous topics as well. Without further ado, let’s get started!\n\n\nBiomedicine / Health AI\n\nThe AI Health Podcast, hosted by Pranav Rajpurkar and Adriel Saporta\n“The AI Health Podcast” was my first foray into the realm of podcasts covering technical topics - with the demands of my doctoral research and my limited time, I was finding it challenging to keep up with the plethora of advances that were concurrently occurring in medicine with the expanded use of AI. Pranav and Adriel do a great job of synthesizing complex topics across the field of health AI, explaining both the biomedical context as well as the impact of the methodologies being developed. I appreciate how each episode is separated into a section covering the context of the topic followed by an interview with an expert. The experts interviewed also come from a variety of backgrounds in both academia and industry, offering diverse perspectives on artificial intelligence and biomedicine.\nGround Truths, hosted by Eric Topol\nEric Topol is a giant in the field cardiovascular medicine, At the same time, he is a keen proponent of expanding the applications of AI to medicine. In this podcast, he interviews leading experts on a variety of topics in biomedicine and health science. I appreciate how the pace of his discussions synthesizes complex areas into very digestible conversations, and I don’t think any other host has the same ability as Eric to bring such esteemed guests to the table so easily!\nThe Pulse, hosted by Wharton Digital Health\nI learned about “The Pulse” from other Wharton students while I was at the University of Pennsylvania. This podcast is a great way to gain a broad perspective of the field of health technology from a commercial perspective - the companies and interviews vary tremendously from episode to episode, and I found these episodes particularly helpful while I was in academia and looking to learn more about opportunities in the health technology space in industry. Given that the series comes from the Wharton School, there is a focus in some episodes on the business / MBA side of health tech, but much of the content is still relevant to broad audiences looking to learn more about the landscape of the field.\n\nData Science / Statistics\n\nThe Data Chief, hosted by Cindi Howson\nHow does one become a “Chief Data Officer”? What does it take to manage the data infrastructure of an organization? How does one continue to support a culture of innovation and progress from such a high-level position? “The Data Chief” answers these questions and more through interviews with leaders across a variety of industries. I appreciate how this podcast exposes listeners to the importance of well-maintained data ecosystems across practically any application area one can think of.\nCasual Inference, hosted by Lucy D’Agostino McGowan and Ellie Murray\nA play on the field of “causal inference,” this podcast covers a variety of topics in epidemiology, statistics, data science, causal inference, and public health. I am a big fan of the hosts’ mantra of “keeping it casual” by requiring guests to explain their subject matter expertise as simply as they can!\nBuild a Career in Data Science, hosted by Jacqueline Nolis and Emily Robinson\nThis podcast serves as an accompaniment to Jacqueline and Emily’s amazing book “Build a Career in Data Science,” but there is absolutely no need to have read the book to follow along! Jacqueline and Emily spend each episode on a chapter from their book, covering practical advice for how to succeed in the data science industry. Topics include how to interview for a data science job, how to communicate with stakeholders, how to pick the right position for you, and more!\nNot So Standard Deviations, hosted by Roger Peng and Hilary Parker\nAs I transitioned from the biomedical focus of my dissertation research to broader data science applications, I wanted to find a podcast that could expose me to the gamut of statistics, machine learning, data science. “Not So Standard Deviations” fits this bill to a tee, including discussions of topics in both data science and tech while making time for fun tangents and detours depending on the news of the day.\n\nMiscellaneous\n\nThe Happiness Lab, hosted by Laurie Santos\nLaurie Santos is a professor of psychology at Yale and has spent her career studying the evolutionary origins of human cognition and the science of happiness. In this podcast, she considers a variety of questions in the happiness space using a science-based approach, interviewing a plethora of guests across the field of human cognition. I’ve found the way I think about how I approach each of my days has notably shifted since finding this podcast!\n“Factually!”, hosted by Adam Conover\nI first learned about Adam Conover from watching his television show “Adam Ruins Everything,” where he reveals hidden truths about a variety of topics that are commonly misunderstood by the public using a comedic lens. “Factually!” continues this tradition of irreverently diving deeper through interviews with experts that cover every topic imaginable, whether it be the ethics of artificial intelligence or “your houseplants can think!”\n\n\nThis concludes my blog post for today. Until next week!"
  },
  {
    "objectID": "posts/cascadia-conf-24/index.html",
    "href": "posts/cascadia-conf-24/index.html",
    "title": "Cascadia R Conference 2024",
    "section": "",
    "text": "Welcome back to another week of [VS]Codes! A few weeks ago, I had the opportunity to attend Cascadia R Conference 2024, a local R conference for the data science community of the Pacific Northwest. It was a great experience getting to see so many different applications of data science across a variety of industries, and I very much enjoyed the experience of connecting with other data scientists from the surrounding PNW area. This blog post will summarize content from some of the talks that I attended that day as well as my personal overall takeaways from the conference.\n\n\n\nKeynote: “Why is everybody talking about Generative AI?,” by Deepsha Menghani from Microsoft\nGenerative Artificial Intelligence (GenAI) is a type of artificial intelligence that can create new content based on the patterns it has learned from existing day. GenAI can be trained through human intervention - a human can reinforce what behavior is correct and what isn’t. GenAI can be very powerful when you make it work for you.\nWe can break GenAI use cases into three different scenarios:\n\nDirect\nCustomized\nCommercial\n\nIn a direct scenario, a prompt leads directly to a response.\nIn a customized scenario, a prompt will lead to data, which will then lead to a response.\nLastly, in a commercial scenario, a prompt will lead to decision making, leading to data, then to actions, then to an outcome, then to feedback, and back to decision making. This will be an iterative process.\nWith this framework of GenAI scenarios, we can see how GenAI might operate across different fields.\n\nGenAI scenarios for different fields\n\n\n\n\n\n\n\n\nField\nDirect\nCustomized\nCommercial\n\n\n\n\nFinance\nFinancial literacy\nBudget and tax optimization\nFraud detection\n\n\nHealthcare\nDeveloping a general health routine\nPersonalized fitness / health data summaries\nMedical image processing\n\n\nEducation\nGeneral conceptual explanations\nPersonalized study plans\nReasoning and content generation (e.g. Khanmigo bot - a safe space to converse while learning)\n\n\nYou!\nCode assistance / copilot (e.g. turning comments into code or synthetic data generation)\nREADME / documentation generation. Return a consistent README / documentation structure\nShiny application support bot. Feed documentation into a bot that sits at the bottom of your web app. This bot can help users when they want to use your dashboard.\n\n\n\nGenAI has many pros and cons to consider:\n\n“The Good, The Bad, and The Ugly” of GenAI\n\n\n\n\n\n\n\nThe Good\nThe Bad\nThe Ugly\n\n\n\n\nKnowledge accessibility\nGarbage in = garbage out\nBias\n\n\nPersonalization\nData Privacy\nResource Intensive\n\n\nCreativity\nDependencies\nCost\n\n\nEfficiency\nHallucinations\nEthics\n\n\n\nMisinformation\nEvolving Regulations\n\n\n\nGenAI can be incredibly useful in commercial settings if you train it on an application that you’ve developed so that it can help users navigate the application. However, this means it’s more important than ever before to have good, comprehensive documentation, because this will be the training data for your GenAI model!\nA caveat to using GenAI - when you have a hammer, everything looks like a nail! Don’t fall into the trap of thinking that GenAI is the only tool you can use… remember to think about the impact that you want!\nQuestions:\n\nHow do you pick the right LLM for your purposes?\n\nGo with accessibility, cost, data privacy\n\nHow do you evaluate the output of an LLM?\n\nEvaluation is such an evolving field. Different models can check for different things (e.g. no swear words). Human evaluation also\n\nWhat is the impact on jobs of data scientists?\n\nPeople who use AI to advance their work will do better. There will be an up-leveling from nitty-gritty work to strategic oversight. Some jobs will become more impactful through the introduction of AI. Other jobs may not find AI to be useful. Employers should train their employees in terms of how they can best apply AI in their jobs.\n\n\n\n\n“R Workflows in Azure Machine Learning for Athletic Data Analysis,” by Emily Kraschel from the University of Oregon\nWhen working with data related to sports, you need to work fast. Data collection is live, and will need to be evaluated over a variety of timescales, from daily to weekly to even longer. Sports analytics work requires efficient storage, as well as a combination of basic analysis and reporting as well as more complicated decision making.\nThis fast space of sports analytics stands in high contrast to the slow work of data science, which generally involves extensive data cleaning, the development of in-depth reports and aggregation, and advanced data analysis.\nThe conflicting needs of data science and sports analytics are thus as follows: - data are changing constantly. There are many new versions of the data and many additions to existing data - data science in general comes with numerous bottlenecks and stopping points - sports data is inherently observational, resulting in an inability to perform controlled experimentation\nThe old framework implemented by the team at the University of Oregon focused on automated reports generated from a data analysis dashboard. This process was good for a fast pace of work, but hindered slower, more complicated analysis. It was challenging to take a step back and decide on new metrics that could be incorporated into the dashboard. There was no central source for raw data, resulting in a variety of athlete IDs coming from different instrument interfaces. Furthermore, with the lack of centrality in the infrastructure of the system, data would exist in different versions, different places, and different file formats. The team was left with a clunky system if they tried to pull data out of the dashboard to perform further, more in-depth analysis.\nThus, the goals of the new framework were as follows: - improve competition outcomes through the generation of more complete, valid, actionable data - lower the barriers to more complex data science and analytics - aggregate the data in a centralized location and unify athlete IDs across instrumentation sources - enhance compute power and the ability to collaborate - reduce bottlenecks in data analysis (e.g. duplication, updates, incomplete data, etc.)\nWith these goals in mind, the team decided to migrate their data infrastructure to the cloud with Azure ML. Services that they have begun to take advantage of include: - analysis in Jupyter notebooks - configurable compute - services for pipeline development and implementation - improved data storage and loading - enhanced data security\nThe team still has an automated dashboard for day-to-day analysis. However, the new platform improves their ability to perform slow work, letting them step back from the constant stream of data. Now they can pull data easily across all instrument APIs into a centralized storage point. They are able to merge data more easily, maintain the most recent version of a given dataset, and perform compute more quickly.\nWith this new cloud-based infrastructure, the team is able to perform a variety of more complicated analysis in R, including developing prediction models for hamstring injuries through the application of discrete time survival models, or evaluating differences in jump height by rate of force development across genders.\n\n\n“Fair Machine Learning,” by Simon Couch from Posit PBC\nWhat does it mean for a model to be good vs. a model to be fair? A good model will produce a high value of sensitivity, specificity, or accuracy, all depending on the metric that you choose. Fairness, on the other hand, is not just about statistical behavior. Fairness is about our beliefs. We can think of fairness as the translation of values into mathematical measures.\nIn general, definitions of fairness are not mathematically or morally compatible (see Mitchel et al. 2021). Metrics such as R2 and AUC are useful for evaluating a model, but at the end of the day, a model is just a single part of a larger system. It is important to think about how model predictions will be used at the end of the day.\nThe hard part of this process is articulating what fairness means to you (or your stakeholders) in the context of a problem. Then, you need to choose a mathematical measure of fairness that speaks to that meaning - this should situate the resulting measure in the context of the entire system.\nChoose tools that help you think about the hard parts of fair ML. The {tidymodels} set of packages, including {rsample}, {recipes}, {parsnip}, {tune}, and {yardstick}, are a great set of software options to support fair machine learning. You can refer to the textbook Tidy Modeling with R or tidymodels.org for more information on how to use these tools.\nQuestions:\n\nWhat is a structured way to get stakeholders involved?\n\nModel cards are a great option. These are just a couple of paragraphs that provide context for how the data were initially generated and various techniques used to model the data. Model cards can be generated using the {vetiver} package.\n\n\n\n\n“How to make a Thousand Plots Look Good: Data Viz Tips for Parameterized Reporting,” by David Keyes from R for the Rest of Us\nParameterized reporting refers to process of creating a single document in markdown/Quarto and using it to make multiple reports at once. For instance, one can work with a single report for the visualization of housing and demographics data, and then expand this report to view data for a variety of towns, counties, and countries.\nHow can we think about intuitive data visualization in the context of parameterized reporting? Here are some rules to consider:\n\nThere is no magic package\nConsider the outer limits of your data\n\n\ne.g. scale of income\n\n\nMinimize text and position it carefully\nDon’t label everything\n\n\n{ggrepel}: repel overlapping labels in ggplot\n{shadowtext}: add background color to text\n\n\nHide small values\nDon’t put text where it could be obscured\n\n\nAdd text elements as multiple layers. Make sure to include a separate element for text position\n\n\nHighlight items strategically\n\n\nMake use of color, size, shadow, outline, and opacity\nConsider the R package {ggfx}\n\nHere’s the ultimate takeaway: there are a countless number of R packages that can help you achieve your goals in modularizing your applications, but at the end of the day, you are the one who has to do the thinking!\n\n\n“Cartographic Tricks and Techniques in R,” by Justin Sherrill from ECONorthwest\nCartography refers to the generation of maps for geographic locations. However, in a more figurative sense, cartography is an exercise in story-telling. Through the information that you convey in your cartographic visualizations, you are choosing the story that you tell. Writing a good story is about making decisions - you need to make the right sacrifices in the information that you choose to exclude.\nWhat are the key principles of “good” cartography? 1. Visual hierarchy (the arrangement of elements that guide the viewer’s eye through the content in the right order) 2. Legibility 3. Figure-ground (the ability to differentiate between an object and its background) 4. Balance\nJacques Bertin and William Bunge were two renowned geographers from the 20th century who consistently followed the principles of good cartography. You can refer to a lot of their work to get inspiration for developing cartographic visualizations. Timo Grossenbacher is a modern-day geographer and data scientist who has used R to develop some beautiful cartographic illustrations. Here is a great example of a bivariate thematic map he generated of Switzerland’s regional income inequality.\nAs Timo demonstrates, you can do pretty much all of the data visualization you want to do with {ggplot2}. {ggplot2} is one of the strongest data visualization tools in existence.\nHere is a set of great packages to apply for geospatial data visualization in R and {ggplot2}:\n1. {ggspatial}: includes the north arrow / scale bar\n2. {mapboxapi}: can show roads, bodies of water, and more.\n3. {patchwork}: can do map insets to show larger context of your zoomed in map - can also use {ggmagnify}\n4. {ggrepel}: label placement to avoid overlapping labels\n5. {st_inscribed_circle}: label an unusually shaped polygon\n6. {ggforce}: label a subset of points in a nice style\n7. {ggfx}: fun effects for {ggplot2}\n8. {ggdensity}: show density patterns\n9. {rmapshaper}: simplify geometries of shapes\n10. {smoothr}: round the corners for shapes\n11. {ggarrow}: make pretty arrows\n12. {ggstar}: pch symbols\n13. {ggsvg}: use SVGs as points in your plot\nThese packages are all very easy to implement and can turn your basic cartographic maps into true works of art! If you’re interested in playing around with cartographic visualizations, you can get access to open-source geospatial data for the state of Washington from the Washington Geospatial Open Data Portal.\n\n\nPersonal Takeaways\nI had a lot of fun attending Cascadia R Conference 2024, and I look forward to being back in 2025! Here are some personal takeaways I got from the conference.\n\nThe R community is extremely creative and fun! This conference felt very different from some of the technical biomedical informatics conferences I’ve attended in the past, many of which were focused purely on scientific advancements. At this conference, there were so many talks that demonstrated all sorts of random things you could do with the language of R, and these wacky detours were both encouraged and celebrated!\nThere is a growing focus on R for production-level ventures, including parameterization and modularization. Many data scientists are interested in scaling R for industrial applications and integrating it with other languages.\nWhile we are currently in a hype cycle for the field of generative AI and large language models, there is still plenty of other interesting work to be accomplished in the field of data science. It’s important to not get carried away by the fear of missing out (FOMO)! Focus on the goals of your work and find the right tools for the job (i.e. problem-first instead of tooling-first solutions)\n\n\nThis concludes my summary of my experience at Cascadia R Conference 2024! I’d like to give a huge thank you to the organizers of this conference for all of their hard work and for bringing the PNW data science community together in such a wonderful event. The next conference I will be attending will be posit::conf(2024), and I look forward to summarizing my experiences there in another blog post. Until next week, [VS]Coders!"
  },
  {
    "objectID": "posts/pytorch-tutorial/index.html",
    "href": "posts/pytorch-tutorial/index.html",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "In today’s blog post, we’ll go through the “Introduction to PyTorch” tutorial available from the PyTorch’s online learning community.\nThis tutorial is designed to walk through every component you would need to start developing models in PyTorch. With that, many of the elements show you how to complete certain steps in multiple ways… In this walkthrough, I will be selecting a subset of elements from these components and streamlining the steps of data processing and model training to showcase exactly how one would use these tools for an example dataset.\nIn a future blog post, I will take the foundation developed through this walkthrough and explore a modifiedd data analysis workflow, with a more complicated neural network and additional hyperparameter training for a different set of data.\nWith background out of the way, let’s get started~\nMost machine learning workflows involve uploading data, creating models, optimizing model parameters, and performing predictions on input data. This tutorial introduces you to a complete ML workflow implemented in PyTorch.\nIn PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters. Tensors are a specialized data structure that are very similar to arrays and matrices.\n# Import required packages\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n#import torchvision\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor, Lambda\nfrom torchvision.io import read_image\nWe start by checking to see if we can train our model on a hardware accelerator like the GPU or MPS if available. Otherwise, we’ll use the CPU.\n# Get cpu, gpu or mps device for training.\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")\n\nUsing mps device\nCode for processing data samples can get messy and hard to maintain. We ideally want our dataset code to be de-coupled from model training code for better readability and modularity. We will break our analysis into the following two sections: Data and Modeling."
  },
  {
    "objectID": "posts/pytorch-tutorial/index.html#data",
    "href": "posts/pytorch-tutorial/index.html#data",
    "title": "Introduction to PyTorch",
    "section": "Data",
    "text": "Data\n\nImporting and transforming the data\nPyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio. In this tutorial, we will be using a TorchVision dataset. The torchvision.datasets module contains Dataset objects for many real-world vision data. Here, we will use the FashionMNIST dataset. Fashion-MNIST is a dataset of images consisting of 60,000 training examples and 10,000 test examples. Each example includes a 28x28 grayscale image and an associated label from one of 10 classes.\nEach PyTorch Dataset stores a set of samples and their corresponding labels. Data do not always come in the final processed form that is required for training ML algorithms. We use transforms to perform some manipulation of the data and make it suitable for training. Every TorchVision Dataset includes 2 arguments: transform to modify the samples and target_transform to modify the labels.\nThe FashionMNIST features are in PIL Image format. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. To make this transformation, we use the ToTensor() function. ToTensor() converts a PIL image or NumPy ndarray into a FloatTensor and scales the image’s pixel intensity values in the range [0., 1.]\n\n# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\nWe now have our data uploaded! We can index an input Dataset manually (i.e. training_data[index]) to get individual samples. Here, we use matplotlib to visualize some samples in our training data.\n\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\n\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPreparing data for training with DataLoaders\nWhile training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Pythonic multiprocessing to speed up data retrieval. DataLoader is an iterable that abstracts this complexity for us in an easy API. We pass our input Dataset as an argument to DataLoader. This wraps an iterable over our dataset, supporting automatic batching, sampling, shuffling, and multiprocess data loading in the process.\nHere we define a batch size of 64 - each element in the DataLoader iterable will return a batch of 64 features and labels.\n\nbatch_size = 64\n\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break\n\nShape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64\n\n\n\n\nIterating over the DataLoader\nNow that we have loaded our data into a DataLoader, we can iterate through the dataset as needed (using next(iter(DataLoader)). Each iteration returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). Because we specified shuffle=True, the data are shuffled after we iterate over all of our batches.\n\n# Display image and label.\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\nimg = train_features[0].squeeze()\nlabel = train_labels[0]\nplt.imshow(img, cmap=\"gray\")\nplt.show()\nprint(f\"Label: {label}\")\n\nFeature batch shape: torch.Size([64, 1, 28, 28])\nLabels batch shape: torch.Size([64])\nLabel: 5"
  },
  {
    "objectID": "posts/pytorch-tutorial/index.html#modeling",
    "href": "posts/pytorch-tutorial/index.html#modeling",
    "title": "Introduction to PyTorch",
    "section": "Modeling",
    "text": "Modeling\n\nDefining a neural network\nNeural networks comprise of layers/modules that perform operations on data. The torch.nn namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the nn.Module. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\nTo define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function.\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nNow that we’ve defined the structure of our NeuralNetwork, we can create an instance of it and move it to a faster device (GPU or MPS) if available to accelerate operations. We can also print its structure to see the layers that we’ve just defined.\n\nmodel = NeuralNetwork().to(device)\nprint(f\"Model structure: {model}\")\n\nModel structure: NeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\nMany layers inside a neural network are parameterized, meaning that they have associated weights and biases that are optimized during training. Subclassing nn.Module automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s parameters() or named_parameters() methods.\nThe linear layer is a module that applies a linear transformation on the input using its stored weights and biases. Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\nIn this model, we use nn.ReLU between our linear layers, but there are other options for activations to introduce non-linearity in your model.\n\nfor name, param in model.named_parameters():\n    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n\nLayer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0069, -0.0090, -0.0053,  ...,  0.0014, -0.0055, -0.0313],\n        [-0.0027, -0.0257, -0.0303,  ..., -0.0329,  0.0320, -0.0336]],\n       device='mps:0', grad_fn=&lt;SliceBackward0&gt;) \n\nLayer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0255, -0.0069], device='mps:0', grad_fn=&lt;SliceBackward0&gt;) \n\nLayer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0210, -0.0186,  0.0411,  ...,  0.0346,  0.0432, -0.0231],\n        [-0.0199,  0.0335, -0.0396,  ..., -0.0416,  0.0382,  0.0423]],\n       device='mps:0', grad_fn=&lt;SliceBackward0&gt;) \n\nLayer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0328, -0.0148], device='mps:0', grad_fn=&lt;SliceBackward0&gt;) \n\nLayer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0231, -0.0346, -0.0262,  ..., -0.0030, -0.0107,  0.0297],\n        [ 0.0025, -0.0110, -0.0214,  ...,  0.0298, -0.0307,  0.0295]],\n       device='mps:0', grad_fn=&lt;SliceBackward0&gt;) \n\nLayer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0358,  0.0367], device='mps:0', grad_fn=&lt;SliceBackward0&gt;) \n\n\n\n\n\nOptimizing the Model Parameters\nNow that we have our data and our model, it’s time to train, validate and test our model by optimizing its parameters on the input data! Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters, and optimizes these parameters using gradient descent.\n\nHyperparameters\nHyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates.\nWe define the following hyperparameters for training: - Number of Epochs - the number times to iterate over the dataset - Batch Size - the number of data samples propagated through the network before the parameters are updated - Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.\n\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5\n\n\n\nOptimization Loop\nTo train our model, we need a loss function as well as an optimizer.\nOnce we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an epoch.\nEach epoch consists of two main parts: - The Train Loop - iterate over the training dataset and try to converge to optimal parameters. - The Validation/Test Loop - iterate over the test dataset to check if model performance is improving.\nThe loss function measures the degree of dissimilarity between an obtained result and the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\nCommon loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss.\nWe pass our model’s output logits to nn.CrossEntropyLoss, which will normalize the logits and compute the prediction error.\n\n# Initialize the loss function\nloss_fn = nn.CrossEntropyLoss()\n\nOptimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer; additionally, there are many different optimizers available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.\nWe initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter.\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nInside the training loop, optimization happens in three steps: - Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration. - Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter. - Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass.\n\n\nFull Implementation\nWe define train_loop that loops over our optimization code, and test_loop that evaluates the model’s performance against our test data.\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    # Set the model to training mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X = X.to(device)\n        y = y.to(device)\n        \n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * batch_size + len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\ndef test_loop(dataloader, model, loss_fn):\n    # Set the model to evaluation mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model.eval()\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n    with torch.no_grad():\n        for X, y in dataloader:\n            X = X.to(device)\n            y = y.to(device)\n        \n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\nIn a single training loop, the model makes predictions on the training dataset (fed to it in batches), and then backpropagates the prediction error to adjust the model’s parameters.\nWe can also check the model’s performance against the test dataset to ensure it is learning.\nThe training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch.\n\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\nprint(\"Done!\")\n\nEpoch 1\n-------------------------------\nloss: 2.296714  [   64/60000]\nloss: 2.285919  [ 6464/60000]\nloss: 2.289640  [12864/60000]\nloss: 2.269527  [19264/60000]\nloss: 2.238294  [25664/60000]\nloss: 2.245018  [32064/60000]\nloss: 2.233091  [38464/60000]\nloss: 2.209641  [44864/60000]\nloss: 2.167873  [51264/60000]\nloss: 2.210138  [57664/60000]\nTest Error: \n Accuracy: 45.4%, Avg loss: 2.163287 \n\nEpoch 2\n-------------------------------\nloss: 2.148807  [   64/60000]\nloss: 2.146786  [ 6464/60000]\nloss: 2.100028  [12864/60000]\nloss: 2.086866  [19264/60000]\nloss: 2.088841  [25664/60000]\nloss: 2.022998  [32064/60000]\nloss: 2.008321  [38464/60000]\nloss: 1.980193  [44864/60000]\nloss: 1.942847  [51264/60000]\nloss: 1.935418  [57664/60000]\nTest Error: \n Accuracy: 47.7%, Avg loss: 1.898360 \n\nEpoch 3\n-------------------------------\nloss: 1.934336  [   64/60000]\nloss: 1.874416  [ 6464/60000]\nloss: 1.709932  [12864/60000]\nloss: 1.756551  [19264/60000]\nloss: 1.738622  [25664/60000]\nloss: 1.706300  [32064/60000]\nloss: 1.609682  [38464/60000]\nloss: 1.628336  [44864/60000]\nloss: 1.539030  [51264/60000]\nloss: 1.561772  [57664/60000]\nTest Error: \n Accuracy: 59.2%, Avg loss: 1.528465 \n\nEpoch 4\n-------------------------------\nloss: 1.439419  [   64/60000]\nloss: 1.322930  [ 6464/60000]\nloss: 1.461314  [12864/60000]\nloss: 1.465872  [19264/60000]\nloss: 1.336003  [25664/60000]\nloss: 1.346315  [32064/60000]\nloss: 1.377878  [38464/60000]\nloss: 1.287794  [44864/60000]\nloss: 1.289096  [51264/60000]\nloss: 1.287725  [57664/60000]\nTest Error: \n Accuracy: 62.3%, Avg loss: 1.254067 \n\nEpoch 5\n-------------------------------\nloss: 1.233157  [   64/60000]\nloss: 1.233771  [ 6464/60000]\nloss: 1.178213  [12864/60000]\nloss: 1.106026  [19264/60000]\nloss: 1.188458  [25664/60000]\nloss: 1.199469  [32064/60000]\nloss: 1.154457  [38464/60000]\nloss: 1.008805  [44864/60000]\nloss: 1.108838  [51264/60000]\nloss: 1.131137  [57664/60000]\nTest Error: \n Accuracy: 64.1%, Avg loss: 1.088662 \n\nDone!\n\n\nWe can now use this model to make individual predictions.\nTo use the model, we pass it the input data. This executes the model’s forward, along with some background operations. Note that we do not call model.forward() directly!\n\nclasses = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\nmodel.eval()\nx, y = test_data[0][0], test_data[0][1]\nwith torch.no_grad():\n    x = x.to(device)\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n\nPredicted: \"Ankle boot\", Actual: \"Ankle boot\""
  },
  {
    "objectID": "posts/obliteride-24/index.html",
    "href": "posts/obliteride-24/index.html",
    "title": "Obliteride 2024",
    "section": "",
    "text": "Welcome back to another week of [VS]Codes! I currently have some longer tutorials and overviews cooking in the background, which I will be sharing in the coming weeks. In the meantime, I’d like to use this week’s post to share some information about an upcoming fundraising event in which I will be participating.\n\nThis coming weekend, I will be taking part in a 25-mile bike ride as a part of Fred Hutch Obliteride. Obliteride is an annual event that raises money for cancer research at Fred Hutch. Since 2013, more than 27,000 participants and volunteers, along with sponsors and over 104,000 Obliteride donors, have raised more than $48 million for breakthrough work at Fred Hutch.\n\nHere are a few examples of work that was supported at Fred Hutch by Obliteride in just the past year:\n\nResearch and clinical advances for a variety of cancers and disease areas, including breast, colon, head and neck, and prostate cancers.\nPartnerships to increase health equity and reduce disparities in labs, clinics, classrooms, and more.\nAdvances in patient care, including diagnostics, surgery, and follow-up.\n\nMy participation in Obliteride 2024 is focused on amplifying the impact of data science across Fred Hutch. My team (the Fred Hutch Data Science Lab) focuses on getting both data and tools into the hands of researchers at Fred Hutch through the creation of centralized data resources and advanced models and workflows for simplified data analysis, as well as the development of comprehensive training resources and communities of practice across a variety of subjects in biomedical data science.\nThis year, with the support of our donors, we aim to accelerate the next discoveries that the Fred Hutch will make via expanded data access, advanced technology, and artificial intelligence. Any donations toward my profile or that of my team’s will directly facilitate the research conducted by groups at Fred Hutch who strive to advance their work by engaging with our data science and AI services.\nMy team and I are a part of Obliteride because we want to help Fred Hutch overcome cancer once and for all. Please consider sponsoring me here - every donation helps!\nUntil next week, [VS]Coders!"
  },
  {
    "objectID": "posts/multiomics-fa/index.html",
    "href": "posts/multiomics-fa/index.html",
    "title": "Factor analysis for multiomics data with MOFA2",
    "section": "",
    "text": "In this week’s blog, I’ll be providing a brief overview on multiomic data and the utility of factor analysis. I will also summarize a tutorial and case study developed by Alex Gurbych from blackthorn.ai that highlights how to perform factor analysis using the Bioconductor package MOFA2. You can review Alex’s original tutorial here. With context out of the way, let’s get started!"
  },
  {
    "objectID": "posts/multiomics-fa/index.html#multiomics",
    "href": "posts/multiomics-fa/index.html#multiomics",
    "title": "Factor analysis for multiomics data with MOFA2",
    "section": "1.1. Multiomics",
    "text": "1.1. Multiomics\nMultiomics refers to a biological analysis approach that considers multiple data modalities concurrently to study a biological system holistically. The individual data modalities of multiomic analysis, also known as ’omes, are comprised of large-scale data and typically summarize the entire set of biomolecules for an organism. Some examples of individual ’omes include the genome, proteome, transcriptome, epigenome, and phenome.\n\n\n\nAn overview of how multiomic data can lend itself to precision medicine applications - Dokyoon Kim Lab for Integrative ’Omics and Biomedical Informatics at the University of Pennsylvania\n\n\nMultiomic studies can lend themselves to the improved characterization of biological processes across molecular layers. By integrating multiple ’omes together, researchers can analyze complex biological big data to find novel associations between biological entities, pinpoint relevant biomarkers, and build elaborate markers of disease and physiology.\nMotivated by this improved ability to represent the underlying biology of a system, multiomic profiling has been increasingly applied across a variety of biological domains, including cancer biology, regulatory genomics, microbiology, and host‐pathogen biology. A common aim of these applications is to characterize heterogeneity between samples, as manifested in one or several of the data modalities. Multiomic profiling is particularly appealing if the relevant axes of variation are not known beforehand, and hence may be missed by studies that consider a single data modality or targeted approaches. One approach to capture the relevant axes of variation in a biological system is factor analysis."
  },
  {
    "objectID": "posts/multiomics-fa/index.html#factor-analysis",
    "href": "posts/multiomics-fa/index.html#factor-analysis",
    "title": "Factor analysis for multiomics data with MOFA2",
    "section": "1.2. Factor analysis",
    "text": "1.2. Factor analysis\nFactor analysis is a statistical method that summarizes the variability across observed, correlated variables in a smaller set of variables. These newly generated variables correspond to the ‘factors’ of the original data - they are generated by determining linear combinations of the observed variables and adding small fixed deviations. Correlations between the factors of an input set of predictors and an output response variable can reveal previously unobserved latent axes of variation that affect the response of the data. Thus, the main goal of factor analysis is to identify hidden variables and evaluate their correlations with output variables of interest."
  },
  {
    "objectID": "posts/multiomics-fa/index.html#mofa2",
    "href": "posts/multiomics-fa/index.html#mofa2",
    "title": "Factor analysis for multiomics data with MOFA2",
    "section": "1.3. MOFA2",
    "text": "1.3. MOFA2\nGiven the utility of factor analysis for the identification of relevant axes of variation in biological systems, it becomes particularly beneficial to have a simple method for integrating and analyzing multiomic data in this manner - Multiomic Factor Analysis (MOFA) offers one such option.\nMOFA is a probabilistic factor model that performs unsupervised integration of multiple modalities of omics data and discovers the principal axes of variation in multiomic data sets. Intuitively, MOFA can be viewed as a versatile and statistically rigorous generalization of principal component analysis (PCA) for multiomics data. MOFA infers a set of hidden factors that capture biological and technical sources of variability. It disentangles axes of heterogeneity that are shared across multiple modalities as well as those specific to individual data modalities. The learnt factors enable a variety of downstream analyses, including identification of sample subgroups, data imputation and the detection of outlier samples. The inferred factor loadings can be sparse, thereby facilitating the linkage between the factors and the most relevant molecular features. Importantly, MOFA disentangles to what extent each factor is unique to a single data modality or is manifested in multiple modalities, thereby revealing shared axes of variation between the different omics layers. Once trained, the model output can be used for a range of downstream analyses, including visualization, clustering, and classification of samples in the low‐dimensional spaces spanned by the factors.\nYou can read more about the MOFA2 package on its Bioconducter documentation page here."
  },
  {
    "objectID": "posts/multiomics-fa/index.html#add-sample-metadata",
    "href": "posts/multiomics-fa/index.html#add-sample-metadata",
    "title": "Factor analysis for multiomics data with MOFA2",
    "section": "4.1. Add sample metadata",
    "text": "4.1. Add sample metadata\nNow that we have trained our model on our data, we can incorporate metadata from the input dataset to evaluate correlations between our different ’omes and additional variables. Let’s get data for age, sex, death status, and treatment status:\n\nAge: age in years\nDied: (T/F) did the patient die?\nSex: (M/F)\ntreatedAfter: (T/F) was the patient treated after?\n\n\n# Load sample metadata\nCLL_metadata &lt;- fread(\"ftp://ftp.ebi.ac.uk/pub/databases/mofa/cll_vignette/sample_metadata.txt\")\n\n# Add sample metadata to the model\nsamples_metadata(MOFAobject_trained) &lt;- CLL_metadata"
  },
  {
    "objectID": "posts/multiomics-fa/index.html#correlation-analysis-of-factors",
    "href": "posts/multiomics-fa/index.html#correlation-analysis-of-factors",
    "title": "Factor analysis for multiomics data with MOFA2",
    "section": "4.2. Correlation analysis of factors",
    "text": "4.2. Correlation analysis of factors\nIn order for our model to work accurately, we must ensure that the factors that we have generated are not correlated with one another. If we observe significant correlations between factors, we either used too many factors or performed and insufficient amount of normalization. We can visualize correlations across factors using the plot_factor_cor function.\n\nplot_factor_cor(MOFAobject_trained)\n\n\n\n\n\n\n\n\nWe can see from our figure that our generated factors show no significant correlations with one another."
  },
  {
    "objectID": "posts/multiomics-fa/index.html#explained-variance-decomposition-by-factor",
    "href": "posts/multiomics-fa/index.html#explained-variance-decomposition-by-factor",
    "title": "Factor analysis for multiomics data with MOFA2",
    "section": "5.1. Explained variance decomposition by factor",
    "text": "5.1. Explained variance decomposition by factor\nWe can use the plot_variance_explained function to determine how much of the variance in each of our individual data modalities is explained by each of our factors:\n\nplot_variance_explained(\n  MOFAobject_trained, \n  max_r2 = 15\n)\n\n\n\n\n\n\n\n\nFrom our explained variance plot, we can see that Factor 1 explains a high amount of variance across all four of our data modalities. Factor 2 explained a high amount of variance for drug response, Factor 3 explains a high amount of variance for drug response, transcriptomics, and genomics, and Factor 4 explains a high amount of variance for transcriptomics."
  },
  {
    "objectID": "posts/multiomics-fa/index.html#explained-variance-per-omic-modality",
    "href": "posts/multiomics-fa/index.html#explained-variance-per-omic-modality",
    "title": "Factor analysis for multiomics data with MOFA2",
    "section": "5.2. Explained variance per omic modality",
    "text": "5.2. Explained variance per omic modality\nWe can also use the plot_variance_explained function to determine the amount of variance that is explained across our four data modalities given all 15 factors in our model.\n\nplot_variance_explained(\n  MOFAobject_trained,\n  plot_total = T\n)[[2]]\n\n\n\n\n\n\n\n\nHere, we can see that the phenotypic and transcriptomic modalities have more variance explained by our model than the genomic or epigenomic modalities."
  },
  {
    "objectID": "posts/multiomics-fa/index.html#factor-values",
    "href": "posts/multiomics-fa/index.html#factor-values",
    "title": "Factor analysis for multiomics data with MOFA2",
    "section": "7.1. Factor values",
    "text": "7.1. Factor values\nWe can make use of the plot_factor function to evaluate how the data points in our dataset are distributed with respect to a factor of choice.\n\nplot_factor(\n  MOFAobject_trained,\n  dodge = TRUE,\n  add_violin = TRUE,\n  factors=c(1, 15)\n)"
  },
  {
    "objectID": "posts/multiomics-fa/index.html#factor-1s-association-with-the-genomic-data-modality",
    "href": "posts/multiomics-fa/index.html#factor-1s-association-with-the-genomic-data-modality",
    "title": "Factor analysis for multiomics data with MOFA2",
    "section": "7.2. Factor 1’s association with the genomic data modality",
    "text": "7.2. Factor 1’s association with the genomic data modality\nWe can use the plot_weights function to determine how different factors are weighted with respect to the features in our data. In the following graph, we focus on the influence of Factor 1 on features present in the genomic data modality. Features with a higher weight represent a stronger association with the factor of interest.\n\nplot_weights(\n  MOFAobject_trained,\n  view = \"Mutations\",\n  factor=1,\n  nfeatures = 15,\n  scale = T\n)\n\n\n\n\n\n\n\n\nFrom this graph, we can see that most features have a weight of 0 with respect to Factor 1, indicating a lack of association with the factor. However, IGHV (immunoglobulin heavy chain variable) has a weight close to 1 with respect to Factor 1. Indeed, mutations in the genetic region are a main clinical marker for CLL!\nThe plot_top_weights function lets us visualize this same output, sorted by absolute value of the weight:\n\nplot_top_weights(\n  MOFAobject_trained,\n  view = \"Mutations\",\n  nfeatures = 15,\n  scale = T,\n  factor=1\n)\n\n\n\n\n\n\n\n\nOnce again, we can see that the weight for IGHV with respect to Factor 1 far exceeds the weights for any other genomic features in our dataset.\nBased on these results, we’d expect to see that samples that have a high positive value for Factor 1 will have IGHV mutations. To confirm this hypothesis, let’s plot the distribution of factor 1 values, colored by IGHV mutation status:\n\nplot_factor(\n  MOFAobject_trained,\n  dodge = TRUE,\n  add_violin = TRUE,\n  color_by = \"IGHV\",\n  factors = c(1)\n)\n\n\n\n\n\n\n\n\nIt does seem like our Factor 1 values correlate with presence/absence of IGHV mutation!"
  },
  {
    "objectID": "posts/multiomics-fa/index.html#factor-1s-association-with-the-transcriptomic-data-modality",
    "href": "posts/multiomics-fa/index.html#factor-1s-association-with-the-transcriptomic-data-modality",
    "title": "Factor analysis for multiomics data with MOFA2",
    "section": "7.3. Factor 1’s association with the transcriptomic data modality",
    "text": "7.3. Factor 1’s association with the transcriptomic data modality\nFrom the variance explained plot we know that Factor 1 drives variation across all four of our data modalities. Let’s visualize the mRNA expression changes that are associated with Factor 1 using the plot_weights function again:\n\nplot_weights(\n  MOFAobject_trained,\n  nfeatures = 10,\n  view = \"mRNA\",\n  factor = 1\n)\n\n\n\n\n\n\n\n\nWe can see from our generated plot that there are a substantial number of transcripts with weights close to -1 or 1 with respect to Factor 1. It is likely that genes with large positive mRNA expression values are more heavily expressed in samples with IGHV mutation. Let’s verify this assumption."
  },
  {
    "objectID": "posts/multiomics-fa/index.html#molecular-signature-clustering-for-factor-1",
    "href": "posts/multiomics-fa/index.html#molecular-signature-clustering-for-factor-1",
    "title": "Factor analysis for multiomics data with MOFA2",
    "section": "7.4. Molecular signature clustering for factor 1",
    "text": "7.4. Molecular signature clustering for factor 1\nLet’s use the plot_data_heatmap function to generate a heatmap of gene expression values against Factor 1 values. Furthermore, we color our heatmap by IGHV mutation status.\n\nplot_data_heatmap(\n  MOFAobject_trained,\n  scale = \"row\",\n  cluster_cols = FALSE,\n  cluster_rows = FALSE,\n  show_colnames = FALSE,\n  denoise = TRUE,\n  features = 25,\n  view = \"mRNA\",\n  factor=1\n)\n\n\n\n\n\n\n\n\nFrom our heatmap, we can see that various transcripts correlate with high Factor 1 value, and that these samples are also positive for IGHV status - our assumption is confirmed!"
  },
  {
    "objectID": "posts/nlp-oncology-overview/index.html",
    "href": "posts/nlp-oncology-overview/index.html",
    "title": "An intro to NLP for oncology",
    "section": "",
    "text": "In this week’s blog, I will be summarizing Yim et al’s 2016 review paper from JAMA Oncology, titled “Natural Language Processing in Oncology: A Review.” This paper was written by researchers at the University of Washington Medical Center, including individuals from the Departments of Biomedical and Health Informatics, the Department of Linguistics, the Division of Oncology at the Department of Medicine, and the Department of Radiology. You can read the abstract for this paper here.\nThe full text of this manuscript is unfortunately not publicly available (you or your institution must have access to JAMA Oncology to read the full text), so I hope that this summary provides those without access to this journal some insight into some exciting content. Furthermore, given that this paper is from 2016, it does not include many of the recent advances in the past 8 years related to transformer / large language models that have become more relevant in NLP applications. Nevertheless, I found this paper to be an excellent summary of how to frame the workflow for tackling NLP research questions, particularly in terms of the collaborations needed among oncologists and informaticians. Much of the content covered remains as relevant today as it did 8 years ago, and I look forward to using my takeaways from this material in my own work at the Fred Hutch Cancer Center.\nWith context out of the way, let’s get started!"
  },
  {
    "objectID": "posts/nlp-oncology-overview/index.html#oncology-focused-nlp",
    "href": "posts/nlp-oncology-overview/index.html#oncology-focused-nlp",
    "title": "An intro to NLP for oncology",
    "section": "Oncology-focused NLP",
    "text": "Oncology-focused NLP\nNatural Language Processing (NLP) refers to any computer-based algorithm that can handle, augment, and transform natural language so that it can be represented for computation.\nNLP is particularly attractive for clinical research applications for the following reasons. One can:\n\ndefine new variables that are not readily available in electronic health records as stored values\nautomate the process of reviewing clinical notes for patient diagnosis instead of requiring a resource-intensive manual review process.\nexpedite biomedical discovery by empowering clinicians with the ability to analyze outcomes in the context of big data\n\nWith respect to the field of oncology, NLP applications typically have the following objectives:\n\nCase identification: determine which patients may have a case of the disease of interest\n\nHere, we process the free text from clinical notes to augment the diagnosis codes that correspond to a patient’s labeled set of diseases or symptoms\n\nStaging: determine the stage of a patient’s cancer progress\n\nHere, we process the free text from clinical notes to determine the stage of a patient’s cancer progression. This objective is more challenging than case identification because it is highly context-specific depending on the type of cancer.\n\nOutcome determination: determine the patient’s ultimate outcome (cancer advancement vs. remission vs. death)\n\nHere, we process the free text from clinical notes to determine the patient’s ultimate outcome (advancement vs. remission vs. death). This objective is the most challenging of the three because it essentially requires us to perform cancer staging with respect to time."
  },
  {
    "objectID": "posts/nlp-oncology-overview/index.html#the-role-of-the-oncologist",
    "href": "posts/nlp-oncology-overview/index.html#the-role-of-the-oncologist",
    "title": "An intro to NLP for oncology",
    "section": "The role of the oncologist",
    "text": "The role of the oncologist\nAs with many objectives in the domain of biomedical informatics, developing NLP systems for oncology applications requires significant involvement from a domain expert.\nAn oncologist can contribute to the development of an NLP system in the following ways:\n\nProject conception: using their domain expertise, an oncologist can devise practical and impactful applications of NLP\nCorpus annotation: an oncologist can assist with annotation of training data by providing the “correct answers” for a body of text based upon a set of annotation guidelines.\nSystem evaluation and error analysis: based on measures such as precision and recall, an oncologist can manually review true and false positives and negatives in the test data to help determine the strengths and weaknesses of the system"
  },
  {
    "objectID": "posts/nlp-oncology-overview/index.html#nlp-tasks-and-strategies-for-oncology",
    "href": "posts/nlp-oncology-overview/index.html#nlp-tasks-and-strategies-for-oncology",
    "title": "An intro to NLP for oncology",
    "section": "NLP tasks and strategies for oncology",
    "text": "NLP tasks and strategies for oncology\nWithin the context of oncology, NLP is typically used for Information Extraction (IE). IE refers to the transformation of unstructured data into a structured form. IE encompasses multiple subtasks, including Named Entity Recognition (NER), Relation Extraction (RE), Text Classification, and Template Extraction.\n\nNER involves grouping words in a text and assigning them to a pre-defined “concept\nRE involves the assignment of relationships between entities\nText Classification assigns categorical label for a body of text\nTemplate Extraction collects a set of related entities, relations, and labels to define a form for structured data.\n\nYou can refer to figure 1 from the paper to see an example of clinical information extraction.\n\n\n\nFig 1 from the manuscript - an example of free-text processing, IE, and text classification\n\n\nStrategies to tackle the above sub-tasks can be grouped into the following three buckets: rule-based, statistical, and hybrid approaches.\n\nRule-based approaches involve heuristic algorithms designed by domain specialists. They can be as simple as individual keyword look-ups or can be defined by complex conditional logic. When the defined rules are simple, rule-based approaches can significantly improve the interpretability of a model. However, if the rules are more complicated, the established model will lose interpretability and become harder to replicate or update.\nStatistical (a.k.a machine learning) systems are algorithms designed to statistically maximize the probability of finding the correct answer based on the distribution of the training data. Such models are less prone to overfitting compared to rule-based approaches, and they can be easily adapted to new data. However, these models require an extensive amount of unbiased, representative training data to work accurately.\nHybrid approaches work to minimize the disadvantages of rule-based and statistical algorithms by combining these two methods."
  },
  {
    "objectID": "posts/nlp-oncology-overview/index.html#resources-needed-for-clinical-nlp",
    "href": "posts/nlp-oncology-overview/index.html#resources-needed-for-clinical-nlp",
    "title": "An intro to NLP for oncology",
    "section": "Resources needed for clinical NLP",
    "text": "Resources needed for clinical NLP\nCreating an NLP system requires the use of three common related NLP resources: extraction tools, ontologies, and corpora.\n\nExtraction tools refer to developed subsystems that can be used on new datasets with little to no modifications. Typical clinical NLP pipelines can involve the following tasks:\n\nSection Identification: identifying sections is particularly useful for clinical notes, where different sections will contain different types of content\nMedical NER: this is a more specialized version of general English NER, taking into account acronyms, abbreviations, and synonyms that are common in medical text. Generally, out-of-the-box NER systems are usually insufficient, requiring the application of in-house rule-based and statistical NER systems. Two freely available development tools include MetaMap and the Mayo Clinical Text Analysis and Knowledge Extraction System\nNegation detection: this helps distinguish the phrase “patient has fever” from “patient has no signs of fever.” This can be a complicated task depending on the language used in the notes.\n\n\nOther common tasks in NLP pipelines include: coreference resolution, temporal classification, medication information extraction, family history extraction, assertion detection, and polarity detection. You can see some examples of these tasks in Figure 4 from the paper:\n\n\n\n\n\nFig 4 from the manuscript - examples of common tasks in NLP pipelines\n\n\n\nOntologies refer to knowledge bases that reference how various concepts are related to one another. The most basic form of an ontology could be a medical dictionary. A more complicated form of an ontology might involve a knowledge graph of concepts and their relationships.\nCorpora are collections of (sometimes annotated) clinical text that can be used to train or test an NLP system. Some examples of publicly available de-identified medical corpora including the i2b2 challenge sets, the Conference and Laboratories of the Evaluation Forum datasets, the MIMIC-II corpus, the MIPACQ corpus, and MTSamples.com."
  },
  {
    "objectID": "posts/nlp-oncology-overview/index.html#challenges-in-designing-an-nlp-system",
    "href": "posts/nlp-oncology-overview/index.html#challenges-in-designing-an-nlp-system",
    "title": "An intro to NLP for oncology",
    "section": "Challenges in designing an NLP system",
    "text": "Challenges in designing an NLP system\nWe can create a desired NLP system by framing our desired objective (case identification, staging, or outcome prediction) as a series of NLP operations. For instance, a basic case identification task may involve NER followed by negation detection. On the other hand, an outcome prediction task may involve a simple NER task or a more complicated algorithm that identifies tumor sizes and associated dates before performing logic operations to deduce that a change took place.\nUltimately, the final success of an NLP system will depend on two key points:\n\nhow do you frame the problem at hand?\nhow effective are the individual components of your pipeline?\n\nMultiple challenges can affect the efficacy of your model. Firstly, creation of an NLP system requires careful planning and investment to develop an annotated text corpus for system training and testing. Model performance can also be highly variable depending on the task at hand – new larger, heterogeneous test data can challenge the accuracy of your system. Multiple iterations of software development are usually needed before you can arrive at a robust NLP product. Lastly, the data science mantra of “garbage in, garbage out” continues to ring true – the tools that you develop will only be as good the quality of the data that train the systems."
  },
  {
    "objectID": "posts/nlp-oncology-overview/index.html#summary",
    "href": "posts/nlp-oncology-overview/index.html#summary",
    "title": "An intro to NLP for oncology",
    "section": "Summary",
    "text": "Summary\nThis concludes my overview of “Natural Language Processing in Oncology: A Review.” I look forward to covering more such papers in the future as I progress with my work in clinical NLP at the Hutch."
  },
  {
    "objectID": "posts/nlp-oncology-overview/index.html#references",
    "href": "posts/nlp-oncology-overview/index.html#references",
    "title": "An intro to NLP for oncology",
    "section": "References",
    "text": "References\n\n“Natural Language Processing in Oncology: A Review”"
  },
  {
    "objectID": "posts/posit-conf-24/index.html",
    "href": "posts/posit-conf-24/index.html",
    "title": "posit::conf 2024",
    "section": "",
    "text": "Welcome back to another week of [VS]Codes! A few weeks ago, I had the opportunity to attend posit::conf 2024, a national data science conference organized annually by Posit, PBC. This year, the conference was held in my home base of Seattle, and it was a wonderful experience getting to attend so many inspiring talks and meet so many members of the data science community. This blog post will summarize content from the workshop I attended on the first day of the conference, as well as some of the talks that I attended over the following couple of days. I’ll conclude this post with my personal overall takeaways from the conference.\n\n\n\n“Package Development: The Rest of the Owl” - Jenny Bryan\nThe title of this talk was inspired by this blog post, and serves to explain how one goes from the basics of creating a new package in R to fleshing out a more complete data product.\nThe key package this workshop highlighted was devtools: you can learn more about this package here.\nThe first thing you should always do when starting to develop a new package is to call library(devtools). You can also force yourself to do this by including the line require(devtools) in your .Rprofile. With devtools loaded, you can call dev_sitrep() and git_sitrep() to get package development and git/GitHub ‘situation reports’ respectively as you work on your package.\nWhen you call the library command in R, your code will go from installed packages to memory. On the other hand, calling the load_all() function from devtools will go through the entire process from source to memory. This includes the following steps:\n\nsimulates building, installing, and attaching your package\nmakes all of the functions in your package available to use\nmakes anything you’ve imported available to use\nallows fast iteration of editing and test-driving your functions.\n\nAs you write code, it is also helpful to call the check() function often to ensure that you are not breaking things as you develop. Your workflow for writing and testing code in your package should emulate the following process:\n\nAnother thing that folks don’t really consider when developing packages is that GitHub code search is your friend. You can check out https://github.com/cran on GitHub - as packages get uploaded to CRAN, this user mirrors the full source code for the package onto GitHub for you. The same thing applies for https://github.com/tidyverse and https://github.com/r-lib.\nRegarding testing - when you are working on package development, most of your time will be spent looking at test files. For testing within the devtools framework, we call use_testthat(). Then we can run test_file() on a test R script that we’ve generated. A good workflow for macro-iteration across all files is to call test() followed by test_coverage() and finally followed by check(). Always aim to make tests self-sufficient and self-contained. It is better to repeat code than to introduce dependencies across tests. Don’t include library() or source() calls in your test files - include these in helper files instead.\nLastly, with respect to documentation - help topics are saved in .Rd files and will live in the man/ folder of your R package. You can build a README.md file from your README.Rmd using the build_readme() function. It is also helpful to insert a roxygen skeleton into your code.\nThe following figure offers a good summary of how to approach package development with devtools:\n\n\n\n“Keynote Session: Updates from Posit” - Hadley Wickham\nThe company that organized this conference, formerly known as RStudio, is now known as Posit, PBC. But what is a PBC? PBC stands for “positive benefit corporation.” PBC’s sit between charities and full for-profit corporations. The mission of Posit is to create free and open source software for data science, scientific research, and technical communication. Posit supports both free and commercial tools - the free tools are meant to allow anyone to do data science work. The commercial tools are intended for larger organizations that apply Posit’s tooling at large.\n\n\n“GitHub: How To Tell Your Professional Story” - Abigail Haddad\nGitHub doesn’t just have to be a way for you to version control your code - it can be a platform for showcasing the work that you care about. You can update the README file for the repo that has the same name as your GitHub username to be able to provide an introduction to your GitHub page. Make sure to also pin repositories to your profile that can showcase your skillset in the right way.\n*Note: I went ahead and did this right after Abigail’s talk. You can check out the result here :)\n\n\n“Oops, I’m A Manager - Finding your Minimal Viable Process” - Andrew Holz\nWhat’s the minimal viable process (MVP) to being a manager? Let’s break these terms down with some layman’s definitions:\n\nP(rocess): how sh*t gets done!\nV(iable): someone is paying the bills, so they expect the process to be meaningful.\nM(inimal): heavy processes don’t promote progress - they paralyze it. Keep the process as lean as possible.\n\nWe can distill this process into three phases: gather, do, and deliver\n\nGather\n\n\nIntentionally consider all stakeholders and the level of input they can and should provide\nSpecify as much detail as the team needs, and no more (see https://tidyfirst.substack.com/p/responsible-slack)\n\n\nDo\n\n\nMake sure you leave as much space as possible for the ‘who,’ ‘what,’ and ‘how’\nDon’t make one person do the same thing over and over again. Also avoid dictating how they go about doing their tasks.\nLastly, be careful when you set up meetings - consider purpose and frequency as well as who needs to be there.\n\n8 people in a meeting is too much - everyone will just be waiting for their turn to speak instead of actively communicating. A 4-person meeting is much better. The best kind of meeting? Context pairs where two individuals are fully aware of the context of their discussion.\n\n\n\nDelivery\n\n\nBeyond shipping your product, other aspects of delivery are often neglected. First, delivering your product involves being able to tell a coherent story with your stakeholders. Second, make sure that your entire team gets credit for its wins. Finally, do your best to own your mistakes.\n\nUltimately, being a good manager is a process of trial and error. There is no “right” process - there is only the “right NOW” process. Observe and reflect along with your team, and make changes to your process over time.\n\n\nSome notes from Lightning Talks - Eric Leung, Ben Arancibia, Claire Bai, Luis D. Verde Arregoitia, Mika Braginsky, Andrew Gard\n\nWhen you’re faced with a new challenging task, don’t reinvent the wheel! Use the best (available) tool for the job\nCompanies like COTA Healthcare are developing R packages like rwnavigator to facilitate outcomes analysis for cancer. The goal of such tasks is to incorporate medical expertise from oncologists and functionality to standardize and simplify code.\nThe ALARM project has developed an R package called “fifty-states” to simulate alternative congressional redistricting plans for all 50 states. You can learn more about their package here.\nDocumenting your code… comment next to the packages you call and also comment above individual steps. Comment as little and as clearly as possible. Automate informative comments by leveraging built-in descriptions, checking code for package components, and examining comments in code.\nDatapages are a great tool for findable, accessible, interoperable, reusable (FAIR), and interactive data sharing. Uploading a static .csv file is easy for you but hard for the audience. On the other hand, making a custom repository and website is easy for the audience but hard for you. Sharing a datapage gives you the best of both worlds and makes it easy to share your data analysis in a robust manner while communicating its impact to your audience.\nLearning in the age of AI… even if we don’t need to code ourselves in the future, we need to have the right vocabulary to be able to tell the AI what to do. Instead of telling students what they should implement, have them analyze AI output and figure out what didn’t work and why it is missing.\nCheck out the TV show The Expanse!\n\n\n\n“Please Let Me Merge Before I Start Crying: And Other Things I’ve Said at the Git Terminal” - Meghan Harris\nGit is not the same as GitHub. Git is the version control system, while GitHub is the developer platform that uses the Git software. R users can interact with Git through:\n\na command-line interface (CLI)\nthe RStudio graphical user interface (GUI)\na third-party UI (e.g. GitHub Desktop)\n\nMerging in git involves the joining of two or more development histories (also known as branches) together. Merging allows you to safely modify work when collaborating with others. A lot of people think they are scared of merges… the truth is, they’re really scared of merge conflicts! Merge conflicts occur when competing changes are made to the same line of a file (a content conflict) or someone edits a file and someone else deletes the file (a structure conflict).\nHere are some tips to dealing with merge conflicts:\n1. Don’t panic! You can use git merge --abort like a time machine to get you to where you were before. REmember, you are in control. You can choose which code to use.\n2. Assess the damage. You can call git status to see what has happened. Don’t be scared, but be careful. It doesn’t matter how complicated the conflict is… the process is the same.\nRemember, merge conflicts are not git problems. They are communication, workflow, or knowledge gap problems. When you are working, be thoughtful before, during, and after your coding sessions.\n\nBefore you code: check your git environment and check the branch status. Always pull first before touching anything. Emergencies are not real!\nWhile you code: commit often, push thoughtfully, and use git stashes when needed.\nAfter you code: you are reviewer #1! Make your code as clear as can be.\n\nWith respect to learning how to get better with git… do what you need to do, however you need to do it. Sometimes, all you really need is a bit more practice.\n\n\n“Keynote Session: A Future of Data Science” - Allen Downey\nWhat does it mean for data work to be successful? It means that we have successfully answered a question of interest. Now how do we go about answering a question? We need the following things:\n\ndata\na simple method / basic visualization\nfree software and tools for reliable science\na distribution system (e.g. a blog)\n\n\nData science involves the application of tools and processes to answer questions, resolve disagreements, and make better decisions. Data science has been on the Gartner hype cycle for a while… where are we today with respect to this cycle for data science? The peak of inflated expectations happened around 2009 to 2012, while the trough of disillusionment happened from 2016 to 2018. Now, we’re starting to hit the plateau of productivity… and the reason for this is because we have not fully embraced computational statistics.\nWhat is the difference between mathematical statistics and computational statistics? Mathematical statistics can be thought of as the start of the field, while computational statistics can be considered its final evolution. Data science as a field exists because statistics missed the boat on computers. Topics like general purpose programming languages, machine learning, and more were not introduced into the statistical discipline early on enough.\nNevertheless, there are many reasons to be optimistic about the progression of statistics and data science. Not only do we have more and more available data, but we also have improved data literacy. However, data bias from the increased consumption of negative media is skewing the trajectory of data science progress downward. Ultimately, data itself is the antidote to this negativity bias. We can use data to understand the world better, so that we know how to make the world better! This is the ultimate benefit of open data!\n\n\n\n“Uniquely Human: Data Storytelling in the Age of AI” - Laura Gast\nYou have to speak for the data! The data will NOT speak for itself. You can do this with the four following modes of persuasion:\n\nLogos (logic): data and methods\nEthos (trust): credibility\nPathos (emotion): narrative\nKairos (time): audience and timing\n\nContext feeds story. Story feeds impact. Context includes the following three components: background, framing, and circumstance. Make sure to contextualize both the inputs and the outputs of your data story. Data visualization can be used to communicate scale of impact.\nStory includes both an arc and a narrative. A story provides your audience with access. Your audience is not passive - you want them to be excited and rooting for the outcome of your analysis. At the same time, you must be careful in a world where “great stories” and things that aren’t true aren’t really the same thing. You want to make sure that you do not lose credibility.\nRegarding impact: data is not information, information is not knowledge, and knowledge is not wisdom. Data does not act, and AI does not have intent. It thus becomes evident that humans cannot be cut out of the process. Remember that you will always have to speak for the data through context, story, and impact.\n\n\n\nPersonal Takeaways\nI had a great time at my first posit::conf, and I look forward to attending more in the future. Here are a few key takeaways related to trends in the field of data science as well as ways that I could update my personal workflows:\n\nIn the coming years, with platforms like Positron and Quarto Live, we will see expanded ways that people can learn and practice data science across languages and disciplines.\nGitHub should be used not just as a version control platform but as a way to share your projects with the public, as well as to learn from other people’s work. Learn how to use GitHub code search!\nDuckDB (and duckplyr) is a simple and highly effective way of storing and manipulating data tables. Stop using .csv’s!\nGenerative AI tools like ChatGPT aren’t the end-all, be-all to data science work, but they can be extremely helpful tools for implementing new tools that have a high learning curve (e.g. CSS)\nLearning new concepts can be a challenging task, particularly when there are multiple ways to do a single thing. It’s often easiest to pick a single way of doing things and master it. Go with what works for you!\nRegardless of how the field of AI continues to progress (whether the hype grows or shrinks), people will remain at the heart of data science. It is our responsibility as ethical data scientists to promote open sharing of data and tools, as well as to advocate for the data and tell the stories we wish to tell when communicating with stakeholders who are not as data literate.\nNothing is ever really an emergency. If someone’s life is on the line, then you can worry. Otherwise, there’s no need for you to stress as much as you currently are!\n\n\nThis concludes my summary of my experience at posit::conf 2024! I’d like to give a huge thank you to Posit for bringing not just the national but also international data science community together in such a fun, educational event. Until next time, [VS]Coders!"
  },
  {
    "objectID": "posts/cnn-overview/index.html",
    "href": "posts/cnn-overview/index.html",
    "title": "An intro to Convolutional Neural Networks",
    "section": "",
    "text": "In this week’s blog, I will be summarizing the introductory “Convolutional Neural Networks” webpage from Fei-Fei Li’s Stanford CS 231n: “Deep Learning for Computer Vision” course. You can read the original page that I am summarizing here, and you can took a look at Fei-Fei’s full course page here. Note that the original webpage includes about twice as much content as what I have in this post, including a more thorough discussion of real-world examples of neural networks, additional schemas such as parameter sharing, and a few coding examples in numpy. My goal for this post is to offer a “CliffsNotes of a CliffsNotes,” presenting just the basic intuition behind CNNs without any further detail. If you want to learn more, I would highly suggest reading the original webpage that this post summarizes, or additional resources such as this 2015 arXiv review paper or this GeeksForGeeks walkthrough that includes some code examples. Without further ado, let’s get started!\nConvolutional Neural Networks (also known as CNNs or ConvNets) are very similar to standard Neural Networks. They are also made up of neurons that have learnable weights and biases, with each neuron receiving an input, performing a dot product, and leading into an optional non-linearity. The overall network will also express a single differentiable score function, as well as include a loss function for the last fully-connected layer of the network.\nSo what is the main difference between CNNs and other NNs? CNNs make the explicit assumption that the inputs are images. This assumption allows us to encode certain properties into the architecture, as well as make the forward function more efficient to implement and reduce the number of parameters in the network."
  },
  {
    "objectID": "posts/cnn-overview/index.html#summary",
    "href": "posts/cnn-overview/index.html#summary",
    "title": "An intro to Convolutional Neural Networks",
    "section": "Summary",
    "text": "Summary\nThis concludes my overview of the introductory “Convolutional Neural Networks” chapter from Fei-Fei Li’s CS231n course. I look forward to summarizing more such chapters in the future and sharing more examples of code walkthroughs of these topics in the future. Until next time!"
  },
  {
    "objectID": "posts/cnn-overview/index.html#references",
    "href": "posts/cnn-overview/index.html#references",
    "title": "An intro to Convolutional Neural Networks",
    "section": "References",
    "text": "References\n\nhttps://cs231n.github.io/convolutional-networks/"
  },
  {
    "objectID": "posts/newsletters/index.html",
    "href": "posts/newsletters/index.html",
    "title": "My blog / newsletter recommendations",
    "section": "",
    "text": "Welcome back to another edition of [VS]Codes! In this week’s post, I’ll be summarizing a list of the top blogs / newsletters to which I subscribe, including content that covers biotech and health AI, data science and software, and a few miscellaneous topics at the end. Note that all of these newsletters allow for free base subscriptions (with options for paid tiered subscriptions to support the creators if desired). Without further ado, let’s get started!\n\n\nMedical Informatics, Biotechnology, and Health AI\n\n“AMIA Informatics SmartBrief” by the American Medical Informatics Association\n“AMIA Informatics SmartBrief” offers a daily snapshot of biomedical informatics with news from a variety of health informatics sources. In particular, I appreciate that subscribers can select the topic categories that most interest them: I am personally subscribed to “Top News,” “Clinical Informatics and Analytics,” “Health Data Science and Artificial Intelligence,” “Population Health,” and “AMIA News”.\n“Decoding Bio” by Amee Kapadia, Pablo Lubroth, Patrick Malone, Morgan Cheatham, Ketan Yerneni, and Zahra Khwaja\n“Decoding Bio” breaks down advancements at the intersection of computation and the life sciences. The authors focus on translating technical developments and trends in the biotech and “tech-bio” spaces into an easy-to-read format. The goal of the newsletter and the larger writing collective is to start conversation around changes in the biotech industry, including the rapid advancement of health AI.\n“Doctor Penguin” by Emma Chen, Shreya Johri, Pranav Rajpurkar, and Eric Topol\nThe aim of “Doctor Penguin” is to help researchers keep up with the latest research updates in the field of AI and healthcare. Each week includes brief summaries of some of the latest top papers, as well as links to the original publications for subscribers to read.\n“The Century of Biology” by Elliot Hershberg\nElliot is a current PhD student in the Department of Genetics at Stanford, as well as a biotechnology investor. His blog, “The Century of Biology,” summarizes the latest progress in the field of biotechnology and offers three main topics of content: Data/Research, Companies/Strategy/Analysis, and Philosophy. Elliot’s long term ambition is “to develop a philosophy outlining why it is a moral and aesthetic imperative to pursue biotechnology.” Elliot is currently taking a break from writing his blog to defend his PhD, but there is a trove of content freely available that provides insightful examinations of biotech developments. I very much look forward to his return to writing later this year!\n\nData Science and Software\n\n“Software Design: Tidy First?” by Kent Beck\nKent Beck’s self-proclaimed mission is “to help geeks feel safe in the world.” His blog achieves this goal by covering a variety of topics in programming and software development that encourage a perspective shift in the way that we approach work in the field of tech and beyond.\n“AlphaSignal” by Lior Sinclair\nAlphaSignal is an AI-driven technical newsletter that helps the scientific community stay up to date with the Machine Learning industry by providing a round-up of publications and breakthroughs identified by its algorithm. Topics covered include relevant AI news, models, research, and repositories;.\n“The Present of Coding” by Abigail Haddad\nAbigail Haddad is a machine learning engineer and data scientist. Her blog focuses on topics in data science, coding, and Large Language Models. I very much appreciated two of most recent posts, “How to Become a Government Data Scientist” and “A Developer is You,” both of which have shifted the way that I think about learning and upskilling.\n“Monday Morning Data Science (MMDS)” by the Fred Hutch Data Science Lab (DaSL)\nMMDS offers a weekly dose of data news, curated by the Fred Hutch Data Science Lab (a.k.a. my team!). The newsletter covers not only updates from DaSL but also blog posts on data science and statistical content from a variety of other data science content creators.\n“Probably Overthinking It” by Allen Downey\nAllen Downey is a Principal Data Scientist at PyMC Labs, professor emeritus at Olin College, and along with other titles, the author of Probably Overthinking It. His blog of the same name tackles a variety of topics with non-intuitive answers from a statistical lens, proving with the power of computational statistics that everything is not as it seems!\n“Ready for R” by Ted Laderas\nTed is a co-worker of mine at Fred Hutch in charge of developing training material and directing communities of practice for Fred Hutch staff. His newsletter offers a glimpse into his outstanding abilities as an educator, helping readers learn the basics of rstats and the tidyverse. He also posts about new R packages and other resources that help readers upskill in their R knowledge.\n\nMiscellaneous\n\n“Barking Up The Wrong Tree” by Eric Barker\nEric’s weekly newsletter offers “a scientific deep dive that will improve your life.” Following along with much of the content from his The Wall Street Journal bestseller “Barking Up the Wrong Tree,” Eric’s blog offers actionable tips based on psychology, neuroscience, and expert insight to “make your life more awesome.”\n“The Doctor’s Kitchen” by Rupy Aujla\nThe goal of Rupy’s platform “The Doctor’s Kitchen” is to “teach people how to cook their way to health.” Rupy started the Doctor’s Kitchen as a way of teaching everybody how they can cook their way to health and to showcase the beauty of food and medicinal effects of eating and living well.\n“How To Be A Dog” by Andrew Knapp\nAndrew Knapp is the best-selling author of the Find Momo series, and he is a true artist in his ability to capture the spirit of life and adventure with his dogs and his camera. His blog highlights the lessons that dogs can teach us in our own lives with respect to appreciating the beauty around us and developing a deeper connection and appreciation for each and every day.\n\n\nThis concludes my blog post for today! Now a note for my subscribers… with fall rolling around and work / life picking up, it’s becoming more challenging to keep up with my weekly schedule of creating new content for the blog! So, I’ll be taking a hiatus for the next month to turn my attention to some immediate deliverables, while also developing new content in the background. When I start posting again toward the end of October, I will likely switch to a bi-weekly release schedule to make this whole project more sustainable with respect to my personal schedule! Until we meet again~"
  },
  {
    "objectID": "posts/work-life-harmony/index.html",
    "href": "posts/work-life-harmony/index.html",
    "title": "Why I think the pursuit of “work-life balance” is overrated",
    "section": "",
    "text": "Triumphant music swells in the background… Duh duh duhhh!!!\nWelcome back to my personal biomedical data science blog, [VS]Codes! It’s so great to be back writing on my platform and connecting with you all once again. I’m very happy to share that my brief hiatus from this blog was extremely productive and gave me a much-needed break - even though I am (finally) no longer in school, the month of September still presented an uptick in obligations as colleagues returned from their summer vacations, and I found myself struggling to balance my other deliverables with my writing for this blog. Indeed, by the middle of August, I found myself feeling more stressed than excited each time I had to produce new content. Now, however, I am re-energized and ready to fill your headspaces once again :) In the spirit of my (self-inflicted) burnout over the past couple of months, I thought it most appropriate to write today about a topic dear to my heart: work-life harmony.\nWork-life harmony is not a new idea by any means, but it exists as a stark reminder of the deficits of its more well-known counterpart: work-life balance. So let’s dive a little deeper into the differences between these two terms."
  },
  {
    "objectID": "posts/work-life-harmony/index.html#work-life-balance",
    "href": "posts/work-life-harmony/index.html#work-life-balance",
    "title": "Why I think the pursuit of “work-life balance” is overrated",
    "section": "Work-life balance",
    "text": "Work-life balance\nWork-life balance emphasizes the need to have a split between your professional and personal lives. The goal of this paradigm is to ensure that clear boundaries exist between the two “most important” parts of your life - when you are at work, you are fully focused on your work. And when you are out of work, you are fully focused on your personal life. The split between one’s personal and professional lives doesn’t need to be exactly 50/50 (maybe it’s 40/60 or 70/30), but there should be a relative “balance” in how much time is offered to both. The work-life balance mindset works great for many individuals, particularly for those who have to work in-person for their professional commitments. In these cases, coming home from the workday becomes a mandatory unplugging from the demands of professional life.\nHowever, for individuals without these forced boundaries (i.e. hybrid/remote workers), maintaining a balance between work and life can end up being more stress-inducing than relaxing! When you are forced to self-define the boundaries between work and life, you may end up feeling guilty every time work eats into your time with your family, or every time personal duties or free time pull you away from a commitment or responsibility you were expected to complete at work. Ultimately, for some people, the pursuit of a work-life balance can cause them to fall between the cracks. They can never feel completely present at work or at home - rather than being there for the people around them, they are trapped in a spiral of guilt."
  },
  {
    "objectID": "posts/work-life-harmony/index.html#work-life-harmony",
    "href": "posts/work-life-harmony/index.html#work-life-harmony",
    "title": "Why I think the pursuit of “work-life balance” is overrated",
    "section": "Work-life harmony",
    "text": "Work-life harmony\nThe paradigm of work-life harmony serves as a “reframing” of work-life balance. Where work-life balance aims to define clear boundaries between our personal and professional lives, work-life harmony emphasizes the inherent interconnectedness of these aspects of ourselves. The adage of “you are not your job” is certainly true, but at the same time, the work that you do everyday and the output that you produce are a major part of your identity! Attempting to fully silo these aspects of yourselves from the rest of your life is often more harmful than helpful.\nWork-life harmony emphasizes flexibility over rigidity - there is no need to feel ashamed if you think about your work when you’re on vacation. In fact, especially if you are excited and passionate about the work that you’re doing, then it is only natural that you will think about it outside of your workday! In a similar vein, there is no need to feel guilty if you want to take time away in the middle of a workday to spend time with your family or to prioritize your health. At the end of the day, we should measure ourselves not by how much continuous time we have spent on a given task, but instead by the positive impact that we have on others and the contributions that we make toward society. Ultimately, if we focus more on showing up for ourselves and leading our days in a manner that is most fulfilling with respect to our own needs, then we will be able to show up a hundred times over for our personal and professional obligations."
  },
  {
    "objectID": "posts/work-life-harmony/index.html#the-law-of-averages",
    "href": "posts/work-life-harmony/index.html#the-law-of-averages",
    "title": "Why I think the pursuit of “work-life balance” is overrated",
    "section": "The law of averages",
    "text": "The law of averages\nThe last point I want to end on today is a key piece of advice I received in the first week of my job from the head of my department, Jeff Leek. When I asked him how he was able to juggle the multitude of commitments that he faced daily, he (fittingly) responded to me with a statistical principle: “Follow the law of averages!”\nThis rule states that if you repeat a random event a sufficient number of times, the average outcome of the event will tend to converge toward an expected value. Sure, we may see outliers in our data every once in a while, but individual short-term fluctuations in outcomes do not matter - what matters at the end of the day is the expected value.\nIn a similar vein, we can treat each day of our lives as an individual outcome. It doesn’t necessarily matter if we have short-term fluctuations or trends in our data… as long as we hit our expected averages. If there’s an important work deadline coming up or you just find yourself particularly excited about a project that you’re working on, then spend time for a week or two outside of your 9-5 going relentlessly after those goals. And if you find yourself feeling a lack of motivation on a given day or have to take time off for a couple of weeks to attend to a friend or family member in need, then do so without guilt. Ultimately, by following the law of averages, we can ensure that we’re staying flexible with respect to our requirements and showing up fully for ourselves and our loved ones."
  },
  {
    "objectID": "posts/work-life-harmony/index.html#additional-resources",
    "href": "posts/work-life-harmony/index.html#additional-resources",
    "title": "Why I think the pursuit of “work-life balance” is overrated",
    "section": "Additional resources",
    "text": "Additional resources\nHopefully my personal interpretation of the differences between work-life harmony and balance was useful to you! At the same time, as I mentioned earlier in this post, work-life harmony is not a new concept, and there are several resources freely available online that go into more detail and provide more varied perspectives. Here are some additional links related to work-life harmony (and a video on the law of averages) that you may find informative:\n\n“From Work-Life Balance to Work-Life Harmony: A New Path for High Achievers,” by Chibs Okereke\n“What Is Work-Life Balance vs. Work-Life Harmony?” by Maryville University\n“What is Work-Life Harmony? (and how you can achieve it),” by Afoma Umesi\n“Law of Averages - How To Be Successful In Anything You do,” by Improvement Pill\n\n\nThis concludes my blog post for today - It’s great to be posting again, and I look forward to sharing similarly styled overviews of work concepts and paradigms in the future. My cadence of blog posts may shift from weekly to bi-weekly depending on work commitments, but I will definitely be back soon! Until next time!"
  },
  {
    "objectID": "posts/lhs/index.html",
    "href": "posts/lhs/index.html",
    "title": "An overview of learning health systems",
    "section": "",
    "text": "Welcome back to another post from my personal biomedical data science blog, [VS]Codes! In today’s post, I will provide a brief overview of the concept of “learning health systems” and how this model is shifting the way that healthcare is approached today.\n\n\n\n\n\n\nTranslational informatics: advancing medical care from “bench to bedside”\nThe discipline of “translational informatics” focuses on the translation of data generated from research-based biomedical endeavors to applications in the clinic, allowing for the improvement of disease diagnosis, staging, prognosis, and treatment. Translational research will typically involve the development of machine learning methods to integrate multimodal, heterogeneous biomedical data for clinical decision support. Ultimately, translational informatics enables the study of increasingly large bodies of biomedical data to inform predictive, preventive, and personalized health applications. Translational informatics can also be deeply tied to the concept of “bench to bedside” biomedical discovery, where the results of research conducted in the laboratory are directly used to create actionable innovations for real-world healthcare settings.\n\n\nLearning Health Systems: implementing the translational medicine paradigm\nLearning Health Systems are intrinsically linked to the concepts of “bench to bedside” care and translational informatics. A Learning Health System (LHS) refers to a healthcare model that makes use of continual data collection and analysis for the improvement of patient care. The key idea driving LHSs is the cyclical nature of advancements in research and clinical care: real-time data coming from routine care and patient experiences inform new directions in biomedical research, improving the way that clinical practice is performed. This continuous learning feedback loop is crucial for the improvement of patient outcomes and longer-term healthcare innovation.\nAs defined by the National Academy of Medicine (NAM), the four key elements of LHSs are:\n\nGeneration, application, and improvement of scientific knowledge\nOrganizational infrastructure to support the engagement of patient communities, healthcare professionals, and researchers for the identification of evidence gaps between biomedical research and clinical care\nDeployment of computational technologies and informatics approaches to organize and leverage large-scale electronic health data for use in research\nQuality improvement at the point of care for each patient using new knowledge generated by research\n\nSome examples of downstream applications of LHSs include:\n\nCohort development for clinical trials and research studies through the collection of patients with similar attributes to one another\nIdentification of both improved as well as sub-optimal examples of patient care and treatments as compared to standardized benchmarks\nThe creation of patient risk models for adverse events and outcomes\nClinical Decision Support (CDS) systems to recommend personalized treatment options\nAutomation of routine care processes\nSurveillance monitoring for disease outbreaks and other treatment issues and complications\n\n\n\nBenefits and challenges of Learning Health Systems\nKey benefits of LHSs include better patient outcomes through personalized care and faster implementation of data-driven research advancements, as well as lowered healthcare costs through the implementation of improved processes and the reduction of ineffective treatments.\nHowever, numerous challenges have also hindered the wide-spread adoption of LHS principles globally. Some of the biggest obstacles to the broader implementation of these guidelines include:\n\nMaintaining data interoperability and security: multimodal, heterogeneous data are often siloed across healthcare systems, making it challenging to integrate data across departments and organizations. Furthermore, these data typically do not follow consistent common data standards, further complicating the process for data integration. LHSs also need to ensure that a system for data democratization is in place, including appropriate data access rights as well as ethical data stewardship that promotes not only data sharing but also patient privacy.\nOvercoming cultural resistance and realigning healthcare incentives: Setting up an LHS requires significant investments in technology infrastructure, data management systems, and staff training. Healthcare organizations are typically slow to adopt such new practices, particularly when they require drastic changes in traditional workflows. Instead, gradual shifts must take place to bring researchers and healthcare providers to alignment on priorities and timelines. Overall healthcare incentives must be shifted from volume to value, and appropriate performance metrics will need to be developed to hold clinicians and their teams accountable toward patient care.\nEnsuring data quality and minimizing bias: The mantra of “garbage in, garbage out” is particularly pertinent in the LHS model of continuous data generation and ingestion - if the data used to train the healthcare system are biased, incomplete, or inaccurate, then the overall reliability and applicability of the system will be drastically reduced.\nKeeping pace with technological advancement and ensuring proper model validation: Rapid advancements in artificial intelligence and data analytics mean that healthcare organizations must constantly stay abreast of new tools and methodologies while continuing to fulfill their current care duties. Given the meteoric changes in the field of information technology, it is essential that a proper system is put in place to safely incorporate the latest shifts in technology into the workings of each LHS. Furthermore, appropriate global standards and metrics must be developed and agreed upon to help determine the efficacy of changes introduced to each LHS.\n\n\n\nLooking toward the future: an iterative process\nWhile many healthcare systems have begun to incorporate elements of LHSs into their workflows, the complete LHS is still very much an aspirational model. Becoming a complete LHS is an iterative process that requires a step-by-step cultural shift toward the effective use of data across both clinical and research settings. Future advancements toward the LHS model will necessitate progress in multiple categories, including technology, regulatory policies and funding, and the direct involvement and engagement of patients.\n\n\n\nReferences\n\nJohns Hopkins Berman Institute of Bioethics - What is a Learning Health System?\nAgency for Healthcare Research and Quality - About Learning Health Systems\nWikipedia - Learning health systems\nMcLachlan et al. 2019 - LAGOS: learning health systems and how they can integrate with patient care\nStanford Medicine Center for Biomedical Informatics Research - Translational Informatics"
  }
]