[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vivek Sriram, PhD, MA",
    "section": "",
    "text": "Welcome to my website! I am a biomedical data scientist and health machine learning researcher based in Seattle, WA.\nMy general research skills and interests include:\n\ntranslational bioinformatics and personalized medicine\nbig data analysis\ndeep learning and generative AI\ndata visualization\nnetwork science\nhuman-centered design\n\nI am always open to speaking, teaching, collaboration, consulting, and outreach opportunities. Feel free to reach out to me with questions or requests at vivek.sriram@gmail.com."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I consider myself a computational scientist with a passion for people-focused projects. I currently work as a clinical data scientist at the Fred Hutchinson Cancer Center Data Science Lab (DaSL) in Seattle, WA. My role centers on the development and productization of statistical models and machine learning systems for electronic phenotyping and cohort identification across the institution. I also serve as a guest lecturer in the Department of Biostatistics, Epidemiology, and Informatics at the University of Pennsylvania and a collaborator with the Health Futures Group at Microsoft Research.\nPrior to my position at Fred Hutch, I completed my Ph.D. and postdoctoral training in Biomedical Informatics and Computational Genomics at the University of Pennsylvania Perelman School of Medicine. There, I worked with Dr. Dokyoon Kim’s Integrative ’Omics and Biomedical Informatics Lab, developing machine learning and graph-based methods to identify genetic contributors to disease comorbidities from large-scale multimodal biomedical data (You can listen to a recording of my public thesis defense here!)\nIn addition to my doctoral degree, I hold a M.A. in Statistics and Data Science from the Wharton School of Business, as well as a B.Sc. with honors in Statistics, a B.Sc. in Computer Science, and a minor in Computational Biology from Duke University."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "[VS]Codes",
    "section": "",
    "text": "Welcome to [VS]Codes, a biomedical data science and clinical informatics blog mixed with professional advice and miscellaneous detours, written by Vivek Sriram. To subscribe to my blog and get updates on my new posts directly in your inbox, click here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy doctoral research: takeaways and advice\n\n\n\n\n\n\nPersonal\n\n\nAdvice\n\n\n\nWe’re all in this together\n\n\n\n\n\nJun 24, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nMy doctoral research: the content\n\n\n\n\n\n\nPersonal\n\n\n\nConte(n)t is everything\n\n\n\n\n\nJun 17, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nMy doctoral research: the background\n\n\n\n\n\n\nPersonal\n\n\n\nContext is everything\n\n\n\n\n\nJun 10, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nBiomedical Data Science sub-disciplines (v0)\n\n\n\n\n\n\nOverviews\n\n\n\nWhat’s in a word?\n\n\n\n\n\nJun 3, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nCorrecting batch effects in single cell RNA-seq data with Monocle 3\n\n\n\n\n\n\nTutorials\n\n\n\nTakeaways from the SASC User Group Workshop #2\n\n\n\n\n\nMay 27, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nMy professional journey\n\n\n\n\n\n\nPersonal\n\n\n\nLife is a highway\n\n\n\n\n\nMay 15, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\n\n\n\n\n\n\nHello world!\n\n\n\n\n\n\nPersonal\n\n\n\nThe start of a new chapter\n\n\n\n\n\nMay 14, 2024\n\n\nVivek Sriram\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Sriram V§, Nam Y§, Shivakumar M, Verma A, Jung S-H, Lee SM*, Kim D*. A Network-Based Analysis of Disease Complication Associations for Obstetric Disorders in the UK Biobank. Journal of Personalized Medicine. 2021; 11(12):1382. https://doi.org/10.3390/jpm11121382\nNam Y§, Jung S-H§, Verma A, Sriram V, Wong H-H, Yun J-S, Kim D*. netCRS: Network-based comorbidity risk score for prediction of myocardial infarction using biobank-scaled PheWAS data. Pacific Symposium on Biocomputing 2022. https://doi.org/10.1142/9789811250477_0030\nSriram V§, Shivakumar M§, Jung S-H, Nam Y, Bang L, Verma A, Lee S, Choe EK, Kim D, NETMAGE: A human disease phenotype map generator for the network-based visualization of phenome-wide association study results, GigaScience, Volume 11, 2022, giac002, https://doi.org/10.1093/gigascience/giac002\nLee S§, Nam Y§, Choi ES, Jung YM, Sriram V, Leiby J, Koo JN, Oh IH, Kim BJ, Kim SM, Kim SY, Kim GM, Joo SK, Shin S, Norwitz E, Park CW, Jun JK, Kim W, Kim D*, Park JS*. Development of early prediction model for pregnancy-associated hypertension with graph-based semi-supervised learning. Scientific Reports, 12(1), 15793, 2022. https://doi.org/10.1038/s41598-022-15391-4\nNam Y§, Jung SH§, Yun JS, Verma A, Sriram V, Shin H, Won H*, Kim D*. Discovering comorbid diseases using an inter-disease interactivity network based on biobank-scale PheWAS data. Bioinformatics 39(1), 2023. https://doi.org/10.1093/bioinformatics/btac822\nWoerner J§, Sriram V§, Nam Y, Verma A, Kim D*. Uncovering genetic associations in the human diseasome using an endophenotype-augmented disease network. Bioinformatics 40(3), 2024. https://doi.org/10.1093/bioinformatics/btae126.\nSriram V, Conard AM, Rosenberg I, Kim D, Hall AK*. Accelerating precision medicine: a proposed framework for large-scale multiomic data integrity, interoperability, analysis, and collaboration in biomedical discovery. medRXiv. https://doi.org/10.1101/2024.03.15.24304358. Preprint.\nNam Y§, Sriram V§, Shivakumar M, Verma A, Yun JS, Kim D*. An enhanced disease network with robust cross-phenotype relationships via variant frequency-inverse phenotype frequency. Preprint.\nSriram V, Woerner J, Ahn YY*, Kim D*. The interplay of sex and genotype in disease associations: a comprehensive network analysis in the UK Biobank. Preprint."
  },
  {
    "objectID": "posts/240514_hello-world/index.html",
    "href": "posts/240514_hello-world/index.html",
    "title": "Hello world!",
    "section": "",
    "text": "Hello world, and welcome to my official blog! I’ve had numerous thoughts swirling around in my head over the past several years, but have had trouble formally committing to bringing them out into the world… however, encouragement from colleagues and recent reading (i.e. Dorie Clark’s The Long Game) have convinced me to finally bring my virtual pen to paper. In particular, I found myself tremendously inspired by the Fred Hutch DaSL Culture and Work Style document and the mantra of “ship as soon as you can.” The following graphic does a great job at reminding us how to avoid the pit of perfectionism!\n\nMy hopes for this blog are to (a) build a community, (b) share my knowledge, and (c) learn alongside my readers. I aim to write weekly (if not more frequent) posts on a variety of subjects, including:\n\nmy personal story\nadvice to others in my field\ntechnical tutorials\nsubject matter overviews, and\nmiscellaneous detours\n\nI have no expectations that my content will be perfect, but then again, learning from mistakes is one of the best parts of life :)\nGiven my background and expertise, my general focus will be on the world of biomedical data science and health technology, but the sky is the limit for the topics we may cover!\nThanks for tuning in - I’m delighted to have you along for the ride…"
  },
  {
    "objectID": "posts/240515_professional-journey/index.html",
    "href": "posts/240515_professional-journey/index.html",
    "title": "My professional journey",
    "section": "",
    "text": "For my first “real” post, I figured it would be a helpful exercise to go through my professional journey and track how I ended up where I currently am. This description is meant to be more of an overview, and I intend to provide more details on individual portions of it in the future.\n\nHaving grown up in the Silicon Valley, I always had a front-row seat to the power and potential of technology to improve people’s lives. All of the biggest tech companies had their headquarters within driving distance from my home, and every day, I could see how they had impacted not only myself but also everyone around me: computers, cellphones, social media, education, automobiles, entertainment… everything was shaped by information technology. Both of my parents were in the software industry too, and seeing the productive, fulfilling jobs that they were able to have made me certain that I wanted to involve technology in my future career. Learning how to code from my mother in high school made me feel like I was being imparted with some special kind of magic - entering the world of software engineering truly felt right at my fingertips.\nAt the same time, growing up in the Silicon Valley felt like growing up in a bubble. I yearned to explore the link between information technology and its downstream applications beyond my baseline understanding of how to write code. So, I sought out more. In high school, I took a breadth of science classes, and I found myself inspired by the concept of “interdisciplinarity”. Instead of being drawn to “information technology,” I was drawn closer and closer to the world of “information science” and its applications to multiple disciplines, including biology and medicine. My drive for interdisciplinary experiences and my desire to explore a world of technological applications outside of the Bay Area led me to Duke University for my undergraduate education - in fact, one of the mantras of the university was “creative thinking across intellectual boundaries”. At Duke, I completed double majors in Computer Science and Statistics. Duke’s affiliated medical campus also gave me chances to explore interdisciplinary applications in the world of biomedical informatics, and I pursued multiple research opportunities, including an Honor’s thesis for my Statistics degree under the supervision of Dr. Li Ma. I also completed a minor in Computational Biology, taking classes such as Computational Genomics with Dr. Alexander Hartemink and Computational Structural Biology with Dr. Bruce Donald. Lastly, I was lucky to have great summer internship mentors (including Li-Yuan Chern at Pharmacyclics and Drs. Zichen Wang and Avi Ma’ayan at the Icahn School of Medicine), who kept me motivated and inspired to stick to my path and pursue an advanced interdisciplinary career.\nBy the end of my undergrad, I knew that I wasn’t going to enter the traditional computer science recruitment cycle for software engineering roles - I instead applied to doctoral programs in biomedical informatics and computational biology that would allow me to build my knowledge base further and prepare me to become a leader in impactful projects that made a clear benefit in people’s lives. I was lucky to earn an admission with the Genomics and Computational Biology program at the University of Pennsylvania Perelman School of Medicine. Again, great mentors from my classes and research rotations (including Drs. Ryan Urbanowicz and Marylyn Ritchie to name a few) helped advance my training and made me a better researcher and scientist day by day. I am most indebted to my PhD advisor, Dr. Dokyoon Kim, for his support and mentorship throughout my PhD and subsequent post-doctoral position. With his leadership style and work ethic, he was a true role model for me throughout my graduate degree. I also fell in love with the combination of technical research and scientific storytelling that came out of my dissertation (to be discussed in a later post). During my PhD, I was able to complete a Master’s degree in Statistics and Data Science from the Wharton School of Business under the supervision of Dr. Anderson Zhang, as well as a summer internship as a User Experience Researcher in Health AI/ML under the supervision of Dr. Mandi Hall with the Health Futures team at Microsoft Research. All of these opportunities helped me to refine a set of motivators for my long-term career:\n\nimpact\nconnection\npassion\nleadership\n\nThese values aided me tremendously in my search for my first job upon the completion of my PhD. Today, I work as a clinical data scientist in the Translational Analytics and Informatics group at the Fred Hutchinson Cancer Center’s Data Science Lab (DaSL) in Seattle, WA (also to be discussed more in a future post). I am tremendously grateful to be a part of a supportive, driven community of fellow data scientists and researchers as we develop the clinical data infrastructure at Fred Hutch, and I look forward to sharing more about my work and career as the years progress!"
  },
  {
    "objectID": "posts/sasc-workshop2/index.html",
    "href": "posts/sasc-workshop2/index.html",
    "title": "Correcting batch effects in single cell RNA-seq data with Monocle 3",
    "section": "",
    "text": "In this week’s blog, I’ll be summarizing takeaways and a code example from the Seattle Area Single Cell (SASC) User Group’s second workshop of the year, which was held on May 16th, 2024. Slides from the workshop can be found here.\nThe SASC User Group, directed by Dr. Mary O’Neill at the Brotman Baty Institute, is designed to create connections and foster community among single-cell researchers at Fred Hutch, UW Medicine, Seattle Children’s, as well as other Seattle-area researchers working with single-cell data. If you are interested in joining the group, you can subscribe to their listserv here. The group holds quarterly meetings rotating across the three campuses, each with a different focus. May’s workshop was dedicated to applying batch correction methods using Monocle 3 to analyze single-cell RNA-seq (scRNA-seq) data.\n\nAll credit for the data and code in this workshop goes to Mary and the folks at the BBI who helped organize this community. I have simply summarized their content and added a few clarifiers in various sections! I claim no significant knowledge myself of working with single cell data - in the future, I hope to release a post that highlights some more of the biological context highlighted through this workshop. You can follow along with the original tutorial and code example at the SASC GitHub page here.\nAnd so, with context out of the way, let’s get started!\n\n\n1. Background\nMonocle 3 is “an analysis toolkit for single-cell RNA-seq data”, developed by the Trapnell Lab at the University of Washington Department of Genome Sciences.\nWhen analyzing any form of data, especially single cell data, it is important to keep the right sources of variation (see Aquino, Bisiaux, Li et al., Nature 2024). Batch effects refer to technical, non-biological factors that cause variation in data, and must be appropriately addressed to avoid confounding in results.\nThe best way to get around batch effects is to avoid introducing them in the first place! Nothing can salvage a poor study design. In a similar vein, it is important to determine whether or not there are actually batch effects in the first place that are influencing your data. Sometimes, batch corrections can introduce more artifacts than they alleviate. So, when applying batch correction methods, apply them thoughtfully. Know what they are doing, what to use them for, and where they can lead you astray.\nA variety of batch correction methods exist for scRNA-seq data (see Antonsson, Melsted, bioRXiv 2024). Generally, single cell analysis falls into two camps:\n\nBatch correction is only for visualization. The batch category is used as a covariate in downstream analysis\nBatch correction is incorporated into the data processing pipeline. Batch corrected data are used in downstream analysis.\n\nFor our code example, the data we are using represent a subset of heart data generated by the BBI after processing through Scale Biosciences’ and Parse Biosciences’ respective single-cell sequencing assays. The same samples are used in both assays - each sample had two different donors. Data were mixed together, and then genetic demultiplexing was performed.\nWe start off by calling our required packages for analysis. In this case, we are using R version 4.4.0. Refer to the following links (or the SASC GitHub page) for help with installing required packages:\n\nBioconductor Installation\nMonocle 3 Installation\nkBET Installation\nHarmony Installation\n\n\nsetwd(\"~/Documents/Developer/vsriram24.github.io/posts/sasc-workshop2\")\n\n#load required packages\nlibrary(monocle3)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(randomcoloR)\nlibrary(kBET)\nlibrary(cowplot)\nlibrary(harmony)\nlibrary(uwot)\nlibrary(batchelor)\n\n\n\n2. Reading in the data\nWith our environment ready to go, we start by reading in our input scRNA-seq data. This dataset is in the form of an S4 object, the standard format for representing sequencing data in Monocle 3. Other packages such as Seurat will have their own file formats to represent data.\n\n#Read in the cell data set containing a random sub-sampling of 35K barcodes from\nsasc &lt;- readRDS(\"BBI_heart_hs_mix_36601humangenes_35000barcodes.RDS\")\n\nWe can use the detect_genes function to count how many cells in our data are expressed above a minimum threshold.\n\nsasc &lt;- detect_genes(sasc)\n\nexpressed &lt;- data.frame(rowData(sasc)) %&gt;% \n  arrange(desc(num_cells_expressed))\n\nWe then use the n.umi attribute from the output of detect_genes to see how many unique molecular identifiers (UMIs) are in our data.\n\nsummary(sasc$n.umi)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    100     306     837    2084    2311  179927 \n\n\nLooking at the donor attribute of our data, and we see that there are numerous doublet (appearing for both donors) as well as unassigned (appearing for neither donor) UMIs in our data that may belong to either of our donors.\n\ntable(sasc$donor)\n\n\n         0          1    doublet unassigned \n     16410      14938        274       3378 \n\n\nWe are also able to see the breakdown of UMIs processed by our Parse and Scale scRNA-seq assays respectively.\n\ntable(sasc$batch)\n\n\nParse Scale \n16263 18737 \n\n\n\n\n3. Quality control\nNow that we have a breakdown of our data, we can perform quality control.\nWe start by calculating the mitochondrial DNA content in our scRNA-seq data. The presence of mitochondrial DNA (mtDNA) in our data represents low quality calls.\n\n# Search for genes with \"MT\" in their name.\nfData(sasc)$MT &lt;- grepl(\n  \"MT-\", \n  rowData(sasc)$gene_short_name\n)\n\ntable(fData(sasc)$MT)\n\n\nFALSE  TRUE \n36588    13 \n\n\nBased upon the mitochondrial DNA content we calculated, we can evaluate the percentage of mitochondrial reads in our data.\n\npData(sasc)$MT_reads &lt;- Matrix::colSums(exprs(sasc)[fData(sasc)$MT,])\npData(sasc)$MT_perc &lt;- pData(sasc)$MT_reads/Matrix::colSums(exprs(sasc))*100\n\nsummary(sasc$MT_perc)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.1303  0.9560  2.8839  3.2258 61.3861       1 \n\n\nWith our calculated mitochondrial percentages, we can start to visualize our data.\nLet’s look at a plot of genes by UMIs, colored by mitochondrial percentage.\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = n.umi, y = num_genes_expressed)) +\n  facet_wrap(~batch, nrow = 1) +\n  geom_point(size = 0.5, alpha = 0.3, aes(color = MT_perc)) +\n  theme_light() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14),\n        axis.title = element_text(size = 16),\n        axis.text.y = element_text(size = 14),\n        aspect.ratio = 1) +\n  scale_color_viridis() +\n  xlab(\"UMIs\") +\n  ylab(\"Number of Genes Captured\") +\n  scale_y_log10() +\n  scale_x_log10() +\n  geom_abline(slope = 1, color = \"grey\") +\n  geom_hline(yintercept = 200, linetype = \"dotted\", color = \"red\") +\n  geom_vline(xintercept = 300, linetype = \"dotted\", color = \"red\")\n\n\n\n\n\n\n\n\nWe can see that in spite of the fact that our two samples were identical, different mitochondrial content are exhibited across our assays. In particular, we see a lot of noise for the UMIs that correspond to under 200 genes.\nNow let’s look at mitochondrial percentage by donor type.\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = donor, y = MT_perc)) +\n  facet_wrap(~batch, nrow = 1, drop = TRUE, scales = \"free_x\") +\n  geom_violin(aes(fill = batch)) +\n  geom_boxplot(notch = T, fill = \"white\", width = 0.25, alpha = 0.3, outlier.shape = NA) +\n  theme_light() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14),\n        axis.title = element_text(size = 16),\n        axis.text.y = element_text(size = 14)) +\n  xlab(\"\") +\n  ylab(\"MT %\") +\n  theme(legend.position = \"none\")\n\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\n\n\n\n\n\n\n\n\n\nWe again see differences across our assays here in terms of MT percentage - in particular, it seems like we would want to have mitochondrial percentage no greater than 10% in our data.\nNow let’s look at UMIs by donor type.\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = donor, y = n.umi)) +\n  facet_wrap(~batch, nrow = 1, drop = TRUE, scales = \"free_x\") +\n  geom_violin(aes(fill = batch)) +\n  geom_boxplot(notch = T, fill = \"white\", width = 0.25, alpha = 0.3, outlier.shape = NA) +\n  theme_light() +\n  geom_hline(yintercept = 300, linetype = \"dashed\", color = \"blue\") + #change to thresholds\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14),\n        axis.title = element_text(size=16),\n        axis.text.y = element_text(size = 14)) +\n  scale_y_log10() +\n  xlab(\"\") +\n  ylab(\"UMIs\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nHere, it seems like data are hard to differentiate under a certain number of UMIs (in this case, 300).\nSo, let’s establish a QC filter for our data: 300 UMIs, 200 genes, and over 10% mitochondrial content.\n\n#Let's remove everything under 300 UMIs, 200 genes, and over 10% mitochondrial percentage\nsasc$qcflag &lt;- ifelse(\n  sasc$n.umi &gt;= 300 & \n  sasc$num_genes_expressed &gt;= 200 & \n  sasc$MT_perc &lt; 10, \n  \"PASS\", \n  \"FAIL\"\n)\n\ntable(sasc$qcflag)\n\n\n FAIL  PASS \n10180 24820 \n\nsasc &lt;- sasc[,sasc$qcflag == \"PASS\"] #filter out failing barcodes\nsasc\n\nclass: cell_data_set \ndim: 36601 24820 \nmetadata(1): cds_version\nassays(1): counts\nrownames(36601): ENSG00000243485 ENSG00000237613 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24820): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(12): barcode n.umi ... MT_perc qcflag\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nLet’s also remove UMIs that were mapped to the wrong genome, nonassignable to a donor, or deemed a doublet.\n\nsasc &lt;- sasc[,sasc$organism == \"human\" & sasc$donor %in% c(0, 1)]\nsasc\n\nclass: cell_data_set \ndim: 36601 24216 \nmetadata(1): cds_version\nassays(1): counts\nrownames(36601): ENSG00000243485 ENSG00000237613 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(12): barcode n.umi ... MT_perc qcflag\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nFinally, we have our semi-cleaned data!\n\ntable(sasc$batch)\n\n\nParse Scale \n11474 12742 \n\ntable(sasc$batch, sasc$donor)\n\n       \n           0    1 doublet unassigned\n  Parse 8637 2837       0          0\n  Scale 4506 8236       0          0\n\n\n\n\n4. Pre-processing and data visualization\nAfter completing quality control of our data, we can start to pre-process it.\n\n# remove non-expressed/non-captured genes \nhist(fData(sasc)$num_cells_expressed)\n\n\n\n\n\n\n\n\nWe can also conduct feature selection for genes if we want. This step is not necessary in every case, but it can help with reducing computational loads. Here, we filter out genes that are not expressed in at least 25 cells.\n\ntable(fData(sasc)$num_cells_expressed &gt; 25)\n\n\nFALSE  TRUE \n15315 21286 \n\n# filter out genes not expressed in at least 25 cells\nsasc &lt;- sasc[fData(sasc)$num_cells_expressed &gt; 25, ]\nsasc \n\nclass: cell_data_set \ndim: 21286 24216 \nmetadata(1): cds_version\nassays(1): counts\nrownames(21286): ENSG00000238009 ENSG00000241860 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(12): barcode n.umi ... MT_perc qcflag\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nNow we apply the estimate_size_factors function from Monocle 3 to evaluate the relative bias in each cell.\n\nsasc &lt;- estimate_size_factors(sasc)\n\nAfter estimating size factors, we can run preprocess_cds, a standardized workflow in the Monocle 3 package that normalizes the data by log and size factor to address depth differences, and then calculates a lower dimensional space that will be used as the input for further dimensionality reduction.\n\nset.seed(1000)\nsasc &lt;- preprocess_cds(sasc) #this may take a few minutes\n\nWe can then call reduce_dimensions from Monocle 3 on our data to get down to the most relevant components.\n\nset.seed(1000)\nsasc &lt;- reduce_dimension(sasc) #this may take a few minutes\n\nNo preprocess_method specified, using preprocess_method = 'PCA'\n\n\nNow let’s plot the cells in our data (first colored by batch and then by donor)\n\nplot_cells(sasc, color_cells_by = \"batch\")\n\nNo trajectory to plot. Has learn_graph() been called yet?\n\n\n\n\n\n\n\n\nplot_cells(sasc, color_cells_by = \"donor\")\n\nNo trajectory to plot. Has learn_graph() been called yet?\n\n\n\n\n\n\n\n\n\nWe now have our dimensionality-reduced data! Let’s use k-means clustering to categorize our cells into clusters.\nOne can (and should) spend a lot of time tweaking their clustering parameters. In this situation, we’ll go with a k of 40 for Leiden clustering. It is advisable to try several k-values and/or resolutions during this exploratory data analysis.\n\nsasc &lt;- cluster_cells(\n  sasc, \n  k=40, \n  cluster_method=\"leiden\", \n  random_seed=1000\n) #this may take a few minutes\n\n# add cluster information for each cell\ncolData(sasc)$k40_leiden_clusters = clusters(sasc) \n\nplot_cells(sasc)\n\n\n\n\n\n\n\n\nLet’s get the top marker genes based on our Leiden clustering:\n\ntop_marker_genes &lt;- top_markers(\n  sasc, \n  group_cells_by=\"k40_leiden_clusters\"\n)\n\nkeep &lt;- top_marker_genes %&gt;%\n  filter(fraction_expressing &gt;= 0.30) %&gt;%\n  group_by(cell_group) %&gt;%\n  top_n(3, marker_score) %&gt;%\n  pull(gene_short_name) %&gt;%\n  unique()\n\n\nplot_genes_by_group(\n  sasc,\n  c(keep),\n  group_cells_by = \"k40_leiden_clusters\", #\"partition\", \"cluster\"\n  ordering_type = \"maximal_on_diag\",\n  max.size = 3\n)\n\n\n\n\n\n\n\n\nLet’s do some more data visualization here. We’ll move out of Monocle 3 and into ggplot2 to improve our flexibility with plotting.\n\n#add UMAP coordinates to the colData for easy plotting\nsasc$UMAP1 &lt;- reducedDim(sasc, \"UMAP\")[,1]\nsasc$UMAP2 &lt;- reducedDim(sasc, \"UMAP\")[,2]\n\n\n#generate a distinguishable color scheme\nset.seed(1000)\ncolpal &lt;- randomcoloR::distinctColorPalette(k=12)\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x=UMAP1, y=UMAP2, color=cell_type)) +\n  facet_wrap(~batch+donor) +\n  geom_point(size=0.5, alpha=0.5) +\n  theme_bw() +\n  scale_color_manual(values=colpal) +\n  theme(legend.position=\"bottom\", aspect.ratio = 1, panel.grid=element_blank()) +\n  guides(color = guide_legend(override.aes = list(size=8, alpha=1)))\n\n\n\n\n\n\n\n\nFrom our visualizations, clusters 2, 3, 4, and 9 all appear to be related cell types - ventricular cardiomyocytes.\nWe can also see across our four plots that we have clear evidence of both technical (batch) and biological (donor) variation!\n\n\n5. Quantifying a batch effect\nNow that we have evidence of a batch effect, let’s quantify it! We’ll make use of the kBET (k-nearest neighbor batch effect test) package from the Theis lab.\n\n#kBET - k-nearest neighbour batch effect test\ndata &lt;- reducedDim(sasc)\nbatch &lt;- sasc$batch\n\nsubset_size &lt;- 0.1 #subsample to 10% of the data for speed\nsubset_id &lt;- sample.int(\n  n = length(batch), \n  size = floor(subset_size * length(batch)), \n  replace=FALSE\n)\n\nset.seed(1000)\nbatch.estimate &lt;- kBET(\n  data[subset_id,], \n  batch[subset_id]\n) #this may take a few minutes\n\n\n\n\n\n\n\n\n\nbatch.estimate$summary\n\n      kBET.expected kBET.observed kBET.signif\nmean    0.002482853     0.9888066           0\n2.5%    0.000000000     0.9753086           0\n50%     0.002743484     0.9917695           0\n97.5%   0.006207133     1.0000000           0\n\n\nBased on our rejection rate plot, it really does seem that we have a batch effect in our data. We can simulate a random batch assignment in our data and look at the same plot to convince ourselves of this observation.\n\nset.seed(1000)\nrandombatch &lt;- sample(sasc$batch, dim(sasc)[2])\nbatch.estimate.fake &lt;- kBET(\n  data[subset_id,], \n  randombatch[subset_id]\n) #this may take a few minutes\n\n\n\n\n\n\n\n\nAlright, so we clearly do have a batch effect!\n\n\n6. Batch correction\nSince we’ve proven that we have a batch effect in our data, let’s perform batch correction using Monocle 3. We make use of the align_cds function, a wrapper built around the reducedMNN function from the batchelor package developed by the Marioni Lab at the University of Cambridge.\n\n### Batch correction built-in to Monocle3\nset.seed(1000)\nbc_cds &lt;- align_cds(\n  sasc, \n  alignment_group = \"batch\", \n  k = 50\n) #this may take a minute\n\nAligning cells from different batches using Batchelor.\nPlease remember to cite:\n     Haghverdi L, Lun ATL, Morgan MD, Marioni JC (2018). 'Batch effects in single-cell RNA-sequencing data are corrected by matching mutual nearest neighbors.' Nat. Biotechnol., 36(5), 421-427. doi: 10.1038/nbt.4091\n\nbc_cds\n\nclass: cell_data_set \ndim: 21286 24216 \nmetadata(2): cds_version citations\nassays(1): counts\nrownames(21286): ENSG00000238009 ENSG00000241860 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(16): barcode n.umi ... UMAP1 UMAP2\nreducedDimNames(3): PCA UMAP Aligned\nmainExpName: NULL\naltExpNames(0):\n\n\nBased on our ‘aligned’ PCA, we can then call reduce_dimensions to generate a corresponding UMAP.\n\n# We can run reduce_dimensions to generate a UMAP from the 'aligned' PCA\nset.seed(1000)\nbc_cds &lt;- reduce_dimension(\n  bc_cds, \n  reduction_method = \"UMAP\", \n  preprocess_method = \"Aligned\"\n)\n\nsasc$aligned_UMAP1 &lt;- reducedDim(bc_cds, \"UMAP\")[,1] #save these in our original cds\nsasc$aligned_UMAP2 &lt;- reducedDim(bc_cds, \"UMAP\")[,2] #save these in our original cds\n\nplot_cells(bc_cds, color_cells_by = \"batch\")\n\nNo trajectory to plot. Has learn_graph() been called yet?\n\n\n\n\n\n\n\n\n\nFinally, we use kBET again to quantitatively verify that we have removed our batch effect from our data.\n\n#Use kBET to quantitatively ask if it removes the batch effect\ndata &lt;- reducedDim(bc_cds, \"UMAP\") #note that we are running this on the UMAP\nbatch &lt;- bc_cds$batch\nsubset_size &lt;- 0.1 #subsample to 10% of the data\nsubset_id &lt;- sample.int(\n  n = length(batch), \n  size = floor(subset_size * length(batch)), \n  replace = FALSE\n)\n\nset.seed(1000)\nbatch.estimate &lt;- kBET(\n  data[subset_id,], \n  batch[subset_id]\n) #this may take a few minutes\n\n\n\n\n\n\n\nbatch.estimate$summary\n\n      kBET.expected kBET.observed kBET.signif\nmean     0.02403292     0.8678189           0\n2.5%     0.01574074     0.8291152           0\n50%      0.02331962     0.8683128           0\n97.5%    0.03364198     0.9012346           0\n\n\nWhile the UMAP looks much better, the kBET metric is telling us that there is still a batch effect. If we run kBET on the ‘aligned’ PCs, the rejection rate is still close to 1. Is it possible we introduced artifacts through our batch correction? Let’s try the Harmony package from the Raychaudhuri Lab at Harvard and see if anything is different.\n\n### Batch correction with Harmony\nset.seed(1000)\nharm_cds &lt;- RunHarmony(sasc, 'batch') #this may take a few minutes\n\nTransposing data matrix\n\n\nInitializing state using k-means centroids initialization\n\n\nHarmony 1/10\n\n\nHarmony 2/10\n\n\nHarmony 3/10\n\n\nHarmony 4/10\n\n\nHarmony converged after 4 iterations\n\nharm_cds #note the \"HARMONY\" in reducedDim\n\nclass: cell_data_set \ndim: 21286 24216 \nmetadata(2): cds_version citations\nassays(1): counts\nrownames(21286): ENSG00000238009 ENSG00000241860 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(18): barcode n.umi ... aligned_UMAP1 aligned_UMAP2\nreducedDimNames(3): PCA UMAP HARMONY\nmainExpName: NULL\naltExpNames(0):\n\nsasc\n\nclass: cell_data_set \ndim: 21286 24216 \nmetadata(2): cds_version citations\nassays(1): counts\nrownames(21286): ENSG00000238009 ENSG00000241860 ... ENSG00000278817\n  ENSG00000277196\nrowData names(6): id gene_short_name ... num_cells_expressed MT\ncolnames(24216): AGGATAATCTCGGCCTTACAGGTCAGCTT 22_60_95__s4 ...\n  24_87_10__s1 20_17_34__s4\ncolData names(18): barcode n.umi ... aligned_UMAP1 aligned_UMAP2\nreducedDimNames(2): PCA UMAP\nmainExpName: NULL\naltExpNames(0):\n\n\n\n# Under the hood, Monocle 3 is using the uwot package to generate UMAPs\nharmony_umap &lt;- umap(reducedDim(harm_cds, \"HARMONY\"), seed=1000) #this may take a minute\nsasc$harmony_UMAP1 &lt;- harmony_umap[,1] #save these to our original cds\nsasc$harmony_UMAP2 &lt;- harmony_umap[,2] #save these to our original cds\n\n\n#Plot\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = harmony_UMAP1, y = harmony_UMAP2, color = batch)) +\n  geom_point(size=0.5, alpha=0.5) +\n  theme_bw() + \n  scale_color_viridis(discrete=T, begin=0.1, end=0.9, option=\"A\") +\n  theme(legend.position=\"bottom\", aspect.ratio = 1, panel.grid=element_blank()) +\n  guides(color = guide_legend(override.aes = list(size = 8, alpha = 1)))\n\n\n\n\n\n\n\n\n\n#Use kBET to quantitatively ask\ndata &lt;- harmony_umap #note, we could alternatively run kBET at the level of the corrected PCs \nbatch &lt;- harm_cds$batch\nsubset_size &lt;- 0.1 #subsample to 10% of the data\nsubset_id &lt;- sample.int(\n  n = length(batch),\n  size = floor(subset_size * length(batch)), \n  replace = FALSE)\n\nset.seed(1000)\nbatch.estimate &lt;- kBET(data[subset_id,], batch[subset_id]) #this may take a few minutes\n\n\n\n\n\n\n\nbatch.estimate$summary\n\n      kBET.expected kBET.observed kBET.signif\nmean    0.003004115     0.8504115           0\n2.5%    0.000000000     0.8106996           0\n50%     0.002743484     0.8518519           0\n97.5%   0.008230453     0.8973251           0\n\n\nOnce again, our UMAP looks better, but our kBET metric suggests that it is still far from perfect.\nSo what benefit does batch correction offer? This is debatable, but certainly one thing it can do is help in identifying cell types. In our case, our toy dataset already had annotated cell types, but if we didn’t know these ahead of time, batch correction could help us identify them.\n\n#cluster cells that have been aligned and plot these on our original UMAP\nbc_cds &lt;- cluster_cells(\n  bc_cds, \n  k = 40, \n  cluster_method = \"leiden\", \n  random_seed = 1000\n) #this may take a few minutes\n\n\nsasc$aligned_clusters &lt;- clusters(bc_cds) #save to original cds object\nsasc$aligned_partitions &lt;- partitions(bc_cds) #save to original cds object\n\n\nggplot(\n  data.frame(pData(sasc)), \n  aes(x = UMAP1, y = UMAP2, color = k40_leiden_clusters)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  theme_bw() + \n  scale_color_manual(values = colpal) +\n  theme(legend.position = \"bottom\", aspect.ratio = 1, panel.grid = element_blank()) +\n  guides(color = guide_legend(override.aes = list(size = 8, alpha = 1)))\n\n\n\n\n\n\n\n\n\ndata.frame(colData(sasc)) %&gt;%\n  group_by(cell_type) %&gt;%\n  count(aligned_partitions) %&gt;%\n  spread(aligned_partitions, n)\n\n# A tibble: 12 × 6\n# Groups:   cell_type [12]\n   cell_type                   `1`   `2`   `3`   `4`   `5`\n   &lt;fct&gt;                     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 Adipocyte                     1    NA    NA    63    NA\n 2 Atrial Cardiomyocyte          5    NA    NA    NA    NA\n 3 Cytoplasmic Cardiomyocyte  1368    16     3     2     3\n 4 Endothelium                2309     9   238    NA    31\n 5 Fibroblast                 2666     6     4    NA     1\n 6 Lymphocyte                  192    NA    NA    NA    NA\n 7 Macrophage                 1752     4    11    NA    NA\n 8 Neuronal                    445     4     2    NA    NA\n 9 Pericyte                    280   880     2     1    NA\n10 Unknown                    2981    71    34    15    17\n11 Vascular Smooth Muscle      116    87     1    NA    NA\n12 Ventricular Cardiomyocyte 10556    15     1    23     1\n\n\nEven with our imperfect batch correction methods, our analysis has started to show us that some of our original clusters are related (ventricular cardiomyocytes).\n\n\n7. Differential gene expression\nWe are going to focus specifically on the ventricular cardiomyocytes for the rest of this code example. We start by subsetting our data down to ventricular cardiomyocytes that belong to clusters 2, 3, 4, and 9.\n\n#combine batch and donor as a new column \nsasc$id &lt;- paste(sasc$batch, sasc$donor, sep=\"_\")\n\n#subset ventricular cardiomyocyte data only \ncds_vent &lt;- sasc[,sasc$cell_type == \"Ventricular Cardiomyocyte\" &\n                   sasc$k40_leiden_clusters %in% c(\"2\", \"3\", \"4\", \"9\")]\n\nDifferential expression analysis can take a long time, so we will run the following code on a subset of pre-defined genes.\n\ngene_list &lt;- c(\"LINC00486\", \"TTN\", \"LINC-PINT\", \"TAS2R14\", \"MT-CO1\",\n               \"MT-ND4\", \"FN1\", \"LAMA2\", \"XIST\", \"PDK4\", \"ZBTB16\",\n               \"PPP1R3E\", \"TMTC1\", \"NT5DC3\", \"RBX1\", \"MRPL45\", \"ESR2\",\n               \"TUBGCP4\", \"MYH7\", \"MYL2\", \"MB\", \"ACTC1\", \"TPM1\", \"MYH6\")\n\ncds_subset &lt;- cds_vent[rowData(cds_vent)$gene_short_name %in% gene_list,]\n\nWe now plot expression levels of these genes, split by donor.\n\nplot_genes_violin(cds_subset, group_cells_by = \"donor\", ncol = 4) +\n  theme(axis.text.x=element_text(angle = 45, hjust = 1))\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\nWarning: Removed 108135 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 108135 rows containing non-finite outside the scale range\n(`stat_summary()`).\n\n\n\n\n\n\n\n\n\nIt is clear that some of our genes are differentially expressed across our two donors. How do we tell what contribution comes from donors and what comes from assay batch? Let’s build models for our data and compare.\nIn the donor model, we assume there are no batch effects and the only contributing variable is donor.\n\ndonor_model &lt;- fit_models(\n  cds_subset,\n  model_formula_str = \"~donor\",\n  expression_family=\"negbinomial\"\n)\n\n\ncoefficient_table(donor_model) %&gt;% \n  filter(term == \"donor1\") %&gt;%\n  filter(q_value &lt; 0.05) %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) %&gt;%\n  arrange(gene_short_name)\n\n# A tibble: 15 × 5\n   id              gene_short_name term     q_value estimate\n   &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 ENSG00000159251 ACTC1           donor1 5.46e-125   0.671 \n 2 ENSG00000115414 FN1             donor1 0           2.40  \n 3 ENSG00000196569 LAMA2           donor1 2.56e- 88  -0.245 \n 4 ENSG00000231721 LINC-PINT       donor1 1.67e-258  -1.91  \n 5 ENSG00000230876 LINC00486       donor1 4.42e-217   1.14  \n 6 ENSG00000198125 MB              donor1 1.38e- 55   0.463 \n 7 ENSG00000198804 MT-CO1          donor1 2.12e-214   1.23  \n 8 ENSG00000198886 MT-ND4          donor1 1.25e-170   1.13  \n 9 ENSG00000092054 MYH7            donor1 1.46e-107  -0.460 \n10 ENSG00000111245 MYL2            donor1 1.76e- 22   0.299 \n11 ENSG00000004799 PDK4            donor1 5.25e- 29  -0.165 \n12 ENSG00000212127 TAS2R14         donor1 6.18e-155  -3.80  \n13 ENSG00000155657 TTN             donor1 2.41e- 24  -0.0999\n14 ENSG00000229807 XIST            donor1 1.59e-167  -5.55  \n15 ENSG00000109906 ZBTB16          donor1 3.44e-178  -0.579 \n\n\nIn our second model, we include batch effects as a predictor variable.\n\n#controlling for batch effects\ndonor_batch_model &lt;- fit_models(\n  cds_subset,\n  model_formula_str = \"~donor + batch\",\n  expression_family=\"negbinomial\"\n)\n\n\ncoefficient_table(donor_batch_model) %&gt;% \n  filter(term == \"donor1\") %&gt;%\n  filter(q_value &lt; 0.05) %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) %&gt;%\n  arrange(gene_short_name)\n\n# A tibble: 15 × 5\n   id              gene_short_name term     q_value estimate\n   &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 ENSG00000159251 ACTC1           donor1 6.11e-  6   0.165 \n 2 ENSG00000115414 FN1             donor1 2.00e-200   2.37  \n 3 ENSG00000196569 LAMA2           donor1 3.44e- 72  -0.275 \n 4 ENSG00000231721 LINC-PINT       donor1 3.45e-  2   0.228 \n 5 ENSG00000198125 MB              donor1 4.23e-  2  -0.0950\n 6 ENSG00000198804 MT-CO1          donor1 4.17e- 43   0.605 \n 7 ENSG00000198886 MT-ND4          donor1 1.48e- 21   0.422 \n 8 ENSG00000197616 MYH6            donor1 6.76e- 23   0.303 \n 9 ENSG00000092054 MYH7            donor1 7.85e- 16  -0.215 \n10 ENSG00000111245 MYL2            donor1 1.12e- 13   0.299 \n11 ENSG00000004799 PDK4            donor1 5.56e-243  -0.534 \n12 ENSG00000212127 TAS2R14         donor1 9.17e-  3  -0.527 \n13 ENSG00000155657 TTN             donor1 2.00e- 36   0.142 \n14 ENSG00000229807 XIST            donor1 1.15e-167  -5.56  \n15 ENSG00000109906 ZBTB16          donor1 3.01e-  3  -0.0961\n\n\nComparing these two models, we can see that some of our genes are significantly influenced by the introduction of batch as a predictor.\n\n# Comparing models of gene expression\ncompare_models(donor_batch_model, donor_model) %&gt;% \n  select(gene_short_name, q_value) %&gt;% \n  data.frame()\n\n   gene_short_name       q_value\n1        LINC00486  0.000000e+00\n2              TTN 4.329415e-286\n3              FN1  1.000000e+00\n4            LAMA2  1.020045e-02\n5             PDK4  0.000000e+00\n6        LINC-PINT  0.000000e+00\n7           ZBTB16 6.655693e-187\n8          TAS2R14  0.000000e+00\n9            TMTC1  2.872632e-05\n10          NT5DC3  5.740679e-02\n11            MYL2  1.000000e+00\n12         PPP1R3E  4.636225e-01\n13            MYH6  1.999277e-62\n14            MYH7  5.726891e-61\n15            ESR2  2.311010e-02\n16           ACTC1 4.529192e-170\n17         TUBGCP4  5.352325e-01\n18            TPM1  9.684985e-01\n19          MRPL45  1.000000e+00\n20              MB 4.086623e-205\n21            RBX1  1.000000e+00\n22            XIST  1.000000e+00\n23          MT-CO1 7.757731e-279\n24          MT-ND4  0.000000e+00\n\n\nLet’s take a closer look at two example genes: LINC00486 and TTN, both of which have highly significant q-values (close to 0).\n\n# Get normalized counts\ncntmtx &lt;- normalized_counts(cds_subset)\ncds_subset$LINC00486 &lt;- cntmtx[\"ENSG00000230876\",]\ncds_subset$TTN &lt;- cntmtx[\"ENSG00000155657\",]\n\n\n#the case of LINC00486\na &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = donor, y = LINC00486)) + \n  geom_violin(aes(fill = donor)) +\n  geom_boxplot(width = 0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nb &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = batch, y = LINC00486)) + \n  geom_violin(aes(fill = \"salmon\")) +\n  geom_boxplot(width=0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nc &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = id, y = LINC00486, fill = donor)) + \n  geom_violin() +\n  geom_boxplot(width=0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nplot_grid(a + theme(legend.position = \"none\"),\n          b + theme(legend.position = \"none\") + ylab(\"\"),\n          c + theme(legend.position = \"none\") + ylab(\"\"), \n          labels=c(\"A\", \"B\", \"C\"),\n          nrow = 1)\n\n\n\n\n\n\n\n\nFrom our model, we can see that the batch variable was the primary contributor to differences in LINC00486 expression across our data.\n\ncoefficient_table(donor_model) %&gt;% \n  filter(gene_short_name == \"LINC00486\" & term ==\"donor1\") %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) \n\n# A tibble: 1 × 5\n  id              gene_short_name term     q_value estimate\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 ENSG00000230876 LINC00486       donor1 4.42e-217     1.14\n\ncoefficient_table(donor_batch_model) %&gt;% \n  filter(gene_short_name == \"LINC00486\" & term %in% c(\"donor1\", \"batchScale\")) %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) \n\n# A tibble: 2 × 5\n  id              gene_short_name term       q_value estimate\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 ENSG00000230876 LINC00486       donor1           1   0.0133\n2 ENSG00000230876 LINC00486       batchScale       0   3.64  \n\n\n\n#the case of TNN\nx &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = donor, y = TTN)) + \n  geom_violin(aes(fill = donor)) +\n  geom_boxplot(width = 0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\ny &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = batch, y = TTN)) + \n  geom_violin(aes(fill = \"salmon\")) +\n  geom_boxplot(width = 0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nz &lt;- ggplot(\n  data.frame(pData(cds_subset)), \n  aes(x = id, y = TTN, fill = donor)) + \n  geom_violin() +\n  geom_boxplot(width = 0.2, fill = \"white\", alpha = 0.3, outlier.shape = NA) +\n  theme_bw() \n\nplot_grid(\n  x + theme(legend.position=\"none\"),\n  y + theme(legend.position=\"none\") + ylab(\"\"),\n  z + theme(legend.position=\"none\") + ylab(\"\"), \n  labels=c(\"A\", \"B\", \"C\"),\n  nrow = 1)\n\n\n\n\n\n\n\n\nOn the other hand, in the case of TTN, we see that both donor and batch affected its expression.\n\ncoefficient_table(donor_model) %&gt;% \n  filter(gene_short_name == \"TTN\" & term ==\"donor1\") %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) \n\n# A tibble: 1 × 5\n  id              gene_short_name term    q_value estimate\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 ENSG00000155657 TTN             donor1 2.41e-24  -0.0999\n\ncoefficient_table(donor_batch_model) %&gt;% \n  filter(gene_short_name == \"TTN\" & term %in% c(\"donor1\", \"batchScale\")) %&gt;%\n  select(id, gene_short_name, term, q_value, estimate) \n\n# A tibble: 2 × 5\n  id              gene_short_name term        q_value estimate\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 ENSG00000155657 TTN             donor1     2.00e-36    0.142\n2 ENSG00000155657 TTN             batchScale 0          -0.405\n\n\n\n\n8. Key takeaways and suggestions\n\nDo not blindly apply batch correction! You risk introducing more artifacts than you remove.\nIn general, it is better to use batch correction methods for data exploration and visualization rather than analysis.\nIf you have a batch effect, model it in your downstream analysis (i.e include it as a covariate) instead of modifying your data directly.\n\n\n\n9. Conclusions\nThis concludes the content that was covered in the SASC User Group workshop on batch correction. I’d like to give a huge thank you again to Mary and the team at the BBI for organizing this tutorial and sharing it freely with the public! I look forward to covering more from these meetings in the future, as well as sharing my own thoughts and exploration of single cell data down the road."
  },
  {
    "objectID": "posts/terminologies/index.html",
    "href": "posts/terminologies/index.html",
    "title": "Biomedical Data Science sub-disciplines (v0)",
    "section": "",
    "text": "Welcome back to another week of [VS]Codes! In today’s post, I will be summarizing my personal definitions and interpretations of the various subdisciplines in biomedical data science. Given my prior background and experience in this subject area, I figured that writing this post would be an easy task! However, referencing the literature further made me realize that the boundaries across the subfields of health technology are fairly nebulous. Different departments, organizations, and individuals all have their own opinions of what their work involves. Dr. Bill Hersh from Oregon Health and Science University has a great paper (all the way back from 2009!) on the need to converge on consistent terminology in the field of biomedical informatics: you can give it a read here.\nGiven the lack of clarity in the subfields of biomedical data science, my goal today will be to provide my personal interpretation of the breakdown of relevant fields, informed by some of the definitions I’ve collated from external references. This description is absolutely meant to be a first draft of sorts (hence the v0 in the title)… I aim to refine these terms further based on additional input down the road."
  },
  {
    "objectID": "posts/terminologies/index.html#information-and-data",
    "href": "posts/terminologies/index.html#information-and-data",
    "title": "Biomedical Data Science sub-disciplines (v0)",
    "section": "Information and Data",
    "text": "Information and Data\nInformation Technology (IT)\nInformation technology (IT) refers to the use of computer systems to manage, process, protect, and exchange information. The overarching goal among its specializations is to use technology systems to solve problems and handle information. [5]\nInformation Science\nInformation science is the theoretical study of how information is created, organized, managed, stored, retrieved, and used. It is an interdisciplinary field that combines aspects of computer science and information management. [6]\nInformatics\nInformatics refers to the use and implementation of technology systems to analyze and manage information. Its primary focus is its application to specific external domains “for the good of people, organizations, and society”. [7, 8]\nData Science\nData science refers the study of data to extract meaningful insights and acquire knowledge. It is an interdisciplinary field that combines principles from a broad range of fields, including mathematics, statistics, artificial intelligence, and computer engineering, to collect, process, and analyze large amounts of data. There is a stronger focus on predictive modeling and algorithmic design. [9, 10, 11]\nData Analytics\nData analysis involves the analysis and examination of large amounts of data to better understand trends in the system being studied. There is a stronger focus on developing tables, visualizations, and overarching products to comprehend the data. [12]\nBased on the above definitions, I visualize the interplay of information and data as follows:\n\nInformation technology serves as the overarching guidebook for computational work. Within IT, information science dictates the way that we collect, store, organize, and manage our data. And finally, under both information technology and information science, data science, data analytics, and informatics intersect with one another, allowing for the development of predictive models and the analysis of data for applications to external domains."
  },
  {
    "objectID": "posts/terminologies/index.html#biomedical-informatics-and-computational-biology",
    "href": "posts/terminologies/index.html#biomedical-informatics-and-computational-biology",
    "title": "Biomedical Data Science sub-disciplines (v0)",
    "section": "Biomedical Informatics and Computational Biology",
    "text": "Biomedical Informatics and Computational Biology\nBiomedicine\nBiomedicine and the biomedical sciences refer to a set of sciences that apply understandings of biology and the natural sciences to develop knowledge, interventions, or technology that are of use in healthcare or public health. [13]\n\nBiomedical Informatics\nBiomedical Informatics\nBiomedical informatics is an interdisciplinary field seeking to study and advance the use of biomedical data to improve individual health, public health, and healthcare. It investigates, simulates, experiments with and translates a wide swath of biological systems to connect basic and clinical research with practical application for the overall betterment of healthcare. [17, 18, 19]\nBioinformatics\nBioinformatics, the application of biomedical informatics in cellular and molecular biology (often with a focus on genomics), is a scientific subdiscipline that involves using computer technology to collect, store, analyze, and disseminate biological data and information, such as DNA and amino acid sequences or annotations about those sequences, to increase our understanding of health and disease. [19, 20]\nTranslational Informatics\nTranslational informatics, the application of biomedical informatics to human health, is focused on the study and application of existing biomedical data to bridge new ways to improve diagnosis, staging, prognosis, and treatment of human disease. [19, 21]\nClinical Informatics\nClinical informatics, the subdiscipline of biomedical informatics related to patient data (typically from electronic medical records), focuses on the application of informatics to specific clinical subdisciplines, such as healthcare, nursing, dentistry, and pathology. [19, 22]\nThe subfields of biomedical informatics can be represented in the following hierarchy:\n\nBioinformatics is focused on processing the core data of biological systems, translational informatics is focused on the translation from biology to medicine, and clinical informatics is focused on the analysis of medical and healthcare data.\n\n\nComputational Biology\nComputational Biology\nComputational biology refers to the use of mathematics, statistics, and algorithms to understand biological systems based on data from experimental measurements. Examples of biological questions that may be tackled include what:\n\nbiological tasks are carried out by particular nucleic acid or peptide sequences\nwhich gene (or genes) when expressed produce a particular phenotype or behavior\nwhat sequence of changes in gene or protein expression or localization lead to a particular disease\nhow do changes in cell organization influence cell behavior. [23]\n\nComputational Genomics\nGenomics is a subfield of biomedicine focused on studying the entire set of DNA of an organism. Genomics research involves identifying and characterizing all the genes and functional elements in an organism’s genome as well as how they interact. [14, 15] Computational genomics refers to the use of computational and statistical analysis to decipher biology from genome sequences and related data, including both DNA and RNA sequence as well as other “post-genomic” data. [24]\nThe following figure shows some of the subfields of biology that can be addressed through computation. Computational genomics could be utilized in all three cases.\n\n\n\nDifferentiating biomedical informatics and computational biology\nBiomedical informatics and computational biology are very similar terms to one another - both involve the interdisciplinary application of information technology to biomedicine. The key differentiators I see between the two terms are the order of prioritization in disciplines and the nature of the data under consideration.\nIn biomedical informatics, the focus is on the development computational infrastructure and analysis to handle large-scale biomedical data.\nOn the other hand, computational biology starts with the focus on a biological experiment. From a biological question, corresponding data are generated, and computational analyses are applied.\nIndeed, I see the second word in each phrase as the “order of operations”: biomedical informatics is centered on the informatics, and computational biology is centered on the biology."
  },
  {
    "objectID": "posts/terminologies/index.html#health-technology-and-biotechnology",
    "href": "posts/terminologies/index.html#health-technology-and-biotechnology",
    "title": "Biomedical Data Science sub-disciplines (v0)",
    "section": "Health Technology and Biotechnology",
    "text": "Health Technology and Biotechnology\nHealth Technology\nHealth technology, or “health tech,” refers to the use of technologies developed for the purpose of improving any and all aspects of the healthcare system. It is focused primarily on the development of healthcare products and services. [25]\nHealth Information Technology\nHealth information technology (health IT) involves the processing, storage, and exchange of health information in an electronic environment. Applications include enhancing the quality of healthcare, preventing medical errors, reducing healthcare costs, and expanding access to healthcare. [26]\nBiotechnology\nBiotechnology (biotech) involves the use of living organisms and/or biological systems to develop or create different products [27].\nBioengineering\nBioengineering involves the application of engineering principles in combination with living organisms and/or biological systems to develop or create different products. These solutions may take the form of devices or computer programs (e.g., simulation of biomedical processes). However, the focus is on the biomedical problem to be solved, not data, information or knowledge. [28]\nDigital Health\nDigital health refers to the use of information and communications technologies in medicine and other health professions to manage illnesses and health risks and to promote wellness. [29]\nI see health technology and biotechnology as synonymous fields to one another, with health tech focused more on advancing human healthcare and biotech focused more on the use of biological systems. The incorporation of IT into health technology yields digital health and health IT, while the incorporation of engineering into biotechnology yields bioengineering."
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello world!",
    "section": "",
    "text": "Hello world, and welcome to my official blog! I’ve had numerous thoughts swirling around in my head over the past several years, but have had trouble formally committing to bringing them out into the world… however, encouragement from colleagues and recent reading (i.e. Dorie Clark’s The Long Game) have convinced me to finally bring my virtual pen to paper. In particular, I found myself tremendously inspired by the Fred Hutch DaSL Culture and Work Style document and the mantra of “ship as soon as you can.” The following graphic does a great job at reminding us how to avoid the pit of perfectionism!\n\nMy hopes for this blog are to (a) build a community, (b) share my knowledge, and (c) learn alongside my readers. I aim to write weekly (if not more frequent) posts on a variety of subjects, including:\n\nmy personal story\nadvice to others in my field\ntechnical tutorials\nsubject matter overviews, and\nmiscellaneous detours\n\nI have no expectations that my content will be perfect, but then again, learning from mistakes is one of the best parts of life :)\nGiven my background and expertise, my general focus will be on the world of biomedical data science and health technology, but the sky is the limit for the topics we may cover!\nThanks for tuning in - I’m delighted to have you along for the ride…"
  },
  {
    "objectID": "posts/professional-journey/index.html",
    "href": "posts/professional-journey/index.html",
    "title": "My professional journey",
    "section": "",
    "text": "For my first “real” post, I figured it would be a helpful exercise to go through my professional journey and track how I ended up where I currently am. This description is meant to be more of an overview, and I intend to provide more details on individual portions of it in the future.\n\nHaving grown up in the Silicon Valley, I always had a front-row seat to the power and potential of technology to improve people’s lives. All of the biggest tech companies had their headquarters within driving distance from my home, and every day, I could see how they had impacted not only myself but also everyone around me: computers, cellphones, social media, education, automobiles, entertainment… everything was shaped by information technology. Both of my parents were in the software industry too, and seeing the productive, fulfilling jobs that they were able to have made me certain that I wanted to involve technology in my future career. Learning how to code from my mother in high school made me feel like I was being imparted with some special kind of magic - entering the world of software engineering truly felt right at my fingertips.\nAt the same time, growing up in the Silicon Valley felt like growing up in a bubble. I yearned to explore the link between information technology and its downstream applications beyond my baseline understanding of how to write code. So, I sought out more. In high school, I took a breadth of science classes, and I found myself inspired by the concept of “interdisciplinarity”. Instead of being drawn to “information technology,” I was drawn closer and closer to the world of “information science” and its applications to multiple disciplines, including biology and medicine. My drive for interdisciplinary experiences and my desire to explore a world of technological applications outside of the Bay Area led me to Duke University for my undergraduate education - in fact, one of the mantras of the university was “creative thinking across intellectual boundaries”. At Duke, I completed double majors in Computer Science and Statistics. Duke’s affiliated medical campus also gave me chances to explore interdisciplinary applications in the world of biomedical informatics, and I pursued multiple research opportunities, including an Honor’s thesis for my Statistics degree under the supervision of Dr. Li Ma. I also completed a minor in Computational Biology, taking classes such as Computational Genomics with Dr. Alexander Hartemink and Computational Structural Biology with Dr. Bruce Donald. Lastly, I was lucky to have great summer internship mentors (including Li-Yuan Chern at Pharmacyclics and Drs. Zichen Wang and Avi Ma’ayan at the Icahn School of Medicine), who kept me motivated and inspired to stick to my path and pursue an advanced interdisciplinary career.\nBy the end of my undergrad, I knew that I wasn’t going to enter the traditional computer science recruitment cycle for software engineering roles - I instead applied to doctoral programs in biomedical informatics and computational biology that would allow me to build my knowledge base further and prepare me to become a leader in impactful projects that made a clear benefit in people’s lives. I was lucky to earn an admission with the Genomics and Computational Biology program at the University of Pennsylvania Perelman School of Medicine. Again, great mentors from my classes and research rotations (including Drs. Ryan Urbanowicz and Marylyn Ritchie to name a few) helped advance my training and made me a better researcher and scientist day by day. I am most indebted to my PhD advisor, Dr. Dokyoon Kim, for his support and mentorship throughout my PhD and subsequent post-doctoral position. With his leadership style and work ethic, he was a true role model for me throughout my graduate degree. I also fell in love with the combination of technical research and scientific storytelling that came out of my dissertation (to be discussed in a later post). During my PhD, I was able to complete a Master’s degree in Statistics and Data Science from the Wharton School of Business under the supervision of Dr. Anderson Zhang, as well as a summer internship as a User Experience Researcher in Health AI/ML under the supervision of Dr. Mandi Hall with the Health Futures team at Microsoft Research. All of these opportunities helped me to refine a set of motivators for my long-term career:\n\nimpact\nconnection\npassion\nleadership\n\nThese values aided me tremendously in my search for my first job upon the completion of my PhD. Today, I work as a clinical data scientist in the Translational Analytics and Informatics group at the Fred Hutchinson Cancer Center’s Data Science Lab (DaSL) in Seattle, WA (also to be discussed more in a future post). I am tremendously grateful to be a part of a supportive, driven community of fellow data scientists and researchers as we develop the clinical data infrastructure at Fred Hutch, and I look forward to sharing more about my work and career as the years progress!"
  },
  {
    "objectID": "posts/phd-context/index.html",
    "href": "posts/phd-context/index.html",
    "title": "My doctoral research: the background",
    "section": "",
    "text": "In the next few posts, I will be providing an overview of my PhD in Biomedical Informatics and Computational Genomics that I completed under the mentorship of Dr. Dokyoon Kim at the University of Pennsylvania Perelman School of Medicine. You can listen to a full presentation of my thesis defense here, and you can read the full text of my dissertation here. Note that all figures featured in this blog post were created using BioRender.com. Today’s post will focus on the background context that motivated my research. Without further ado, let’s get started!\n\n\nHuman biology is a complicated field. Millions of cells, cellular components, molecules, and chemicals interact with one another moment by moment to keep us alive each and every day. And while biologists and clinicians have defined a variety of classifications and ontologies to organize and understand these systems, the intricacies of the field continue to require extensive research and investment. Achieving a better understanding of human biology necessitates a unified language.\nAt the core of human biology and its defined ontologies lies the field of genetics. Genetics refers to the study of heritable traits (also known as phenotypes). If we define genetics as the language of biology, then the vocabulary of genetics would be the gene - a unit of heritable information that influences how we develop and operate. Extending the analogy further, the letters of genetics would be the nucleotide, a set of four molecules whose arrangements define different genes. Indeed, differences in individual nucleotides, also known as single-nucleotide variants/polymorphisms or “SNVs/SNPs”, are responsible for genetic variation across individuals and can lead to differing phenotypic outcomes. Genetic variation is a significant component of the diversity of life.\nGiven the complex interactions that occur across biomolecules in our bodies, it becomes apparent that networks of interacting genes drive our ability to live. Human diseases can be thought of as disruptions to these networks. The field of medicine aims to prevent, alleviate, and cure disease through the maintenance of health and the development of novel therapeutics.\nUnfortunately, for the most part, medicine today still operates from a “good enough” perspective - many patients are treated with the same medications without regard for or understanding of differences in their individual backgrounds or health profiles. Indeed, much more can be done to enhance the accuracy and efficacy of treatment.\nThe field of precision medicine uses large-scale multimodal/multiomic data to individualize patient care and gain a comprehensive understanding of human health. The goal of precision medicine is to achieve more accurate and precise disease prediction, prevention, treatment, and therapeutics. The field of genomics, involving the study of genetics from a “big data” lens, offers a significant opportunity to advance precision medicine research.\nGenomics refers to the study of an individual’s entire set of genes (a.k.a. their genome). We can work with genomic data from large-scale biomedical data, including both electronic health records (EHRs) and patient biobanks. EHRs, also known as electronic medical records (EMRs), refer to large clinical databases of patient medical history and clinical data. Biobanks, on the other hand, refer to biomedical databases with large quantities of patient biological samples, often including access to their genetic information. Combining these two data sources into a merged EHR-linked biobank provides an extra level of power in the study of genomics and medicine. EHR-linked biobanks offer the ability to identify and evaluate statistically significant genetic contributors to human disease. For instance, a genome-wide or phenome-wide association study (GWAS/PheWAS) applied to an EHR-linked biobank can identify associations between a variety of diseases and SNPs.\nMany research efforts in the field of precision medicine have used the results of PheWASs to identify genetic contributors to diseases. With such discoveries, patient genetic profiles can be built into diagnosis/treatment pipelines, allowing for the personalization of patient care.\nNotably, so far, most precision medicine research efforts that have made their way into the clinic have focused on one disease at a time. However, complex diseases rarely impact patients one-at-a-time. Shared SNPs and genes can contribute to the onset of multiple diseases in a single patient over time. These disease “multimorbidities” can lead to increase healthcare costs, health burdens, and risk of death. Thus, it becomes clear that we must evaluate the genetics of not only individual diseases but also cross-phenotype associations if we wish to gain a deeper understanding of overall patient health.\nGiven the significance of cross-phenotype associations, the field of “network medicine” offers a helpful framework to investigate the associations between diseases. Thus, the objective of my dissertation was to apply a “network medicine” approach to investigate genetic contributors to disease multimorbidities:\n\n\n\nFig 1. An overview of the process of using PheWAS results from an EHR-linked biobank for network medicine\n\n\nI broke this objective down into three chapters:\n\nCreation: construct and analyze a network of diseases derived from an EHR-linked biobank for the evaluation of genetic similarity between phenotypes\nComparison. generate and compare different disease networks generated from different populations and from genetic components.\nTranslation. extend the conclusions drawn from disease network analysis and comparison to downstream precision medicine applications.\n\n\n\n\nFig 2. The three sub-chapters of my PhD dissertation\n\n\nIn the coming week(s), I will go in-depth into the published manuscripts and preprints that correspond to these chapters, as well as my overall takeaways from my PhD research! Till next time~"
  },
  {
    "objectID": "posts/phd-papers/index.html",
    "href": "posts/phd-papers/index.html",
    "title": "My doctoral research: the content",
    "section": "",
    "text": "In last week’s post, I provided an overview of the context for my PhD research in Biomedical Informatics and Computational Genomics that I completed under the mentorship of Dr. Dokyoon Kim at the University of Pennsylvania Perelman School of Medicine. Today’s post will focus on some of the actual content that came out of my research. You can listen to a full presentation of my thesis defense here, and you can read the full text of my dissertation here. Note that all figures featured in this blog post were created using BioRender.com.\n\n\nAs discussed last week, the objective of my dissertation was to apply a network medicine approach to investigate genetic contributors to disease multimorbidities.\n\n\n\nFig 1. An overview of the process of using PheWAS results from an EHR-linked biobank for network medicine\n\n\nI broke this objective down into three chapters:\n\nCreation: construct and analyze a network of diseases derived from an EHR-linked biobank for the evaluation of genetic similarity between phenotypes\nComparison. generate and compare different disease networks generated from different populations and from genetic components.\nTranslation. extend the conclusions drawn from disease network analysis and comparison to downstream precision medicine applications.\n\n\n\n\nFig 2. The three sub-chapters of my PhD dissertation\n\n\nIn today’s post, I will provide an example manuscript from each of these chapters to provide more insight into some of the work that I did.\nChapter 1. Creation\nExample manuscript - NETMAGE: A human disease phenotype map generator for the network-based visualization of phenome-wide association study results\n\nDisease-disease networks (DDNs), graphs where nodes represent diseases and edges represent associations between diseases, can provide an intuitive way of understanding the relationships between diseases.\nUsing summary statistics from a phenome-wide association study (PheWAS), we can generate a corresponding DDN where edges represent shared genetic variants (e.g. SNPs) between diseases.\nSuch a network can help us analyze genetic associations across the “diseasome,” the landscape of all human diseases, and identify potential genetic influences for disease multimorbidities.\nTo improve the ease of network-based analysis of shared genetic components across diseases, we developed the humaN disEase phenoType MAp GEnerator (NETMAGE), a web-based tool that produces interactive DDN visualizations from PheWAS summary statistics. You can try out the tool we developed at the following link: https://hdpm.biomedinfolab.com/netmage/\n\nUsers can search their generated maps by various attributes and select nodes to view related diseases, associated variants, and various network statistics.\n\nAs a test case, we used NETMAGE to construct an example network from UK BioBank (UKBB) PheWAS summary statistic data. You can explore this network at the following link: https://hdpm.biomedinfolab.com/ddn/ukbb\n\nOur map correctly displayed previously identified disease comorbidities from the UKBB and identified concentrations of hub diseases in the endocrine/metabolic and circulatory disease categories.\n\nBy examining the associations between diseases in our map, we can identify potential genetic explanations for the relationships between diseases and better understand the underlying architecture of the human diseasome.\nYou can read the published manuscript for this project here.\n\nChapter 2. Comparison\nExample manuscript - The interplay of sex and genotype in disease associations: a comprehensive network analysis in the UK Biobank\n\nGiven that many individual diseases exhibit sex-specific differences in their genetic influences (also known as “genotype-by-sex” or “GxS” effects), we aimed to determine whether disease multimorbidities are also influenced by GxS interactions.\nThrough the comparison of sex-stratified DDNs, we investigated differences across the sexes in patterns of shared genetic architecture between diseases.\nUsing sex-stratified phenome-wide association study summary data from the UK Biobank, we built male- and female-specific DDNs for 103 different diseases.\nWe compared our networks using the network comparison methods highlighted in figure 3:\n\n\n\n\nFig 3. Overview of network comparison methods for comparing sex-stratified DDNs\n\n\n\nComparing the two graphs reveals that the diseasomes of males and females are similar to one another in terms of network topology.\n\n\n\n\nTable 1. Network statistics for our two sex-stratified DDNs\n\n\n\nSome diseases, however, seem to exhibit sex-specific influence in cross-phenotype associations. For instance, autoimmune and inflammatory disorders including multiple sclerosis and osteoarthritis are centrally involved only in the female-specific DDN, while cardiometabolic diseases and skin cancer are more prominent only in the male-specific DDN.\n\n\n\n\nTable 2. Most central diseases in our sex-stratified DDNs, based on centrality measures including degree, weighted degree, and betweenness centrality.\n\n\n\nNotably, discrepancies in embedding distances and clustering patterns across the networks imply a more expansive genetic influence on multimorbidity risk for females than males.\n\n\n\n\nFig 4. Heatmaps of edge sets across disease categories for our two sex-stratified DDNs. Brighter colors indicate more edges shared between disease categories.\n\n\n\nIn summary, our analysis affirms the presence of GxS interactions in cross-phenotype associations, emphasizing the continued need for investigation of the role of sex in disease onset and its importance in biomedical discovery and precision medicine research.\nThis manuscript is currently under review for publication.\n\nChapter 3. Translation\nExample chapter - An enhanced disease network with robust cross-phenotype relationships via variant frequency-inverse phenotype frequency.\n\nDDNs constructed from PheWAS data offer a unique ability to observe and evaluate associations between diseases from large-scale biomedical data.\nThese publications all follow a similar approach when constructing a DDN:\n\n(a) a single statistical significance level (p-value) is selected to determine associations between diseases and SNPs.\n(b) the links between diseases and SNPs are compressed into links between diseases to generate the DDN\n(c) a similarity metric such as cosine similarity is used to determine how similar two diseases are based on the number of shared associated SNPs\n\nThis process for constructing DDN seems straightforward, but it has the following limitations (see Figure 5):\n\n(a) the entire structure of the DDN can vary depending on the selection of significance level threshold in the PheWAS-driven complex relationship.\n(b) the effect of individual SNPs on the interactions across more than 2 diseases is masked\n(c) the chosen similarity metric can mask the exact amount of similarity between diseases\n\n\n\n\n\nFig 5. Overview of current approaches for constructing a DDN and their limitations.\n\n\n\nTo address the discussed limitations of previous approaches to developing DDNs, we proposed a new method inspired by natural language processing to generate networks from PheWAS data\nTaking inspiration from the NLP method “term frequency - inverse document frequency” (TF-IDF), we propose a new method we call “variant frequency - inverse phenotype frequency” (VF-IPF), which will weight the contributions of SNPs to disease associations. The outcome of this method presents itself as follows:\n\nIf a SNP is significant for only a few diseases, it is upweighted.\nIf a SNP is significant for many diseases, it is downweighted (similar to searching for the word “the” in a manuscript)\nIf a SNP is not significant for diseases, it is downweighted.\n\n\n\n\n\nFig 6. An overview of the VF-IPF algorithm\n\n\n\nTo test how the proposed method affects the way we represent cross-phenotype associations, we constructed an enhanced disease-disease network (eDDN) using UK biobank PheWAS summary statistics and tested the eDDN with three downstream tasks (see Figure 7), including:\n\nco-occurrence disease prediction when index disease of interest is given,\nnovel disease connection prediction, and\ntherapeutic drug prediction based on disease similarity.\n\n\n\n\n\n\nFig 7. Downstream tasks for the eDDN\n\n\n\nComparing our eDDN’s effectivness at predicting known disease comorbidities compared to other DDNs, we see that our eDDN has the highest AUC (i.e. it has the best performance).\n\n\n\n\nFig 8. The eDDN can predict disease comorbidities better than standard DDNs\n\n\n\nFurthermore, we see the utility of the eDDN in evaluating potential options for drug repurposing in the treatment of rheumatoid arthritis.\n\n\n\n\nFig 9. The eDDN can help with drug repurposing applications, suggesting alternative pre-existing treatments for rheumatoid arthritis.\n\n\n\nIn summary, we find that our proposed eDDN more effectively captures genetic associations between diseases from PheWAS data compared to previous approaches.\nThis manuscript is currently under review for publication.\n\n\nToday’s post was meant to give a sample of some of my work during my PhD. To read more about my currently published manuscripts, you can refer to my Google Scholar profile here.\nIn next week’s post, I will conclude this series on my PhD work with my personal takeaways from my program as well as tips for current, incoming, and aspiring PhD students, including selecting a program, selecting a thesis advisor, picking projects, and more! Until then~"
  },
  {
    "objectID": "posts/phd-takeaways/index.html",
    "href": "posts/phd-takeaways/index.html",
    "title": "My doctoral research: takeaways and advice",
    "section": "",
    "text": "In the last few posts, I have provided an overview of my PhD in Biomedical Informatics and Computational Genomics that I completed under the mentorship of Dr. Dokyoon Kim at the University of Pennsylvania Perelman School of Medicine. You can listen to a full presentation of my thesis defense here, and you can read the full text of my dissertation here. In today’s post, I will conclude this series on my doctoral research with my personal takeaways and tips for picking, pursuing, and finishing a PhD program."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#why-did-i-choose-to-pursue-a-phd-after-my-undergrad",
    "href": "posts/phd-takeaways/index.html#why-did-i-choose-to-pursue-a-phd-after-my-undergrad",
    "title": "My doctoral research: takeaways and advice",
    "section": "Why did I choose to pursue a PhD after my undergrad?",
    "text": "Why did I choose to pursue a PhD after my undergrad?\nAs I explained in my previous post describing my professional journey, I’ve always wanted to be a leader in impactful projects that made a clear benefit in people’s lives. When I was an undergrad, I began to research job opportunities that I found aligned with my interests. Looking at the qualifications required for these roles, and based on further advice from colleagues and mentors in my summer internships, I realized that in order for me to be able to become a leader in an interdisicplinary field in the future, I would have to pursue education beyond my undergraduate degree. Without a Master’s or PhD, I knew that I would eventually hit a wall in my career progression.\nI knew earning a graduate degree in the future after working for some time was a completely valid option. But, I also felt that knowing my own personality, it would be harder to bring myself back to school after a few years - I would feel the pay differential more keenly, and I would have to retrain myself to become a student. So, I decided to apply for graduate programs during my senior year of college.\nI considered both Master’s and PhD programs, but in weighing the opportunity costs for the computational fields I was considering, I ended up focusing my attention on doctorate degrees. I knew that a PhD would help me transition more clearly into cutting-edge research-based career opportunities. I also ideally wanted to avoid having to pay for a Master’s degree. A PhD program, on the other hand, would support me with a stipend for the duration of the program. Finally, I knew that with a computationally-focused PhD, it would be very reasonable to aim to graduate in about five years, which would be shorter than the average timeline for a PhD in the United States.\nI applied only for PhD programs where I knew I would feel content about spending 5+ years of my time. I also applied to one Master’s program in case I didn’t get in to any PhD programs. If nothing worked out for me, my back-up plan would have been to try to find a 1-year Master’s program attached to my undergrad or look for a short-term research position so that I could gain more experience and reapply fully for Master’s programs in the following year. Luckily, PhD admissions worked out, and I found a program that fit with everything I was looking for!"
  },
  {
    "objectID": "posts/phd-takeaways/index.html#why-did-i-apply-for-bioinformatics-programs",
    "href": "posts/phd-takeaways/index.html#why-did-i-apply-for-bioinformatics-programs",
    "title": "My doctoral research: takeaways and advice",
    "section": "Why did I apply for bioinformatics programs?",
    "text": "Why did I apply for bioinformatics programs?\nI chose to apply for bioinformatics / computational biology programs based on both my interest in the field as well as my likelihood of getting into such a department. I had always been motivated by the concepts of interdisclipinary research involving informational technology. So, I knew that I wanted to pursue a graduate degree related to data science, computer science, or statistics. At the same time, focusing on the area of biomedical research felt like an untapped market to me - there was so much data to work with and so much opportunity to advance the field. My time at Duke also gave me extensive exposure to research in the biomedical informatics domain. As a result, I felt that I would be a competitive applicant for cutting-edge programs in biomedical informatics and computational biology compared to other disciplines. I also felt that even if I wanted to pivot to a career that didn’t involve biomedical applications in the future, having a computational PhD would be sufficient to qualify me for such roles."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#why-did-i-pick-penn-genomics-and-computational-biology",
    "href": "posts/phd-takeaways/index.html#why-did-i-pick-penn-genomics-and-computational-biology",
    "title": "My doctoral research: takeaways and advice",
    "section": "Why did I pick Penn Genomics and Computational Biology?",
    "text": "Why did I pick Penn Genomics and Computational Biology?\nComing from a more computational background, I wanted a program that could help me catch up in topics in which I was lacking while also advancing further in my training for subjects where I already had the expertise. Penn GCB offered a very customized approach for selecting coursework, allowing me to take more foundational classes in genetics and molecular biology while pursuing more advanced curriculum in statistics and computer science.\nPenn also had multiple professors under whom I could see myself working, as well as access to interesting medical data due to its association with the University of Pennsylvania Health System.\nI also considered the happiness of students currently in the program and the livability of Philadelphia. It was apparent to me after my interview weekend that students were able to have fulfilling lives outside of their research, and that Philadelphia would be an exciting (and affordable) city for me to spend my twenties!\nLastly, I appreciated that Penn offered the opportunity to pursue a free Master’s degree in Statistics and Data Science from the Wharton School of Business concurrently with my PhD. Given my undergraduate degree in Statistics, I felt that this would be something I could more easily pursue, and that it would also give me a leg up in terms of foundational knowledge and branding in the future if I chose to pivot away from biomedical research.\n\n\n\nA view of my thinking face for all of my major life decisions during my PhD."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#how-did-i-choose-my-principal-investigator-pi",
    "href": "posts/phd-takeaways/index.html#how-did-i-choose-my-principal-investigator-pi",
    "title": "My doctoral research: takeaways and advice",
    "section": "How did I choose my Principal Investigator (PI)?",
    "text": "How did I choose my Principal Investigator (PI)?\nThe highest priority for me in picking my PhD lab was finding a group whose research spoke to me. After this, there were a few pieces that led me to settle on Dr. Dokyoon Kim.\nComing directly out of my undergrad, I knew that I would need a lot of support from my PI. So, I wanted to work with a more junior professor who would have the time to help me when I needed it. I also liked the idea of being one of the first students in a new lab and marking my own path. Based on my rotation, I could tell that my PI was highly attentive, and I had plenty of face-time with him each week, as well as support from post-docs and engineers in the lab whenever I needed it.\nFurther, I appreciated that my PI had a program set up with internationally-based clinicians to stay in Philadelphia and work in the lab each year. This gave countless opportunities to learn from and collaborate with people in the medical field as well as gain deeper insight into the impact that our projects could have downstream.\nLastly, it just so happened that my rotation with my PI coincided with March 2020, the start of the full impact of the COVID-19 pandemic on the U.S. In the face of entirely remote work for an unknown amount of time, it was immediately apparent to me that my PI would be great about supporting me virtually for however long we were required to work from home."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#how-did-i-choose-my-dissertation-topic",
    "href": "posts/phd-takeaways/index.html#how-did-i-choose-my-dissertation-topic",
    "title": "My doctoral research: takeaways and advice",
    "section": "How did I choose my dissertation topic?",
    "text": "How did I choose my dissertation topic?\nTo read more about the context and content of my dissertation, you can read my previous posts here and here.\nPersonally, I’ve always found that I excel the most when I am passionate about the projects I’m pursuing. It is important for me to not only see the motivators of my work, but also its downstream impact. As a result, I wanted to pursue a dissertation that felt intuitive and important.\nComing from a computational background, I did not have a disease area of interest that I had to focus on. Indeed, the biological question at hand was less important to me in my initial choice of project than its impact.\nLastly, I wanted the opportunity to familiarize myself with new types of data, to develop new computational methods and tools that could be used by the biomedical research community, and to see the translational impact of my work on people’s lives.\nMy rotation project (you can read the published manuscript for this project here) gave me an incredible view into the potential of my research trajectory at the start of my PhD. I loved how intuitive the baseline concepts of network medicine were, and I could see how it had the potential to bring together scientific storytelling aspects of data visualization with advanced technical research in graph-based machine learning. Ultimately, this dissertation topic felt like something that I could truly take full ownership of."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#my-personal-phd-pros",
    "href": "posts/phd-takeaways/index.html#my-personal-phd-pros",
    "title": "My doctoral research: takeaways and advice",
    "section": "My personal PhD pros",
    "text": "My personal PhD pros\n\nI gained strong interdisciplinary expertise in my subject matter:\n\nBiomedical informatics, computational genomics, translational science\nData science, software development, statistics, machine learning\n\nI learned how to conduct independent research and lead the direction of projects\nI gained valuable experience in mentoring and teaching others\nI was able to network with many amazing colleagues both in academia and industry in my discipline\nI was able to keep making income throughout the duration of my degree\nAfter my degree, I am taken more seriously by people in my field whom I meet for the first time\nI feel tremendously prepared to take on leadership roles in exciting interdisciplinary research areas in the future"
  },
  {
    "objectID": "posts/phd-takeaways/index.html#my-personal-phd-cons",
    "href": "posts/phd-takeaways/index.html#my-personal-phd-cons",
    "title": "My doctoral research: takeaways and advice",
    "section": "My personal PhD cons",
    "text": "My personal PhD cons\n\nHaving a PhD will make the jobs that you seek more niche\n\nWhen you pursue a PhD for career advancement, you’re typically seeking a career that is beyond the norm\nBachelor’s-level (and to an extent, Master’s-level) jobs are less individually unique from one another, but they are more broadly available (e.g. software engineer)\nFinding the right type of opportunity for a PhD-level individual requires patience\n\nPursuing a PhD is a stressful experience!\n\nIt can be hard to set boundaries between your work and your personal life. There are no clear deadlines for your projects either… your work will expand to fill your time unless you set your own pace\nA lot of luck is involved in how quickly you can make progress. There are so many factors out of your control that can affect the success of your experiments and your publications\n\nThere is a financial cost to pursuing a PhD\n\nIf you can be accepted to a PhD program, then you can be accepted to a much higher-paying job in industry. It is a very personal decision regarding whether or not this drop in salary is worth it"
  },
  {
    "objectID": "posts/phd-takeaways/index.html#choosing-to-do-a-phd",
    "href": "posts/phd-takeaways/index.html#choosing-to-do-a-phd",
    "title": "My doctoral research: takeaways and advice",
    "section": "Choosing to do a PhD",
    "text": "Choosing to do a PhD\nThe biggest piece of advice I have here is… make sure you’re doing a PhD for the right reason.\n\nIt’s important to think about why you want a graduate degree in the first place… Becoming a professor? Seeking those longer-term “extraordinary” opportunities in industry? Pure academic curiosity?\n\nI know people in all three of these camps, and I think they’re all very justified reasons. There are plenty more reasons to do a PhD outside of these - everyone has their own individual biases that draw them to the experience.\n\nThere are also plenty of reasons NOT to do a PhD.\n\nThe worst reason to pursue a PhD is for the “prestige.”\n\nIf you don’t find yourself intrinsically excited about the work you’re doing at the end of the day, then stop wasting your time! It’s not worth spending so much time on something just because you want other people to think more of you.\nA caveat - this lack of intrinsic excitement is different from joining a PhD program and then experiencing lulls in your research where you’re frustrated with your progress. This latter occurrence is totally normal and quite common! At the end of the day, when the experiments work out, you’ll remember why you started your program in the first place.\n\nThe second worst reason to pursue a PhD is that you don’t know what else to do with your time.\n\nGetting a PhD is not a passive experience. You cannot just “let it happen to you.” You have to be active about seeking out opportunities, making connections with others, and progressing on your work in order to succeed.\n\nIf you find yourself in either of these camps, then I can guarantee that you will be miserable and that you will burn out.\n\nIf you want to gain more knowledge, there are plenty of more lucrative / less time-intensive ways to do so than pursuing a PhD:\n\nPursue a different type of graduate degree\nFind a job relevant to your career\nJoin a bootcamp\nTake individual classes / online courses\nPursue independent projects (maybe even start a blog! ;))\n\nThere will always be a tradeoff when you decide to pursue a PhD. Some doors will open and other doors will close. Think about what’s best for YOU in your life and for your career.\n\n\n\n\nMy transition from first-year student to graduate. A pandemic and a doctorate degree will make you older AND wiser!"
  },
  {
    "objectID": "posts/phd-takeaways/index.html#selecting-a-program",
    "href": "posts/phd-takeaways/index.html#selecting-a-program",
    "title": "My doctoral research: takeaways and advice",
    "section": "Selecting a program",
    "text": "Selecting a program\nI’ve discussed earlier why I personally chose to attend Penn GCB, but here are some good questions to ask yourself when selecting a PhD program to attend:\n\nDo you like the work that you’ll be doing?\n\nThis is the whole point of a PhD - to do cutting-edge work. You need to like the field you’re in and the opportunities that will be available to you\n\nIs there more than one professor with whom you could see yourself working?\n\nEven if you have a professor who has committed to taking you on, this point is important to consider. Professors are people too, and they move around universities all the time. Make sure you’re not joining a program just because of a single person - otherwise, if they leave or if you position doesn’t work out, you may find yourself scrambling to find a new professor in the middle of your PhD who may not even align with your research interests.\n\nWill you have the right level of support for your background?\n\nSome people will come with a lot of experience and need less guidance when they start their PhDs. Others will come with minimal experience and need more hand-holding.\n\nI was in this second camp - I needed lots of hand-holding for biomedical concepts, and I wanted more independence in my explorations of computer science and statistics\n\nMake sure that the program can help you up-skill as needed (i.e. through coursework, registering for conferences / workshops, connecting you with the right mentors, etc.)\n\nHow is work-life harmony handled in this program?\n\nI cannot stress this enough - you are more than your work. You will need to work hard in a PhD, but you cannot let it absorb your entire life. You will burn out if you do. A PhD is a marathon and not a sprint.\nYou won’t necessarily need an active student community, but your peers are the only people who will truly understand the day-to-day of what being a PhD student means. You’ll find that being a part of such a community can be tremendously rewarding, and that your colleagues will be a huge help in your times of need.\nYou can gauge the status of how well work boundaries are set by your program through the students you meet during interview/admit visits. Obviously a lot of your work-life balance will come down to the lab you join. But in general, are the students happy with their choice to join this program? Do they have time for things outside of their work?\n\nDo you like where will you be living?\n\nDo you like the location of the program? Can you see yourself spending 5+ years there?\nIs the place you’re living affordable given the stipend that the program offers? How are housing/rental costs in the area?\nIs it easy to get to work? If not, how often will you expect to be coming in to campus?\n\nHow does the program support career progression and what are the types of opportunities that may be available to you after graduation?\n\nWhat kinds of support systems does the program have for career development?\n\nIs there support for internships? Fellowships / grants? Travel opportunities to conferences? Mentorship / teaching opportunities? The ability to earn additional certificates / degrees?\n\nWhat do alumni from the program usually do after graduation? Where (physically) do they end up? How much did the program and/or their research focus help with finding a job?\nIn these situations, when you have an admission to a program and you’re trying to decide on it, it’s great to speak to not only current students but also alumni to get a sense for the pros and cons of the program. You may not know the right questions to ask during these informational interviews, but if the program has a good community, then they’ll be happy to help you out regardless."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#selecting-a-thesis-advisor",
    "href": "posts/phd-takeaways/index.html#selecting-a-thesis-advisor",
    "title": "My doctoral research: takeaways and advice",
    "section": "Selecting a thesis advisor",
    "text": "Selecting a thesis advisor\n\nYour PhD advisor doesn’t have to be your best friend, but you should ideally have a friendly relationship with them :) Are they a nice person to work with? Do they have your best interests at heart? Or are they more concerned with using your time and work to advance the standing of their lab?\nDo you like the work that you’d be doing with this PI? It’s important that you don’t pick a lab just because you like the mentor’s personality.\nPick a PI who will lift you up rather than hold you down\n\nYou want to be challenged, but you don’t want to make life harder for yourself. Your PI is directly responsible for:\n\nthe type of research you explore\nwhen you graduate\nhow much work-life balance you have\n\nAt the end of the day, a great PI should always be your advocate!\n\nRegarding co-mentorships…\n\nCo-mentorships across two groups can be great if you have the right projects in mind and need both labs’ expertise.\nHowever, these can also go very poorly if your work aligns more with one group than the other. There’s a high likelihood that you will fall between the cracks and be stuck in your PhD for much longer than you need to be.\n\nSometimes it’ll still work out, especially if both PI’s already collaborate. But in these cases, I personally think it seems unnecessary to have both professors be your mentor. You can always have one of these professors serve on your thesis committee instead.\n\nHere’s my personal opinion… If you are coming in with less experience, I would advise picking a single PI. If you are coming in with more experience and know exactly what type of dissertation you want to work on, then you can consider multiple PIs."
  },
  {
    "objectID": "posts/phd-takeaways/index.html#picking-research-projects",
    "href": "posts/phd-takeaways/index.html#picking-research-projects",
    "title": "My doctoral research: takeaways and advice",
    "section": "Picking research projects",
    "text": "Picking research projects\n\nHere’s a big “duh” piece of advice – pick a research topic you’re excited about! Why would you spend 5+ years of your life on something that doesn’t get you excited?\nAnother point - prioritize skill-building when you can, but don’t prioritize it over progress on your dissertation.\n\nYou can up-skill in specific areas as much as you want after your PhD. If you are distracted by “side-quests,” you will take longer to graduate when you could have instead finished your degree earlier and kept progressing in your career.\n\nThink about the trade-off between your academic passions and the logistics of your work. Try to find the optimal balance across academic curiosity, skill-building, and time required for the research project.\n\nData generation will always take longer than expected. The easiest way to cut down on the time needed for your PhD is to work on projects where the data are already generated :)"
  },
  {
    "objectID": "posts/phd-takeaways/index.html#wrapping-up-your-phd",
    "href": "posts/phd-takeaways/index.html#wrapping-up-your-phd",
    "title": "My doctoral research: takeaways and advice",
    "section": "Wrapping up your PhD",
    "text": "Wrapping up your PhD\n\nIn my personal opinion, ending a PhD is an exercise in self-confidence and believing in oneself. Ultimately, completing your PhD means knowing how to advocate for yourself.\n\nThis can be easier or harder depending on your PI and your thesis committee. Some PIs / committees will be on the same page as you. They may even say themselves that it’s time for you to defend.\nOthers will not tell you they think you are ready to leave. It is up to you to justify in your committee meetings why you feel qualified to defend and graduate.\n\nHere’s my biggest indicator for when it’s time to graduate - when you no longer feel that you NEED guidance from your superiors.\n\nYou don’t need to feel that you have nothing more to learn. In fact, pursuing a PhD will teach you that you always have more to learn!\nYou don’t even have to have fulfilled all the goals of your dissertation… the aims that you come up with at the start of your disseration are somewhat arbitrary benchmarks.\nInstead… Are you able to devise a full research project concept and methodology? Can you procure the right data and follow through on the analysis? Are you able to communicate your results in a cogent, impactful manner?\n\nI hit a point toward the end of my PhD where I felt I could still keep learning and exploring, but I was coming up with all of the directions of the exploration myself. In other words, I was an independent researcher! This was my cue to wrap up and defend.\n\n\nWith that, we’ve reached the conclusion of my series on my doctoral research! If you’ve read this far, I hope you found the information I shared to be useful. The process of picking, starting, and completing a PhD is a tremendous challenge, and if you’re struggling at any point with any of the concepts I’ve covered today, feel free to reach out to me on LinkedIn or shoot me an email at vivek.sriram@gmail.com! I am always happy to chat and offer my two cents.\nAnd a last reminder… as I’ve said earlier in this post - you are not your work! Regardless of the stressors and major decisions that surround you, never forget to remember what matters most at the end of the day: your personal happiness and well-being. Make sure to take time to enjoy the little things in life, like this squirrel :)\n\n\nImage References:\n\nParamount Plus\nMemebase.com\nBusiness Insider"
  }
]