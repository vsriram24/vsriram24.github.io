<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Home</title>
<link>https://vsriram24.github.io/blog.html</link>
<atom:link href="https://vsriram24.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description>Vivek Sriram&#39;s personal website</description>
<generator>quarto-1.6.42</generator>
<lastBuildDate>Mon, 27 Oct 2025 07:00:00 GMT</lastBuildDate>
<item>
  <title>Foundations of Deep Learning</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/dl-for-bio-lec2/</link>
  <description><![CDATA[ 





<p>Welcome to Week 2 of my <a href="https://bios740.github.io/about/">Deep Learning in Biology</a> self-study series! In today’s post, I’ll dive into the core building blocks of deep learning: how neural networks work, how they’re trained, and the modern architectures driving today’s AI systems, particularly in biomedical contexts.</p>
<p><img src="https://vsriram24.github.io/posts/dl-for-bio-lec2/mtn.jpg" class="img-fluid"></p>
<section id="neural-network-basics" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-basics">Neural Network Basics</h2>
<p>At the heart of deep learning are <strong>neural networks</strong>, composed of layers of interconnected units (also known as neurons). Each <strong>connection</strong> has an associated <strong>weight</strong> that determines the strength of influence one neuron has on the next. These weights are <strong>learnable parameters</strong> adjusted during training.</p>
<p>Each neuron also has a <strong>bias</strong>, which shifts the input to the activation function. Biases help the model better fit data by offsetting the activation threshold—just like an intercept in linear regression.</p>
<p>To introduce <strong>non-linearity</strong>, we make use of <strong>activation functions</strong>. These allow networks to approximate complex, real-world relationships:</p>
<ul>
<li><p><strong>Sigmoid</strong>: Outputs between 0 and 1; good for binary classification.</p></li>
<li><p><strong>Tanh</strong>: Outputs between -1 and 1; zero-centered and better for deep networks.</p></li>
<li><p><strong>ReLU</strong>: Efficient and widely used; outputs positive values only.</p></li>
<li><p><strong>Softmax</strong>: Used for multi-class classification; outputs probabilities that sum to 1.</p></li>
</ul>
</section>
<section id="forward-backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="forward-backpropagation">Forward &amp; Backpropagation</h2>
<p>Training a neural network involves two primary steps:</p>
<ol type="1">
<li><p><strong>Forward Propagation</strong><br>
Inputs pass through the network layer by layer until predictions are made.</p></li>
<li><p><strong>Backpropagation</strong><br>
Errors from the output are propagated backward to update the weights. Gradients are computed using the <strong>chain rule</strong>, guiding the model to minimize its <strong>loss function</strong>.</p></li>
</ol>
<p>This cycle repeats over many <strong>epochs</strong> (passes through the training data), gradually refining the model.</p>
</section>
<section id="key-deep-learning-model-architectures" class="level2">
<h2 class="anchored" data-anchor-id="key-deep-learning-model-architectures">Key Deep Learning Model Architectures</h2>
<p>Modern deep learning uses specialized architectures, each tailored to specific data types and tasks. We will go more in depth into these subtypes of deep learning models in the coming weeks:</p>
<section id="convolutional-neural-networks-cnns" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-networks-cnns">1. Convolutional Neural Networks (CNNs)</h3>
<ul>
<li><p><strong>Use case</strong>: Image data</p></li>
<li><p><strong>Components</strong>:</p>
<ul>
<li><p><strong>Convolutional layers</strong>&nbsp;extract spatial features.</p></li>
<li><p><strong>Pooling layers</strong>&nbsp;reduce dimensionality.</p></li>
<li><p><strong>Fully connected layers</strong>&nbsp;make predictions.</p></li>
</ul></li>
<li><p><strong>Biomedical applications</strong>: Diagnostic imaging, tumor segmentation</p></li>
<li><p><strong>Example models</strong>: LeNet-5, AlexNet, VGGNet</p></li>
</ul>
</section>
<section id="recurrent-neural-networks-rnns" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-networks-rnns">2. Recurrent Neural Networks (RNNs)</h3>
<ul>
<li><p><strong>Use case</strong>: Sequential data (e.g., EHRs, time-series)</p></li>
<li><p><strong>Feature</strong>: Loops that “remember” prior inputs</p></li>
<li><p><strong>Variants</strong>: LSTM, GRU—solve vanishing gradient issues</p></li>
<li><p><strong>Biomedical applications</strong>: Monitoring vitals over time, clinical event prediction</p></li>
</ul>
</section>
<section id="u-net" class="level3">
<h3 class="anchored" data-anchor-id="u-net">3. U-Net</h3>
<ul>
<li><p><strong>Use case</strong>: Image segmentation</p></li>
<li><p><strong>Architecture</strong>: U-shaped with encoder-decoder paths and skip connections</p></li>
<li><p><strong>Variants</strong>: 2D/3D U-Net, Attention U-Net</p></li>
<li><p><strong>Biomedical applications</strong>: Organ delineation, lesion detection</p></li>
</ul>
</section>
<section id="autoencoders" class="level3">
<h3 class="anchored" data-anchor-id="autoencoders">4. Autoencoders</h3>
<ul>
<li><p><strong>Use case</strong>: Unsupervised feature learning, dimensionality reduction</p></li>
<li><p><strong>Structure</strong>: Encoder compresses, decoder reconstructs</p></li>
<li><p><strong>Variants</strong>: Standard AE, Variational AE (VAE)</p></li>
<li><p><strong>Biomedical applications</strong>: Denoising images, detecting anomalies</p></li>
</ul>
</section>
<section id="graph-neural-networks-gnns" class="level3">
<h3 class="anchored" data-anchor-id="graph-neural-networks-gnns">5. Graph Neural Networks (GNNs)</h3>
<ul>
<li><p><strong>Use case</strong>: Graph-structured data (networks)</p></li>
<li><p><strong>Structure</strong>: Nodes, edges, graph convolutions</p></li>
<li><p><strong>Variants</strong>: GCN, GAT, GRN, Graph Autoencoder</p></li>
<li><p><strong>Biomedical applications</strong>: Drug discovery, gene interaction networks</p></li>
</ul>
</section>
<section id="generative-adversarial-networks-gans" class="level3">
<h3 class="anchored" data-anchor-id="generative-adversarial-networks-gans">6. Generative Adversarial Networks (GANs)</h3>
<ul>
<li><p><strong>Use case</strong>: Data generation</p></li>
<li><p><strong>Mechanism</strong>: Two networks (Generator vs.&nbsp;Discriminator) in a game-like setup</p></li>
<li><p><strong>Variants</strong>: DCGAN, CycleGAN, Pix2Pix</p></li>
<li><p><strong>Biomedical applications</strong>: Synthesizing high-resolution medical images, augmenting datasets</p></li>
</ul>
</section>
<section id="transformers" class="level3">
<h3 class="anchored" data-anchor-id="transformers">7. Transformers</h3>
<ul>
<li><p><strong>Use case</strong>: Sequence modeling without recurrence</p></li>
<li><p><strong>Core idea</strong>: Self-attention mechanisms</p></li>
<li><p><strong>Variants</strong>: BERT, AlphaFold</p></li>
<li><p><strong>Biomedical applications</strong>: Protein folding, DNA sequence analysis</p></li>
</ul>
</section>
<section id="deep-reinforcement-learning-drl" class="level3">
<h3 class="anchored" data-anchor-id="deep-reinforcement-learning-drl">8. Deep Reinforcement Learning (DRL)</h3>
<ul>
<li><p><strong>Use case</strong>: Decision-making in dynamic environments</p></li>
<li><p><strong>Key concepts</strong>: Agent, environment, reward, policy, value function</p></li>
<li><p><strong>Example algorithms</strong>: DQN, PPO, A3C, SAC</p></li>
<li><p><strong>Applications</strong>: Robotics, autonomous surgery, adaptive diagnostics</p></li>
</ul>
</section>
</section>
<section id="perceptrons-multilayer-perceptrons-mlps" class="level2">
<h2 class="anchored" data-anchor-id="perceptrons-multilayer-perceptrons-mlps">Perceptrons &amp; Multilayer Perceptrons (MLPs)</h2>
<p>Before we go into some of the “deeper” forms of DL, let’s start with the basics. The <strong>Perceptron</strong> is the simplest form of a neural network: A perceptron takes multiple inputs, applies weights, adds bias, and produces a binary output. Perceptrons also make use of a step activation function (a binary deactivate / activate) to determine the final output.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/dl-for-bio-lec2/Perceptron-unit.svg.png" class="img-fluid figure-img" width="290"></p>
<figcaption>A diagram of a perceptron</figcaption>
</figure>
</div>
<p><strong>Multilayer Perceptrons (MLPs)</strong> expand on the perceptron model - MLPs contain multiple layers, including an input layer, hidden layer(s), and an output layer. MLPs are capable of learning non-linear patterns in the data using their multiple hidden layers, and we can specify different types of MLPs using the following hyperparameters: width (number of units per layer), depth (number of hidden layers), and capacity (total number of learnable parameters).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/dl-for-bio-lec2/Multi-Layer_Perceptron_(MLP)_Neural_Network._From_Left_to_right_Inputs,_Weights,_Perceptron_Neurons_in_Hidden_Layer,_Weights_and_Output_Layer.png" class="img-fluid figure-img" width="452"></p>
<figcaption>A diagram of a multilayer perceptron</figcaption>
</figure>
</div>
<section id="the-universal-approximation-theorem" class="level3">
<h3 class="anchored" data-anchor-id="the-universal-approximation-theorem">The Universal Approximation Theorem:</h3>
<p>The universal approximation theorem states that a neural network with enough width or depth can approximate any continuous function. However, when considering the structure of a neural network, it is important to remember the tradeoffs between width and depth:</p>
<ul>
<li><p><strong>Wider networks</strong> are simpler but more inefficient</p></li>
<li><p><strong>Deeper networks</strong> are more compact but harder to train</p></li>
</ul>
</section>
</section>
<section id="activation-functions" class="level2">
<h2 class="anchored" data-anchor-id="activation-functions">Activation Functions</h2>
<p>Activation functions are added to the end of each perceptron to introduce non-linearity into the produced output. Different activation functions serve different purposes. In binary classification, we would introduce a sigmoid activation function in the output layer to convert the output to a 0-1 value. For multi-class classification, this activation function would become a softmax, letting us convert the output to one of multiple categories. And for a regression, the activation function can simply be a linear function, returning the output of the neural network scaled to an appropriate value. For intermediate hidden layers, rectified linear units (ReLUs) can generate output if the input is positive and return zeroes for any negative inputs. Alternatively, Tanh activation functions can be used if the data are centered, shifting output to a value between -1 and 1.</p>
</section>
<section id="loss-functions" class="level2">
<h2 class="anchored" data-anchor-id="loss-functions">Loss Functions</h2>
<p>Loss functions are used to quantify the difference between predictions and true labels. Different types of loss functions can apply different constraints and transformations to the data, allowing us to manipulate the behavior of the model to forms that we find most pertinent.</p>
<section id="for-regression" class="level3">
<h3 class="anchored" data-anchor-id="for-regression">For regression:</h3>
<ul>
<li><p><strong>MSE</strong> penalizes large errors more, but is sensitive to outliers.</p></li>
<li><p><strong>MAE</strong> institutes an equal penalty for all errors (even if they are large), but is more robust to outliers.</p></li>
<li><p><strong>Huber Loss</strong> is a hybrid of MSE and MAE, allowing for robustness against large errors and outliers.</p></li>
</ul>
</section>
<section id="for-classification" class="level3">
<h3 class="anchored" data-anchor-id="for-classification">For classification:</h3>
<ul>
<li><p><strong>Binary Cross-Entropy</strong> is used for binary tasks, while <strong>Cross-Entropy</strong> is effective for multi-class tasks.</p></li>
<li><p><strong>KL Divergence</strong> measures the divergence between two distributions, and is used in architectures such as Variational Autoencoders (VAEs)</p></li>
<li><p><strong>Negative Log Likelihood (NLL)</strong> can be used with log-softmax outputs</p></li>
<li><p>As a special case, <strong>Dice Loss</strong> is effective for segmentation tasks with imbalanced classes</p></li>
</ul>
</section>
</section>
<section id="optimization-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="optimization-algorithms">Optimization Algorithms</h2>
<p>In addition to selecting the appropriate loss function to maximize the utility of mdel training, an effective optimization algorithm should be chosen as well to improve model training. One of the most basic forms of model optimization is <strong>stochastic gradient descent (SGD) -</strong> however, this approach can be noisy and slow. Concepts like momentum and adaptive learning rates can be introduced to reduce oscillations around local loss minima - for instance, the <strong>Adaptive Moment Estimation (Adam) optimizer</strong> combines the advantages of multiple optimization algorithms to adjust learning rates during training.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/dl-for-bio-lec2/Gradient_descent.gif" class="img-fluid figure-img"></p>
<figcaption>A visual depiction of gradient descent</figcaption>
</figure>
</div>
<p>Even with advances in optimization algorithms, a variety of challenges remain. Vanishing gradients occur when the derivatives of the activation functions become close to zero during backpropagation, hindering effective training and weight updates. This issue can arise for many deep neural networks, and it is particularly associated with the sigmoid and tanh activation functions that have smaller ranges of derivatives.</p>
<p>Exploding gradients represent the reverse problem, when gradients of the network’s loss function become excessively large with respect to the weights of the model - this outcome is more tied to the weights of the network than the choice of activation function, with high weights leading to large derivatives. As a result, gradients fail to converge, leading to networks oscillating around local minima.</p>
<p>Solutions for these issues include <strong>batch normalization</strong>, where the activations are normalized within each mini batch of data, effectively scaling gradients and reducing variance. <strong>Gradient clipping</strong> can also impose a threshold on the gradients during backpropagation, preventing them from becoming too small or exploding. Other activation functions (e.g.&nbsp;ReLUs) and other types of networks (ResNets, LSTMs, and GRUs) can address the vanishing gradient problem by skipping certain layers during backpropagation or incorporating gating mechanisms.</p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>Deep learning is a powerful tool for the analysis of multidimensional, complex data. Understanding the basics of how neural networks work, how they’re trained, and when to use different architectures is crucial for building practical and effective models, especially in high-stakes domains like the field of biomedicine.</p>


</section>

 ]]></description>
  <category>Overviews</category>
  <guid>https://vsriram24.github.io/posts/dl-for-bio-lec2/</guid>
  <pubDate>Mon, 27 Oct 2025 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/dl-for-bio-lec2/wrappedup.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>An Intro to Deep Learning for Biology</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/dl-for-bio-lec1/</link>
  <description><![CDATA[ 





<p>Welcome back to [VS]Codes! In an effort to find publicly available material that could help me bolster my deep learning fundamentals and offer an opportunity for structured self-learning, I came across* this delightful course from the UNC Biostatistics program, titled “<a href="https://bios740.github.io/about/">Deep Learning Methods in Biomedical Sciences</a>.” As the course description explains:</p>
<p><em>“This course … offers an in-depth exploration of how advanced computational techniques intersect with complex biomedical data to drive new discoveries. As biomedical datasets expand in size and complexity, traditional algorithms often struggle to capture subtle patterns and relationships. In contrast, deep learning … offers robust solutions for some of the field’s most challenging problems.”</em></p>
<p>So, I plan to work through the lectures and homework assignments offered by this course, summarizing my takeaways and highlighting key points. By reframing the content from these lecture slides into corresponding blog posts, my goal is to turn my understanding of these topics into mastery! And hopefully, you’ll learn a little something along the way too. With that, let’s get started!</p>
<p>*I believe I first heard about this course from my old colleague <a href="https://leibyj.github.io">Jake Leiby</a>. Thank you, Jake!</p>
<p><img src="https://vsriram24.github.io/posts/dl-for-bio-lec1/reflection.JPG" class="img-fluid" width="675"></p>
<section id="ai-vs.-ml-vs.-dl-vs.-genai-whats-in-a-name" class="level2">
<h2 class="anchored" data-anchor-id="ai-vs.-ml-vs.-dl-vs.-genai-whats-in-a-name">AI vs.&nbsp;ML vs.&nbsp;DL vs.&nbsp;GenAI: what’s in a name?</h2>
<p>At its core, <strong>deep learning (DL)</strong> is a sub-field of machine learning that uses <strong>neural networks</strong> to extract and learn complex patterns from data. Here’s a breakdown of how it fits into the broader AI landscape:</p>
<ul>
<li><p><strong>Artificial Intelligence (AI)</strong>: Algorithms that simulate human intelligence (e.g., decision-making, language translation).</p></li>
<li><p><strong>Machine Learning (ML)</strong>: A subset of AI where systems learn patterns from data, rather than being explicitly programmed.</p></li>
<li><p><strong>Deep Learning (DL)</strong>: A further subset of ML that uses <em>deep</em> (i.e.&nbsp;multi-layered) neural networks to model hierarchical relationships in data.</p></li>
<li><p><strong>Generative AI (GenAI)</strong>: DL-based systems that can generate new data (e.g.&nbsp;text, images, molecules) using learned patterns.</p></li>
</ul>
<p><img src="https://vsriram24.github.io/posts/dl-for-bio-lec1/comparingAI.png" class="img-fluid"></p>
<p>Several factors have led to the rapid growth of DL in recent years, including (a) faster compute (with the spread of GPUs and TPUs over CPUs), (b) the availability of large datasets, and (c) advances in optimization algorithms and neural network architectures. These advances have made DL especially effective at handling <strong>unstructured data</strong>, including images, text, and genomic sequences.</p>
</section>
<section id="neural-networks-the-backbone-of-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="neural-networks-the-backbone-of-deep-learning">Neural Networks: The Backbone of Deep Learning</h2>
<p>Neural networks are the core structures in DL. A typical neural net has:</p>
<ul>
<li><p><strong>An input layer</strong> that receives raw data</p></li>
<li><p><strong>Multiple hidden layers</strong> that learn and represent features of the data through nonlinear transformations</p></li>
<li><p><strong>An output layer</strong> that produces predictions or classifications</p></li>
</ul>
<p><img src="https://vsriram24.github.io/posts/dl-for-bio-lec1/ann.png" class="img-fluid" width="461"></p>
<p>The <strong>depth</strong> of a neural network refers to the number of layers that it contains, while the <strong>width</strong> refers to the number of neurons per layer. A network’s <strong>capacity</strong> refers to the product of these two values.</p>
<p>Key components of neural networks include:</p>
<ul>
<li><p><strong>Neurons</strong>: Nodes that perform weighted computations</p></li>
<li><p><strong>Weights and biases</strong>: Parameters learned during training</p></li>
<li><p><strong>Activation functions</strong>: Introduce non-linearity. Examples include the ReLU, tanh, sigmoid functions</p></li>
</ul>
<p><img src="https://vsriram24.github.io/posts/dl-for-bio-lec1/neuron.svg" class="img-fluid" width="481"></p>
<section id="shallow-vs.-deep-models" class="level3">
<h3 class="anchored" data-anchor-id="shallow-vs.-deep-models">Shallow vs.&nbsp;Deep Models</h3>
<p>While both shallow and deep networks can be used to approximate complex functions, <strong>deep networks are able to do so more efficiently</strong> - they can create more nonlinear “pieces” using fewer parameters. This hierarchical representation of data is particularly helpful when learning from high-dimensional modalities (e.g., multiomics or imaging).</p>
</section>
</section>
<section id="types-of-learning" class="level2">
<h2 class="anchored" data-anchor-id="types-of-learning">Types of Learning</h2>
<p>Deep learning supports various learning paradigms:</p>
<ul>
<li><p><strong>Supervised Learning</strong>: Learn from labeled data (e.g., disease prediction from gene expression)</p></li>
<li><p><strong>Unsupervised Learning</strong>: Discover hidden patterns without labels (e.g., clustering cells)</p></li>
<li><p><strong>Reinforcement Learning</strong>: Learn to make decisions in an environment (e.g., adaptive treatment strategies)</p></li>
</ul>
<p>There are also newer paradigms such as:</p>
<ul>
<li><p><a href="https://www.geeksforgeeks.org/machine-learning/self-supervised-learning-ssl/"><strong>Self-supervised</strong> <strong>learning</strong></a><strong>:</strong> Train the model using self-learned labels</p></li>
<li><p><a href="https://lilianweng.github.io/posts/2021-05-31-contrastive/"><strong>Contrastive learning</strong></a><strong>:</strong> Train the model to distinguish similar and dissimilar data points</p></li>
<li><p><a href="https://www.ibm.com/think/topics/few-shot-learning"><strong>Few-shot learning</strong></a><strong>:</strong> Train a model to learn from a small number of labeled examples</p></li>
<li><p><a href="https://developers.google.com/machine-learning/gan/generative"><strong>Generative modeling</strong></a><strong>:</strong> Models that learn to create new, original data that resemble the training data</p></li>
</ul>
</section>
<section id="statistical-modeling-vs.-algorithmic-modeling" class="level2">
<h2 class="anchored" data-anchor-id="statistical-modeling-vs.-algorithmic-modeling">Statistical Modeling vs.&nbsp;Algorithmic Modeling</h2>
<p>Statistics can be broken into two distinct historical cultures:</p>
<ul>
<li><p><strong>Data modeling</strong>: Assumes a predefined probabilistic model (e.g., linear regression)</p></li>
<li><p><strong>Algorithmic modeling</strong>: Focuses on flexible predictive models without assuming a specific data-generating process</p></li>
</ul>
<p>DL belongs firmly in the <strong>algorithmic camp.</strong></p>
</section>
<section id="why-deep-learning-for-biology" class="level2">
<h2 class="anchored" data-anchor-id="why-deep-learning-for-biology">Why Deep Learning for Biology?</h2>
<p>The field of biology presents several challenges that are ideal for the application of deep learning, including high-dimensional, noisy, and heterogeneous data, as well as complex, nonlinear relationships across systems.</p>
<p>Deep learning shines in these situations because it:</p>
<ul>
<li><p>Learns features directly from raw data without the need for preprocessing or feature extraction</p></li>
<li><p>Can handle multimodal inputs</p></li>
<li><p>Scales to large datasets, which are becoming increasingly available in biology and medicine</p></li>
</ul>
<p>The field of biomedical AI is moving fast - some exciting applications include:</p>
<ul>
<li><p><strong>AI-driven public health</strong>: Forecasting disease spread and optimizing interventions</p></li>
<li><p><strong>Personalized drug response modeling</strong>: Predicting individual reactions to drugs using multiomic and clinical data</p></li>
<li><p><strong>Multiomic integration</strong>: Combining modalities including genomics, transcriptomics, and proteomics for better disease understanding</p></li>
<li><p><strong>Generalist medical AI</strong>: Developing large foundation models that work across data modalities for tasks like diagnosis, prognosis, and treatment planning</p></li>
</ul>
</section>
<section id="getting-started-with-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-pytorch">Getting Started with PyTorch</h2>
<p><img src="https://vsriram24.github.io/posts/dl-for-bio-lec1/pytorch.png" class="img-fluid" width="398"></p>
<p>PyTorch is the deep learning framework that will be used for the rest of this course. Here are its core components:</p>
<ul>
<li><p><code>torch.nn</code>: Defines layers and building blocks for neural networks</p></li>
<li><p><code>torch.nn.Module</code>: Base class for all PyTorch models</p></li>
<li><p><code>torch.optim</code>: Optimization algorithms for training (e.g., SGD, Adam)</p></li>
<li><p><code>torch.utils.data.Dataset</code>: Custom dataset wrappers</p></li>
<li><p><code>torch.utils.data.DataLoader</code>: Batches and shuffles data for training</p></li>
</ul>
<p>We’ll dive deeper into each of these as we build models in future tutorials.</p>
</section>
<section id="wrap-up" class="level2">
<h2 class="anchored" data-anchor-id="wrap-up">Wrap-Up</h2>
<p>That’s it for Lecture 1! We covered a broad overview of what deep learning is, why it matters for biology, and how we’ll start building models in PyTorch. Next up: neural network fundamentals, including back-propagation, initialization, and practical model training.</p>


</section>

 ]]></description>
  <category>Overviews</category>
  <guid>https://vsriram24.github.io/posts/dl-for-bio-lec1/</guid>
  <pubDate>Mon, 13 Oct 2025 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/dl-for-bio-lec1/tonguesout.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Bridging the Health AI Readiness Gap</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/stanford-aimi-25/</link>
  <description><![CDATA[ 





<p>Welcome back to another week of <em>[VS]Codes</em>! A few weeks ago, I had the opportunity to attend the <a href="https://aimi.stanford.edu/aimi25">2025 Stanford AIMI Symposium</a>, a hybrid in-person and remote conference hosted by the <a href="https://aimi.stanford.edu/">Stanford Center for Artificial Intelligence in Medicine and Imaging (AIMI)</a>, where researchers, clinicians, and industry leaders came together to discuss one of the most urgent challenges in health AI: closing the gap between technical innovation and real-world clinical adoption. Below are my notes and takeaways from all of the talks and panels that I attended.</p>
<p><img src="https://vsriram24.github.io/posts/stanford-aimi-25/aimi25.png" class="img-fluid"></p>
<hr>
<section id="opening-keynote-closing-the-gap-between-ai-capabilities-and-health-system-readiness" class="level2">
<h2 class="anchored" data-anchor-id="opening-keynote-closing-the-gap-between-ai-capabilities-and-health-system-readiness">Opening Keynote: “Closing the Gap Between AI Capabilities and Health System Readiness”</h2>
<ul>
<li><a href="https://profiles.ucsf.edu/julia.adler-milstein">Julia Adler-Milstein</a> (UCSF)</li>
</ul>
<p>Adler-Milstein mapped the history of <em>enterprise technology</em> (how we get large organizations to adopt new tech) in healthcare into four major waves: <strong>EHRs</strong>, <strong>Interoperability</strong>, <strong>Telemedicine</strong>, and <strong>AI</strong></p>
<p>Each past wave has been catalyzed by different forces. For instance, EHR development was driven by federal policy, while telemedicine arose as a common practice as a result of the COVID-19 pandemic.</p>
<p>AI, however, is different. There is no policy mandate or acute crisis driving its adoption. Rather, it presents a fast-moving set of technologies with the potential to solve sticky healthcare problems in new and exciting ways.</p>
<p>The core tension of the AI era? <strong>Technology is outpacing the people, processes, and infrastructure meant to adopt it</strong>. Adler-Milstein challenged the audience to think beyond model performance and to consider “market readiness” and organizational capacity as equally important. Her framework for this process, the <em>Health AI Delivery Agenda</em>, focused on five under-examined but critical questions:</p>
<ol type="1">
<li><strong>Will electronic health information (eHI) and LLMs finally deliver on the promise of patient engagement?</strong> We’ve had access to electronic health information for years, but how do we help patients understand their data? Should prompt engineering be built into patient portals?</li>
<li><strong>Is there institutional capacity to act on AI predictions?</strong> Do implementation leaders have enough authority to change existing clinical workflows?</li>
<li><strong>What is the optimal “unit” of AI?</strong> Can we move beyond modular predictive models to closed-loop systems that span administrative and clinical functions?</li>
<li><strong>How do we establish robust, ongoing monitoring of AI performance?</strong> Who notices when models degrade or perform poorly across subpopulations? And who is responsible for fixing these issues when they arise?</li>
<li><strong>What are the best investments for creating an AI-ready workforce?</strong> Should we start with the creation of <a href="https://www.vocabulary.com/dictionary/stopgap#:~:text=Definitions%20of%20stopgap,expedient">stopgap</a> training programs? How do we assess skill erosion and development?</li>
</ol>
</section>
<section id="panel-1-hard-won-lessons-from-the-front-lines-of-clinical-ai" class="level2">
<h2 class="anchored" data-anchor-id="panel-1-hard-won-lessons-from-the-front-lines-of-clinical-ai">Panel 1: Hard-won lessons from the front lines of clinical AI</h2>
<p><strong>Panelists</strong>: <a href="https://www.linkedin.com/in/ruijunchen/">Ray Chen</a> (Ambience Healthcare), <a href="https://www.linkedin.com/in/toyinfalola/">Toyin Falola</a> (Providence Health), <a href="https://profiles.stanford.edu/sneha-shah-jain">Sneha Jain</a> (Stanford), <a href="https://www.linkedin.com/in/izzomd/">Joseph Izzo</a> (Kaiser Permanente / San Joaquin General)</p>
<p>This panel focused on implementation reality: governance, trust, and usability. One memorable metaphor compared AI stakeholders to a “chicken and pig starting a restaurant,” highlighting the difference between basic involvement and deep commitment. The success of clinical AI, they argued, hinges on executive-level sponsorship: “Some AI tools are helpful. A few are transformational. But none can flourish without institutional commitment at the top.”</p>
<p>Another recurring theme from the panel was the concept of workflow integration. For clinicians to trust and adopt tools, AI cannot be an extra step - it must be invisible, reliable, and tightly woven into existing practices.</p>
</section>
<section id="scientific-talks-session-1" class="level2">
<h2 class="anchored" data-anchor-id="scientific-talks-session-1">Scientific Talks Session 1</h2>
<ol type="1">
<li><p><a href="https://www.linkedin.com/in/carolcain/">Carol Cain</a> (Kaiser Permanente): <strong>Quality Assurance for AI-Powered Clinical Documentation</strong></p>
<p>Cain shared a case study on implementing ambient scribe technology (<a href="https://www.abridge.com">Abridge</a>) across Kaiser Permanente - arguably the fastest roll-out of any new tool in their history. Their goal: reduce clinician documentation burden while preserving the quality of patient interactions. Even a 5-minute delay in note generation is a deal-breaker in clinical workflows. Their evaluation framework balanced speed and safety using a combination of structured note quality reviews, starred tech performance ratings, and user feedback and scenario-specific assessments</p>
<p>Common issues identified through their evaluation framework included content omission, over-synthesis of information, and confusion between speaker voices (<a href="https://en.wikipedia.org/wiki/Speaker_diarisation">speaker diarisation</a>).</p>
<p>Their strategy to address these challenges? Decentralized risk identification combined with centralized improvement cycles - “We needed fast feedback loops, but not at the expense of local nuance.” Ultimately, success came from balancing centralized governance with local responsiveness.</p></li>
<li><p><a href="https://www.linkedin.com/in/daniel-morgan-03899252/">Daniel Morgan</a> (Veterans Affairs): <strong>Detecting HAIs with LLMs</strong></p>
<p>Morgan presented an ambitious pilot: using GPT-4 to help identify central line-associated bloodstream infections (CLABSI), an important quality metric tied to CMS reimbursement.</p>
<p>In current practice, the identification of CLABSI requires manual chart review, which can be slow, subjective, and inconsistently applied. Using a secure, local deployment of GPT-4, the VA was able to compare CLABSI charting across three review processes: human-only review, AI-only review, and AI-assisted human review.</p>
<p>Morgan and his team found that AI-assisted review was both faster (14 vs.&nbsp;25 mins) and more accurate than manual review alone. Still, trust remained a hurdle: clinicians felt compelled to double-check outputs of the model in the EHR. Furthermore, GPT-4 made some avoidable errors in the study. Nevertheless, these errors were easily correctable by the providers, suggesting that a collaborative framework may be optimal. Next steps include integrating different forms of structured and unstructured data into the predictive models, as well as expanding to other hospital-acquired conditions.</p></li>
</ol>
</section>
<section id="panel-2-the-foundation-model-roadmap-what-health-ai-teams-need-to-know" class="level2">
<h2 class="anchored" data-anchor-id="panel-2-the-foundation-model-roadmap-what-health-ai-teams-need-to-know">Panel 2: The foundation model roadmap – what health AI teams need to know</h2>
<p><strong>Panelists</strong>: <a href="https://profiles.stanford.edu/emily-alsentzer">Emily Alsentzer</a> (Stanford), <a href="https://www.linkedin.com/in/khaled-saab-181034122/">Khaled Saab</a> (Google DeepMind), <a href="https://www.linkedin.com/in/karan1149/">Karan Singhal</a> (OpenAI), <a href="https://dbmi.hms.harvard.edu/people/marinka-zitnik">Marinka Zitnik</a> (Harvard)</p>
<p>This panel dove into practical implications of working with foundation models in health. Key takeaways included the following:</p>
<ul>
<li><p>Smaller, better models are on the rise, but performance still hinges on context and fine-tuning. Medical fine-tuning may degrade a larger model’s general skills, making training pipelines for healthcare use cases a delicate balance. Researchers and clinicians will have to carefully consider how ’omics data and other nontraditional inputs are incorporated into these systems to ensure that scalability and precision are maintained as best as possible.</p></li>
<li><p>With respect to the <a href="https://en.wikipedia.org/wiki/Nondeterministic_algorithm">nondeterminism</a> of LLMs, concepts including <a href="https://zilliz.com/glossary/semantic-similarity">semantic linkage</a> and prompt variability remain under-explored areas. Furthermore, tradeoffs in transparency and control have to be considered when choosing between open and closed models.</p></li>
</ul>
</section>
<section id="lightning-talks-session-1" class="level2">
<h2 class="anchored" data-anchor-id="lightning-talks-session-1">Lightning Talks Session 1</h2>
<ol type="1">
<li><a href="https://www.linkedin.com/in/brice-gaudilliere-1528147/">Brice Gaudillere</a> (Stanford): <strong>Driving Precision Medicine by Decoding the Human Immune System</strong></li>
</ol>
<ul>
<li><p>Aiming to build the first foundation model for complex immunological datasets to identify biomarkers for tools</p></li>
<li><p>90% of ’omics studies have fewer than 100 patients involved… makes it difficult to identify statistically relevant biomarkers</p></li>
<li><p>Use a noise injection technique to identify the most reliable, stable biomarkers</p></li>
</ul>
<ol start="2" type="1">
<li><a href="https://www.linkedin.com/in/drxuanzhao/">Xuan Zhao</a> (Flourish Science): <strong>Empowering Every Mental Health and Wellness Journey</strong></li>
</ol>
<ul>
<li><p>Switch from reactive to proactive mental health treatment</p></li>
<li><p>The “Last Mile Problem” of science - how do we deliver the right insight at the right time?</p></li>
<li><p>Use AI to facilitate personalized mental health support</p>
<ul>
<li>Awareness, Action, Habit Building, Social Connection</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li><a href="https://www.linkedin.com/in/marcos-rojas-pino/">Marcos Rojas</a> (Stanford): <strong>Clinical Mind AI: Improving Clinical Reasoning with AI</strong></li>
</ol>
<ul>
<li><p>Medical school and residency are focused on teaching clinical reasoning</p></li>
<li><p>Challenges in teaching present opportunities to use new technologies:</p></li>
<li><p>An AI-powered learning platform designed to enhance clinical reasoning for healthcare practitioners</p>
<ul>
<li><p>Create customized AI-simulated patients</p></li>
<li><p>Provide real-time feedback</p></li>
<li><p>Adapt to diverse medical curricula</p></li>
</ul></li>
</ul>
</section>
<section id="panel-3-preparing-clinicians-for-an-ai-enabled-future" class="level2">
<h2 class="anchored" data-anchor-id="panel-3-preparing-clinicians-for-an-ai-enabled-future">Panel 3: Preparing clinicians for an AI-enabled future</h2>
<p><strong>Panelists:</strong> <a href="https://www.linkedin.com/in/erichorvitz/">Eric Horvitz</a> (Microsoft), <a href="https://www.linkedin.com/in/daniel-ting-0b327895/">Daniel Ting</a> (Duke), <a href="https://www.linkedin.com/in/danielayang/">Daniel Yang</a> (Kaiser Permanente), <a href="https://www.linkedin.com/in/julie-wu-4730321a5/">Julie Tsu-Yu Wu</a> (Veterans Affairs)</p>
<p>The biggest point of consensus for these panelists was the following: “Clinicians won’t be replaced. But clinicians who use AI effectively will set the new norm.” True innovation lies in co-creation between technologists and care teams. Lastly, the use of AI in compute was compared to the transition from horses to cars - before the invention of cars, if asked what they wanted, the public would have said “faster horses.” We need to think outside of the box of what’s possible, broadening the AI vision beyond note summarization and information retrieval to new applications that would never have been possible before.</p>
</section>
<section id="scientific-talks-session-2" class="level2">
<h2 class="anchored" data-anchor-id="scientific-talks-session-2">Scientific Talks Session 2</h2>
<ol type="1">
<li><a href="https://profiles.stanford.edu/francois-camille-grolleau-raoux">Francois Raoux</a> (Stanford): <strong>MedAgentBrief</strong></li>
</ol>
<ul>
<li><p>What’s the most significant issue in healthcare today? The explosion of (often hidden) information that overwhelms human capabilities</p>
<ul>
<li><p>Pages of unstructured, sometimes inconsistent data from discharge summaries</p></li>
<li><p>Provider “pajama time”: late-night summary writing</p></li>
<li><p>Average chart-closure delay: 24-72 hours post-discharge</p></li>
<li><p>Diverts time from hand-offs / direct patient conversations</p></li>
</ul></li>
<li><p>High-quality summaries could solve these issues</p>
<ul>
<li><p>Two approaches toward scalable and trustworthy evaluation sof hospital course summaries</p>
<p>a. Human Expert Evaluation: trust-worthy but slow</p>
<p>b. LLM judge: lack of trust; need an oracle LLM to judge</p></li>
</ul></li>
<li><p>How to get best of both worlds?</p>
<ul>
<li>MedFactEval: experts come up with expected outputs (what key points need to be included). Expensive, but one-time cost
<ul>
<li>Then an LLM judges the generated text to make sure physician opinions are present</li>
<li>Compared to full physician evaluations</li>
</ul></li>
</ul></li>
<li><p>Impact Evaluation</p>
<ul>
<li>Safety, Effectiveness, Expansion, Post-deployment surveillance</li>
</ul></li>
</ul>
<ol start="2" type="1">
<li><a href="https://profiles.stanford.edu/vasiliki-bikia">Vicky Bikia</a> (Stanford): <strong>Toward Scalable Clinical Evaluation: Building Ground Truth for Discharge Summaries Using LLMs</strong></li>
</ol>
<ul>
<li><p>Fluent ≠ Factual. Traditional NLP baselines can’t assess clinical factuality</p></li>
<li><p>Perturbation Engine - introduce clinical errors into reference summaries that can be used to train smaller models with fewer parameters</p>
<ul>
<li>Creating labeled ground truth data at scale with controlled, clinically meaningful errors</li>
</ul></li>
<li><p>SAFRAN builds on this foundation with:</p>
<ul>
<li><p>clinically grounded error categories</p></li>
<li><p>scalable perturbation-based datasets</p></li>
<li><p>student-teacher LLM evaluation and quantitative scoring</p></li>
</ul></li>
<li><p>Together, enable scalable, automated benchmarking of LLM-generated clinical summaries</p>
<ul>
<li>Bridge expert review and real-world deployment</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li><a href="https://profiles.stanford.edu/xiaohan-wang">Xiohan Wang</a> (Stanford): <strong>Towards AI-Assisted Surgery and Surgical Training</strong></li>
</ol>
<ul>
<li><p>Spatio-temporal modeling of hands during surgeries</p></li>
<li><p>Video Self-training with augmented reasoning</p>
<ul>
<li>Iterative self-training to create VideoVLM with better reasoning capabilities</li>
</ul></li>
</ul>
</section>
<section id="panel-4-roi-which-ai-solutions-are-built-to-scale" class="level2">
<h2 class="anchored" data-anchor-id="panel-4-roi-which-ai-solutions-are-built-to-scale">Panel 4: ROI – Which AI solutions are built to scale?</h2>
<p><strong>Panelists:</strong> <a href="https://www.linkedin.com/in/lynne-chou-o-keefe-5617a/">Lynne Chou O’Keefe</a> (Define Ventures), <a href="https://www.linkedin.com/in/fabiënneterhuurne/">Fabiënne ter Huurne</a> (Bayer), <a href="https://www.linkedin.com/in/galymimanbayev/">Galym Imanbayev</a> (Lightspeed Venture Partners), <a href="https://www.linkedin.com/in/christine-nguyen310/">Christine Nguyen</a> (Inland Empire Health Plan)</p>
<p>This venture-focused panel tackled the business reality of health AI. Some key points included the following:</p>
<ul>
<li>Companies like Abridge are gaining traction by positioning themselves as viable alternatives to dominant players like Nuance. Success requires clearly demonstrating your unique value in a crowded market.</li>
<li>Health tech evolves in phases - early versions don’t need to be perfect, but teams must be bold enough to launch, iterate, and build momentum. Intermediate phases are about accumulating wins and expanding use cases, while late stage phases should focus on transforming care delivery and redefining value.</li>
<li>Venture capital looks for scalable ROI across healthcare verticals, but alignment with patient outcomes is key. Innovations should ideally benefit both investors and patients, particularly by lowering long-term costs.</li>
<li>While improving outcomes can eventually reduce costs, not every new technology is cost-effective upfront. Strategic investment and evaluation are essential.</li>
<li>Much of today’s innovation focuses on simplifying administrative tasks to free up provider time and improve efficiency. Clinical AI is likely to be the next major focus of funding, but administrative solutions are still going to have major impact in the long-term</li>
</ul>
</section>
<section id="lightning-talks-session-2" class="level2">
<h2 class="anchored" data-anchor-id="lightning-talks-session-2">Lightning Talks Session 2</h2>
<ol type="1">
<li><a href="https://www.linkedin.com/in/dimitrytran/">Dimitry Tran</a> (Harrison.ai)</li>
</ol>
<ul>
<li>There is a global shortage of providers that is leading to delay of care
<ul>
<li>10 years’ worth of promise to catch up to in fields of radiology and pathology</li>
<li>We need to go from “isolated clinical predictions” and “AI point solutions” to comprehensive AI.</li>
<li>Harrison.ai creates models that have the ability to handle multiple subtypes of diagnoses</li>
</ul></li>
<li>Next steps: Vision Language Models
<ul>
<li>AI takes in imaging and patient history to write a report</li>
<li>Fill in a template with section/headings/etc.</li>
</ul></li>
</ul>
<ol start="2" type="1">
<li><a href="https://www.linkedin.com/in/kichun/">Kimberly Chun</a> (Cohere): <strong>Path to Agentic AI for Healthcare</strong></li>
</ol>
<ul>
<li>Cohere offers customized, multilingual, accurate, secure models for both on-prem and cloud platforms</li>
<li>Narrow products that are entirely focused around the needs of individual customers
<ul>
<li>Collaboration across a variety of sectors</li>
</ul></li>
<li>The path to agentic AI includes:
<ul>
<li>Single container deployment of autonomous agent applications (Application Layer)</li>
<li>RAG-optimized generative models</li>
<li>Search applications</li>
<li>Plug-and-play customizable integrations with popular business apps</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li><a href="https://www.linkedin.com/in/jayodita-sanghvi-34b1057/">Jayodita Sangvhi</a> (Color): <strong>Large Language Experts: Trustworthy AI for Complex Clinical Workflows</strong></li>
</ol>
<ul>
<li><p>What will AI’s role in medicine be? A smart efficient assistant vs.&nbsp;a trustworthy expert?</p>
<ul>
<li>There needs to be a balance between efficiency and accuracy/trust</li>
</ul></li>
<li><p>In oncology, timely workup between diagnosis and treatment is critical but complex</p>
<ul>
<li>There are a lot of steps to go from first abnormal screen to treatment</li>
</ul></li>
<li><p>Our goal is to smooth the process out for improved prognosis</p></li>
<li><p>Color Copilot: generative AI mixed with clinical guidelines</p>
<ul>
<li><p>Input guidelines into LLM to create structured questions/prompts</p></li>
<li><p>Extract clinical decision factors from patient summary → input into a logic evaluater</p></li>
<li><p>Last step: another LLM to provide contextualized recommendation / explanation for reasoning</p></li>
</ul></li>
</ul>
<ol start="4" type="1">
<li><a href="https://www.linkedin.com/in/daniel-golden/">Daniel Golden</a> (Google Health): <strong>MedGemma: Accelerating the Health AI developer community with open medical foundation models</strong></li>
</ol>
<ul>
<li><p>About Google Research’s Health AI team</p>
<ul>
<li><p>Mission: catalyze the adoption of human-centered AI in health</p></li>
<li><p>Show the world how to build safe and effective AI</p></li>
<li><p>Enable others to build AI for their use cases</p></li>
</ul></li>
<li><p>Health AI Developer Foundations (HAI-DEF)</p>
<ul>
<li><p>Open models to accelerate the development of AI for healthcare and life sciences</p></li>
<li><p>Medical Embeddings</p></li>
<li><p>TxGemma (therapeutics simulation)</p></li>
<li><p>MedGemma (Medical image and text comprehension)</p></li>
</ul></li>
<li><p>MedGemma includes 2 different models</p>
<ul>
<li><p>Smaller 4B param model for multimodal data integration</p></li>
<li><p>Larger 27B param thinking model: clinical reasoning, triage, summarization / retrieval, etc.</p></li>
</ul></li>
<li><p>Models are:</p>
<ul>
<li><p>Medically tuned - customized image encoder and tuned text capabilities</p></li>
<li><p>High performance - performance approaches larger, proprietary models</p></li>
<li><p>Open to all - ideal starting point for developers working on medical research or products/applications</p></li>
</ul></li>
<li><p>Although it may not be as accurate as the biggest models, it allows data to stay on site, permits adaptation for specific research applications, offline use, low inference costs, and medical device regulatory approval</p>
<ul>
<li><p>Publicly available on Hugging Face</p></li>
<li><p><a href="https://goo.gle/hai-def" class="uri">https://goo.gle/hai-def</a></p></li>
</ul></li>
</ul>
<ol start="5" type="1">
<li><a href="https://www.linkedin.com/in/krishnaramkenthapadi/">Krishnaram Kenthapadi</a> (Oracle Health AI): <strong>Oracle Health Clinical AI Agents: Insights from Building and Deploying AI Agents</strong></li>
</ol>
<ul>
<li><p>Administrative burden - we need to restore the joy in providing patient care</p></li>
<li><p>Clinical AI Agent - voice-first agentic experiences</p>
<ul>
<li>Make AI accessible</li>
</ul></li>
<li><p>Multi-agent orchestrator interacts with UI agents, API agents, search agents, etc. LLM based tool calling and argument extraction</p>
<ul>
<li>App context, conversation history, patient context as input</li>
</ul></li>
<li><p>Engage with domain experts to improve data quality and evaluation methodology in addition to model dev</p>
<ul>
<li><p>Prioritizing which investmenets are likely to become important</p></li>
<li><p>Early and rapid iteration to validate key assumptions and obtain early feedback</p></li>
</ul></li>
<li><p>Trustworthy AI is crucial for adoption of agents in healthcare</p>
<ul>
<li><p>Long-term monitoring is important too</p></li>
<li><p>Catching new biases, performance degradation</p></li>
</ul></li>
</ul>
</section>
<section id="panel-5-publishing-health-ai-how-journals-are-shaping-the-future" class="level2">
<h2 class="anchored" data-anchor-id="panel-5-publishing-health-ai-how-journals-are-shaping-the-future">Panel 5: Publishing health AI – how journals are shaping the future</h2>
<p>Panelists: <a href="https://www.linkedin.com/in/charlotte-j-haug-md-phd-msc-53162721/">Charlotte Haug</a> (NEJM AI), <a href="https://hswen.ucsf.edu">Yulin Hswen</a> (JAMA), <a href="https://www.linkedin.com/in/drchrispaton/">Chris Paton</a> (BMJ Digital Health and AI)</p>
<p>AI publishing in healthcare needs a major shift. Journals should prioritize transparency, standardized evaluation methods, and clear explanations of modeling decisions, with editors aligned on consistent expectations. Given the patchy state of international regulation, alternative frameworks are needed to balance access and reproducibility. The current publishing pace lags far behind the rapid evolution of AI, and while large language models show promise, they’re not yet equipped to support peer review. Journals also tend to overvalue novelty and impact at the expense of incremental but meaningful progress. Ultimately, the focus must shift from algorithmic flashiness to real-world improvements in patient outcomes.</p>
</section>
<section id="fireside-chat-a-policy-perspective-for-moving-clinical-ai-forward" class="level2">
<h2 class="anchored" data-anchor-id="fireside-chat-a-policy-perspective-for-moving-clinical-ai-forward">Fireside Chat: A Policy Perspective for Moving Clinical AI Forward</h2>
<p>Speakers: <a href="https://law.stanford.edu/michelle-m-mello/">Michelle Mello</a> (Stanford), <a href="https://profiles.ucsf.edu/julia.adler-milstein">Julia Adler-Milstein</a> (UCSF)</p>
<p>In today’s current political environment, the U.S. may be drifting toward an unregulated clinical AI marketplace - nevertheless, thoughtful regulation is essential for sustainable innovation. As Mello put it, we need to avoid the “Magpie vs.&nbsp;Cat” trap - don’t chase every shiny new model like a magpie… instead, be more like a cat: selective, skeptical, and willing to walk away from hype.</p>
<p>Effective AI governance should empower organizations to say “no,” keeping the patient as the ultimate North Star. This means shifting from narrow, individual-level predictions to systems-level thinking, and avoiding the “Turing Trap” of simply trying to mimic human behavior. Instead, AI should enable new capabilities and long-term value creation.</p>
<p>While speed is important, lasting impact matters more, especially in a field where success depends on not just clinical accuracy, but also clinical feasibility. Furthermore, “snapshot AI” isn’t enough - healthcare is inherently longitudinal, and predictions for patients must be treated the same way.</p>
<p>Finally, there’s a major medical data gap. Healthcare training datasets are far smaller than in other domains, making it critical to invest in robust, domain-specific data strategies to ensure meaningful progress.</p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h2>
<p>Overall, the Stanford AIMI Conference revealed a major theme for me: <strong>AI alone won’t transform healthcare</strong>. While AI’s technical trajectory is unstoppable, its impact is neither inevitable nor evenly distributed. Current bottlenecks in the incorporation of AI into larger clinical practice are human, organizational, and regulatory.</p>
<p>Nevertheless, the talks and panels at the conference also highlighted a hopeful next step in the pursuit of health AI - the next frontier doesn’t lie just in the creation of better models, but in <strong>designing institutional ecosystems ready to absorb, govern, and benefit from them</strong>.</p>
<p>Ultimately, the implementation of broader health AI won’t succeed through technology itself. It requires courageous leadership, human trust, and organizational vision.</p>


</section>

 ]]></description>
  <category>Conferences</category>
  <guid>https://vsriram24.github.io/posts/stanford-aimi-25/</guid>
  <pubDate>Mon, 23 Jun 2025 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/stanford-aimi-25/tonguesout.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The Sloan Precision Oncology Institute’s AI Symposium 2025</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/sloan-precision-ai-conf/</link>
  <description><![CDATA[ 





<p>Welcome back to another post from <em>[VS]Codes</em>! Last week, I had the opportunity to attend the Fred Hutch <a href="https://www.fredhutch.org/en/research/institutes-networks-ircs/sloan-precision-oncology-institute.html">Sloan Precision Oncology Institute</a>’s AI Symposium, co-hosted by <a href="https://www.fredhutch.org/en/provider-directory/eric-collisson.html">Dr.&nbsp;Eric Collisson</a>, Director of Translational Integration at the Sloan Precision Oncology Institute and <a href="https://www.fredhutch.org/en/faculty-lab-directory/leek-jeff.html">Dr.&nbsp;Jeff Leek</a>, Fred Hutch’s Chief Data Officer. The focus of the symposium was to bring together leading researchers, clinicians, and technologists to explore how artificial intelligence, in particular, large language models (LLMs) and multimodal learning, can transform precision oncology. This blog post summarizes my notes from each of the panels that I attended and offers some of my biggest overall takeaways.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/sloan-precision-ai-conf/keynote.jpg" class="img-fluid figure-img"></p>
<figcaption>Dr.&nbsp;Ali Farhadi, the keynote speaker of the event, presenting on the development of AI agents for scientific research</figcaption>
</figure>
</div>
<hr>
<section id="ai-in-healthcare" class="level3">
<h3 class="anchored" data-anchor-id="ai-in-healthcare">1. AI in Healthcare</h3>
<section id="hannaneh-hajishirzi-university-of-washington-and-allen-institute-for-ai-post-training-language-models-and-reasoning" class="level4">
<h4 class="anchored" data-anchor-id="hannaneh-hajishirzi-university-of-washington-and-allen-institute-for-ai-post-training-language-models-and-reasoning"><a href="https://homes.cs.washington.edu/~hannaneh/"><strong>Hannaneh Hajishirzi</strong></a> <strong>(University of Washington and Allen Institute for AI)</strong> – Post-Training Language Models and Reasoning</h4>
<p>Hannaneh Hajishirzi emphasized the importance of post-training techniques in ensuring that language models align with human values and remain safe and effective for downstream tasks. Her team has focused on building an open ecosystem for instruction tuning through efforts like <a href="https://allenai.org/tulu">Tulu</a>, a framework for specialized model training that preserves foundational capabilities. She highlighted the challenges of data curation, advocating for the use of synthetic data generated by larger models to train smaller ones more efficiently. The talk also covered advances in preference optimization strategies including RLAIF (reinforcement learning from AI feedback) and RLVR (reinforcement learning with verifiable rewards), which aim to enhance model reasoning and human alignment.</p>
<ul>
<li>Key takeaways:
<ul>
<li>Post-training is essential for making LLMs safe and useful</li>
<li>Open-source tools like Tulu help democratize access to instruction tuning.</li>
</ul></li>
</ul>
</section>
<section id="eli-van-allen-dana-farber-cancer-institute-enhancing-precision-cancer-medicine-with-biologically-guided-ai" class="level4">
<h4 class="anchored" data-anchor-id="eli-van-allen-dana-farber-cancer-institute-enhancing-precision-cancer-medicine-with-biologically-guided-ai"><a href="https://www.dana-farber.org/find-a-doctor/eliezer-van-allen"><strong>Eli Van Allen</strong></a> <strong>(Dana-Farber Cancer Institute)</strong> – Enhancing Precision Cancer Medicine with Biologically-Guided AI</h4>
<p>Eli Van Allen began his talk with an overview of <a href="https://www.nature.com/articles/s41586-021-03922-4">P-NET</a>, a biologically-informed sparse neural network that embeds known molecular pathway structures (via the <a href="https://reactome.org">Reactome database</a>) into its architecture, allowing for greater biological interpretability. He addressed the need to integrate fast-evolving multi-omics data in clinical practice, noting that existing clinical trial frameworks are not always equipped for the pace of precision oncology. He also shared updates on the <a href="https://moalmanac.org">Molecular Oncology Almanac</a>, a clinical platform that can help match patients’ molecular profiles to relevant treatment or trial options.</p>
<ul>
<li>Key takeaways:
<ul>
<li>Biologically grounded AI models can improve interpretability and clinical relevance</li>
<li>Real-time integration of genomic data is vital for precision medicine.</li>
</ul></li>
</ul>
</section>
<section id="pang-wei-koh-university-of-washington-and-allen-institute-for-ai-evaluating-model-weaknesses-and-data-privacy" class="level4">
<h4 class="anchored" data-anchor-id="pang-wei-koh-university-of-washington-and-allen-institute-for-ai-evaluating-model-weaknesses-and-data-privacy"><a href="https://koh.pw"><strong>Pang Wei Koh</strong></a> <strong>(University of Washington and Allen Institute for AI)</strong> – Evaluating Model Weaknesses and Data Privacy</h4>
<p>Pang Wei Koh introduced <a href="https://arxiv.org/abs/2503.08893">EvalTree</a>, a method for identifying failure profiles for language models by organizing errors into a capability-based hierarchy. This framework improves upon broader evaluation metrics like accuracy or precision on an entire benchmarking dataset, facilitating targeted improvements in model performance and offering suggestions for more precise training data augmentation. He also discussed the limitations of current approaches to data sanitization, noting that de-identification for the purposes of protecting Personally Identifiable Information (PII) is often incomplete and that balancing privacy with utility remains an open challenge in clinical NLP.</p>
<ul>
<li>Key takeaways:
<ul>
<li>Fine-grained evaluations like EvalTree can help systematically improve model behavior</li>
<li>Ensuring full protection of PII in clinical text while facilitating utility of the data remains a complex challenge in the coming years.</li>
</ul></li>
</ul>
</section>
</section>
<section id="ai-in-electronic-medical-records" class="level3">
<h3 class="anchored" data-anchor-id="ai-in-electronic-medical-records">2. AI in Electronic Medical Records</h3>
<section id="travis-zack-ucsf-adapting-language-models-for-medical-information-retrieval" class="level4">
<h4 class="anchored" data-anchor-id="travis-zack-ucsf-adapting-language-models-for-medical-information-retrieval"><a href="https://www.ucsfhealth.org/providers/dr-travis-zack"><strong>Travis Zack</strong></a> <strong>(UCSF)</strong> – Adapting Language Models for Medical Information Retrieval</h4>
<p>Travis Zack presented an overarching framework for adapting LLMs to medical tasks, outlining the stages from pretraining and alignment to specialization and clinical deployment. He emphasized the need for <a href="https://www.sciencedirect.com/topics/social-sciences/construct-validity">construct validity</a> in medical benchmarks and careful prompt engineering to ensure LLM outputs are clinically meaningful. He compared proprietary models (GPT, MedPalm) with open ones (e.g., Llama, Mistral), highlighting trade-offs between performance, control, and privacy. Fine-tuning and <a href="https://huggingface.co/blog/moe">mixture-of-experts</a> approaches were suggested as promising paths for clinical specialization.</p>
<ul>
<li>Key takeaways:
<ul>
<li>Successful medical LLM deployment requires careful benchmarking and specialization</li>
<li>Open models offer more control but often need fine-tuning to match clinical needs.</li>
</ul></li>
</ul>
</section>
<section id="kenneth-kehl-dana-farber-cancer-institute-ai-in-cancer-care-and-clinical-research" class="level4">
<h4 class="anchored" data-anchor-id="kenneth-kehl-dana-farber-cancer-institute-ai-in-cancer-care-and-clinical-research"><a href="https://www.dana-farber.org/find-a-doctor/kenneth-l-kehl"><strong>Kenneth Kehl</strong></a> <strong>(Dana-Farber Cancer Institute)</strong> – AI in Cancer Care and Clinical Research</h4>
<p>Kenneth Kehl traced the history of AI in healthcare back to the 1970s and focused on how today’s models can improve clinical trial matching and observational research. He described the development of <a href="https://matchminer.org">MatchMiner</a> and <a href="https://arxiv.org/abs/2412.17228">MatchMiner-AI</a>, which uses LLMs to extract relevant biomarkers and eligibility criteria for cancer trial matching. He also advocated for democratizing advanced analytics via user-friendly tools, enabling researchers without deep AI expertise to explore and analyze oncology datasets.</p>
<ul>
<li>Key takeaways:
<ul>
<li>LLMs can scale and automate trial matching</li>
<li>Simplifying access to analytics tools broadens research participation.</li>
</ul></li>
</ul>
</section>
<section id="rui-zhang-university-of-minnesota-cancer-phenotype-extraction-and-cardiotoxicity-prediction" class="level4">
<h4 class="anchored" data-anchor-id="rui-zhang-university-of-minnesota-cancer-phenotype-extraction-and-cardiotoxicity-prediction"><a href="https://med.umn.edu/bio/rui-zhang"><strong>Rui Zhang</strong></a> <strong>(University of Minnesota)</strong> – Cancer Phenotype Extraction and Cardiotoxicity Prediction</h4>
<p>Rui Zhang presented <a href="https://pubmed.ncbi.nlm.nih.gov/35333345/">CancerBERT</a> and <a href="https://arxiv.org/abs/2406.10459">CancerLLM</a>, domain-adapted language models trained on clinical notes and pathology reports to improve cancer phenotyping and prediction of cardiotoxic side effects. He warned about LLM hallucinations in medical NLP and advocated for manual review of model outputs and the inclusion of example-driven prompts to reduce error. His models demonstrated improved performance on structured phenotype extraction from complex free text.</p>
<ul>
<li>Key takeaways:
<ul>
<li>Domain-specific LLMs outperform general ones in clinical NLP</li>
<li>Human-in-the-loop validation is critical for safe deployment.</li>
</ul></li>
</ul>
</section>
</section>
<section id="keynote" class="level3">
<h3 class="anchored" data-anchor-id="keynote">3. Keynote</h3>
<section id="ali-farhadi-university-of-washington-and-allen-institute-for-ai-when-ai-meets-science" class="level4">
<h4 class="anchored" data-anchor-id="ali-farhadi-university-of-washington-and-allen-institute-for-ai-when-ai-meets-science"><a href="https://homes.cs.washington.edu/~ali/"><strong>Ali Farhadi</strong></a> <strong>(University of Washington and Allen Institute for AI)</strong> – When AI Meets Science</h4>
<p>Ali Farhadi critiqued the overreliance on general-purpose models for scientific discovery and advocated for bespoke expert models tailored to specific scientific problems. He introduced <a href="https://allenai.org/olmo">OLMo</a> and <a href="https://allenai.org/blog/olmoe-an-open-small-and-state-of-the-art-mixture-of-experts-model-c258432d0514">OLMoE</a>, open-source transformer models designed with science applications in mind, as well as <a href="https://huggingface.co/collections/allenai/pixmo-674746ea613028006285687b">PixMo vision-language datasets</a> and the <a href="https://allenai.org/blog/molmo">Molmo</a> model, which explore multimodal representations by combining vision and language. Farhadi emphasized <em>traceability</em> as a vital feature for scientific trust and reproducibility, enabling models to link outputs back to individual samples in the training data.</p>
<ul>
<li>Key takeaways:
<ul>
<li>Scientific AI requires purpose-built models, not just general-purpose LLMs</li>
<li>Transparency and traceability are essential for amplifying scientific trust.</li>
</ul></li>
</ul>
</section>
</section>
<section id="multimodal-data" class="level3">
<h3 class="anchored" data-anchor-id="multimodal-data">4. Multimodal Data</h3>
<section id="adam-yala-uc-berkeley-and-ucsf-machine-learning-to-personalize-cancer-care" class="level4">
<h4 class="anchored" data-anchor-id="adam-yala-uc-berkeley-and-ucsf-machine-learning-to-personalize-cancer-care"><a href="https://www.adamyala.org"><strong>Adam Yala</strong></a> <strong>(UC Berkeley and UCSF)</strong> – Machine Learning to Personalize Cancer Care</h4>
<p>Adam Yala shared the development and real-world deployment of <a href="https://jclinic.mit.edu/mirai/">MIRAI</a>, an open source, state of the art, deep learning-based breast cancer risk prediction model that integrates with clinical workflows to enable Same-Day Assessment (SDA). This approach reduces diagnostic delays and supports personalized screening. He also described <a href="https://arxiv.org/abs/2503.12355">Atlas</a>, a multimodal model using hierarchical attention to combine imaging, pathology, and EHR data. Lastly, Yala encouraged the audience to consider not just risk prediction but also clinical screening policies from an AI-based perspective. Using reinforcement learning, Yala proposed individualized screening policies that optimize long-term patient outcomes by balancing benefits and harms.</p>
<ul>
<li>Key takeaways:
<ul>
<li>Multimodal AI models like MIRAI can meaningfully improve cancer risk stratification and reduce delays</li>
<li>Techniques such as reinforcement learning can be applied to personalize not only disease risk prediction but also clinical decision-making.</li>
</ul></li>
</ul>
</section>
<section id="sohrab-shah-memorial-sloan-kettering-cancer-center-multimodal-analysis-as-a-frontier-of-computational-oncology" class="level4">
<h4 class="anchored" data-anchor-id="sohrab-shah-memorial-sloan-kettering-cancer-center-multimodal-analysis-as-a-frontier-of-computational-oncology"><a href="https://www.mskcc.org/profile/sohrab-shah"><strong>Sohrab Shah</strong></a> <strong>(Memorial Sloan Kettering Cancer Center)</strong> – Multimodal Analysis as a Frontier of Computational Oncology</h4>
<p>Sohrab Shah discussed the integration of radiology, pathology, genomics, and clinical text data to enable patient stratification and survival prediction in cancer care. He highlighted spatial biology as a key frontier, using spatial relationships and graph-based encodings to model the tumor microenvironment. His team’s approach uses multimodal graph neural networks to unify heterogeneous data into predictive and interpretable models for precision oncology.</p>
<ul>
<li>Key takeaways:
<ul>
<li>Integrating spatial and multimodal data enables richer biological understanding</li>
<li>Graph-based models offer a powerful framework for clinical predictions.</li>
</ul></li>
</ul>
</section>
<section id="robert-grant-princess-margaret-cancer-centre-charting-a-path-to-ai-augmented-clinical-oncology" class="level4">
<h4 class="anchored" data-anchor-id="robert-grant-princess-margaret-cancer-centre-charting-a-path-to-ai-augmented-clinical-oncology"><a href="https://www.ices.on.ca/ices-scientists/robert-grant/"><strong>Robert Grant</strong></a> <strong>(Princess Margaret Cancer Centre)</strong> – Charting a Path to AI-Augmented Clinical Oncology</h4>
<p>Robert Grant focused on how to responsibly bring AI tools into clinical settings. He emphasized the need for models that are not only accurate but well-calibrated, especially at different decision thresholds. He compared early vs.&nbsp;late fusion techniques for combining heterogeneous data modalities and argued that in many cases, simpler models like logistic regression can outperform more complex ones. For the near future, he encouraged prospective, patient-centered evaluation studies to assess AI’s true clinical value and called attention to the potential of LLMs for structuring messy EHR data.</p>
<ul>
<li>Key takeaways:
<ul>
<li>Model deployment must be guided by careful, patient-focused evaluation.</li>
<li>Simpler models can outperform complex ones in practice</li>
</ul></li>
</ul>
</section>
</section>
<section id="conclusions" class="level3">
<h3 class="anchored" data-anchor-id="conclusions"><strong>Conclusions</strong></h3>
<p>This concludes my summary of the Sloan Institute for Precision Oncology’s AI Symposium! My biggest takeaways from the conference were the following:</p>
<ul>
<li><p><strong>Post-training and fine-tuning are critical for safe, effective deployment of LLMs in healthcare.</strong>Across several talks, speakers emphasized the need to align language models with clinical goals through instruction tuning, preference optimization, and domain adaptation.</p></li>
<li><p><strong>Interpretability and domain alignment are non-negotiable in clinical AI.</strong> Whether through biologically informed architectures like P-NET or traceability tools like EvalTree and OLMOE, researchers are prioritizing transparency and aligning models with established scientific or clinical knowledge.</p></li>
<li><p><strong>LLMs show promise in structuring unstructured clinical data but must be validated carefully.</strong> Hallucination risks, privacy concerns, and generalizability remain open challenges, making human-in-the-loop validation and error analysis crucial for safe use in EMRs and trial matching.</p></li>
<li><p><strong>Deployment success depends on real-world integration and usability.</strong> Practical deployment of models requires seamless integration into clinical systems, prospective evaluation, and attention to model calibration across decision thresholds.</p></li>
</ul>
<p>With that, I’d like to give a huge thank you to the organizers of this conference for all of their hard work as well as to all of the speakers and panelists for contributing their time and knowledge to a series of excellent discussions.</p>


</section>

 ]]></description>
  <category>Conferences</category>
  <guid>https://vsriram24.github.io/posts/sloan-precision-ai-conf/</guid>
  <pubDate>Tue, 27 May 2025 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/sloan-precision-ai-conf/waiting.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>How to activate a virtual Python environment</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/python-venv/</link>
  <description><![CDATA[ 





<p>Welcome back to my personal biomedical data science blog, [VS]Codes! Today’s blog post will be a brief tutorial on how to create and activate a virtual environment for your Python project.</p>
<p>I’ve recently been working on a lot of code with older Python packages that are not supported for the most recent versions of Python. So, I’ve needed to find a way to make use of an alternate version of Python for my project. The solution? <strong>Setting up a Python virtual environment.</strong></p>
<p><img src="https://vsriram24.github.io/posts/python-venv/needle.jpg" class="img-fluid" width="500"></p>
<p>The following StackOverflow <a href="https://stackoverflow.com/questions/41972261/what-is-a-virtualenv-and-why-should-i-use-one">post</a> provides a great overview of the benefits of Python virtual environments. Here’s a brief summary of its key takeaways:</p>
<ul>
<li><p><strong>What are virtual environments?</strong></p>
<ul>
<li>“Virtual environments are lightweight, self-contained Python installations, designed to be set up without requiring extensive configuration or specialized knowledge.”</li>
</ul></li>
<li><p><strong>Why is it bad to use your system’s Python installation?</strong></p>
<ul>
<li>“Running with the system Python and libraries limits you to one specific Python version, chosen by your OS provider. Trying to run all Python applications on one Python installation makes it likely that version conflicts will occur among the collection of libraries. It’s also possible that changes to the system Python will break other OS features that depend on it.”</li>
</ul></li>
<li><p><strong>What do virtual environments do?</strong></p>
<ul>
<li>“Virtual environments avoid the need to install Python packages globally. When a virtual environment is active, <code>pip</code> will install packages entirely within the environment, which does not affect the base Python installation in any way.”</li>
</ul></li>
</ul>
<p>Now let’s go into the steps for setting up a Python virtual environment.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>These next steps assumes that you are working on a Mac. Our example will focus on setting up a Python 3.10 virtual environment. Refer to the StackOverflow post above for instructions for Windows systems.</p>
</div>
</div>
<section id="step-1-install-the-version-of-python-that-you-need" class="level2">
<h2 class="anchored" data-anchor-id="step-1-install-the-version-of-python-that-you-need">Step 1: Install the version of Python that you need</h2>
<p>First, install Python 3.10 if you don’t already have it. If you’re on macOS, you can use Homebrew to install Python 3.10 from your Terminal:</p>
<pre><code>brew install python@3.10</code></pre>
<p>Then, verify your installation:</p>
<pre><code>python3.10 --version</code></pre>
</section>
<section id="step-2-create-a-virtual-environment" class="level2">
<h2 class="anchored" data-anchor-id="step-2-create-a-virtual-environment">Step 2: Create a Virtual Environment</h2>
<p>Now, we can create a new virtual environment for our project using Python 3.10:</p>
<p>Navigate to your project folder in your Terminal:</p>
<pre><code>cd path/to/your/project</code></pre>
<p>Create a virtual environment using the following command:</p>
<pre><code>python3.10 -m venv [environment_name]</code></pre>
<p>For instance, if we wanted to name our environment <code>.breaking_bad</code>, our command would be:</p>
<pre><code>python3.10 -m venv .breaking_bad</code></pre>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is good practice to start the name of your virtual environment with a period (.) so that the folder becomes hidden by default on your system - in the process, you can keep your project directory clean, help prevent accidental Git commits, and more easily avoid potential namespace conflicts.</p>
</div>
</div>
<p>Finally, we can active the virtual environment we’ve just created!</p>
<pre><code>source .breaking_bad/bin/activate</code></pre>
<p>After activation, our terminal prompt should show the name of our environment (e.g.&nbsp;<code>(.breaking_bad)$</code>).</p>
</section>
<section id="step-3-install-your-required-python-packages" class="level2">
<h2 class="anchored" data-anchor-id="step-3-install-your-required-python-packages">Step 3: Install your required Python packages</h2>
<p>Once inside your virtual environment, you can install any Python packages that you want! For instance:</p>
<pre><code>(.breaking_bad)$ pip install medspacy</code></pre>
<p>And ta-da! Mission accomplished!</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li><a href="https://stackoverflow.com/questions/41972261/what-is-a-virtualenv-and-why-should-i-use-one">What is a virtualenv and why should I use one (StackOverflow)</a></li>
</ul>


</section>

 ]]></description>
  <category>Tutorials</category>
  <guid>https://vsriram24.github.io/posts/python-venv/</guid>
  <pubDate>Thu, 15 May 2025 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/python-venv/lewk.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>An introduction to ‘Infrastructure as Code’ (IaC)</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/infrastructure-as-code/</link>
  <description><![CDATA[ 





<p>Welcome back to my personal biomedical data science blog, <em>[VS]Codes</em>! In today’s post, I will provide a brief overview of the concept of “infrastructure as code” and why this practice holds an essential place in the operational strategies of modern data-driven organizations.</p>
<p><img src="https://vsriram24.github.io/posts/infrastructure-as-code/rocket.jpg" class="img-fluid" width="556"></p>
<hr>
<p>In the evolving landscape of software development and operations, <strong>Infrastructure as Code (IaC)</strong> has become a cornerstone of modern <strong>DevOps</strong> practices. By treating infrastructure configuration and provisioning as code, teams can achieve consistency, scalability, and automation in deploying their applications.</p>
<section id="but-first-what-even-is-devops" class="level2">
<h2 class="anchored" data-anchor-id="but-first-what-even-is-devops">But first… what even is DevOps?</h2>
<p><strong>DevOps</strong> refers to a set of practices and principles that aim to unify software development (the “dev”) and IT operations (the “ops”) to enhance collaboration, automate workflows, and improve the software delivery lifecycle.</p>
<section id="key-principles-of-devops-include" class="level4">
<h4 class="anchored" data-anchor-id="key-principles-of-devops-include">Key principles of DevOps include:</h4>
<ol type="1">
<li><p><strong>Collaboration and communication</strong>: Breaking down the siloes between developers and operations teams.</p></li>
<li><p><strong>Automation</strong>: Using tools to automate testing, integration, deployment, and infrastructure management.</p></li>
<li><p><strong>Continuous Integration / Continuous Deployment (CI/CD)</strong>: Deploying frequent, reliable code releases.</p></li>
<li><p><strong>Monitoring and feedback</strong>: Tracking system performance and collecting user feedback for continuous improvement.</p></li>
<li><p><strong>Security</strong>: Ensuring that appropriate security and governance practices are built into the entire development pipeline</p></li>
</ol>
<p>Some examples of popular DevOps tools include: Docker, Kubernetes, Terraform, Jenkins, GitLab CI/CD, and Prometheus.</p>
</section>
</section>
<section id="what-is-infrastructure-as-code" class="level2">
<h2 class="anchored" data-anchor-id="what-is-infrastructure-as-code">What is Infrastructure as Code?</h2>
<p><strong>Infrastructure as Code (IaC)</strong> refers to the practice of managing and provisioning computing infrastructure through machine-readable configuration files. Instead of manually setting up servers, databases, and networking configurations, IaC allows you to define these resources in code, which can then be version-controlled and automated.</p>
<p>Popular IaC tools include:</p>
<ul>
<li><p><strong>Terraform</strong> – A cloud-agnostic tool for defining infrastructure using a declarative language.</p></li>
<li><p><strong>Ansible</strong> – A configuration management tool that automates system administration tasks.</p></li>
<li><p><strong>AWS CloudFormation</strong> – A native AWS service that defines infrastructure using JSON or YAML templates.</p></li>
<li><p><strong>Pulumi</strong> – Uses general-purpose programming languages (Python, TypeScript, etc.) for infrastructure definition.</p></li>
</ul>
</section>
<section id="why-does-iac-matter" class="level2">
<h2 class="anchored" data-anchor-id="why-does-iac-matter">Why does IaC matter?</h2>
<section id="consistency-and-replicability" class="level4">
<h4 class="anchored" data-anchor-id="consistency-and-replicability">1. <strong>Consistency and replicability</strong></h4>
<p>When infrastructure is defined as code, deployments become predictable and reproducible, reducing the risk of human error.</p>
</section>
<section id="version-control-and-collaboration" class="level4">
<h4 class="anchored" data-anchor-id="version-control-and-collaboration">2. <strong>Version control and collaboration</strong></h4>
<p>With infrastructure stored in a repository (i.e.&nbsp;in GitHub or GitLab), teams can collaborate, track changes, and roll back configurations when needed.</p>
</section>
<section id="automation-and-speed" class="level4">
<h4 class="anchored" data-anchor-id="automation-and-speed">3. <strong>Automation and speed</strong></h4>
<p>IaC eliminates the need for manual setup, enabling rapid provisioning and scaling of infrastructure.</p>
</section>
<section id="cost-and-efficiency" class="level4">
<h4 class="anchored" data-anchor-id="cost-and-efficiency">4. <strong>Cost and efficiency</strong></h4>
<p>By automating infrastructure management, teams can optimize resource allocation, reduce downtime, and eliminate inefficiencies.</p>
</section>
</section>
<section id="getting-started-with-iac" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-iac">Getting started with IaC</h2>
<section id="choose-the-right-tool" class="level4">
<h4 class="anchored" data-anchor-id="choose-the-right-tool">1. <strong>Choose the right tool</strong></h4>
<p>Select an IaC tool that aligns with your current cloud provider, your existing workflows, and the expertise of your team.</p>
</section>
<section id="define-your-infrastructure" class="level4">
<h4 class="anchored" data-anchor-id="define-your-infrastructure">2. <strong>Define your infrastructure</strong></h4>
<p>Write infrastructure definitions using a <em>declarative</em> (more common) or an <em>imperative</em> approach.</p>
<ul>
<li><p><strong>Declarative (functional) approach</strong>: specify what the final state of the infrastructure should be. The IaC tool will automatically determine how to achieve that state.</p></li>
<li><p><strong>Imperative (procedural) approach</strong>: define how to provision and configure infrastructure, often in a step-by-step manner.</p></li>
</ul>
</section>
<section id="version-and-store-your-code" class="level4">
<h4 class="anchored" data-anchor-id="version-and-store-your-code">3. <strong>Version and store your code</strong></h4>
<p>Use tools like Git to version-control your infrastructure code, ensuring transparency and collaboration.</p>
</section>
<section id="test-and-validate" class="level4">
<h4 class="anchored" data-anchor-id="test-and-validate">4. <strong>Test and validate</strong></h4>
<p>Use functions built into your IaC tool of choice (e.g.&nbsp;<code>terraform plan</code> or <code>ansible --check</code>) to test your infrastructure configurations before applying them.</p>
</section>
<section id="automate-deployment" class="level4">
<h4 class="anchored" data-anchor-id="automate-deployment">5. <strong>Automate deployment</strong></h4>
<p>Integrate IaC with CI/CD pipelines to enable automated provisioning and updates.</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Infrastructure as Code has transformed the way that teams manage their infrastructure by making the process automated, scalable, and repeatable. By adopting IaC, organizations can streamline their development workflows, reduce errors, and enhance collaboration. Whether you’re new to IaC or looking to refine your current approach, embracing this paradigm can significantly improve your organization’s operational efficiency!</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references"><strong>References</strong></h2>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/DevOps">Wikipedia - DevOps</a></p></li>
<li><p><a href="https://www.reddit.com/r/devops/comments/812527/can_someone_explain_what_devops_is/">Reddit - Can someone explain what DevOps is?</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Infrastructure_as_code">Wikipedia - Infrastructure as Code</a></p></li>
<li><p><a href="https://www.redhat.com/en/topics/automation/what-is-infrastructure-as-code-iac">RedHat - What is Infrastructure as Code?</a></p></li>
<li><p><a href="https://www.ibm.com/think/topics/infrastructure-as-code">IBM - Infrastructure as Code</a></p></li>
<li><p><a href="https://www.puppet.com/blog/what-is-infrastructure-as-code">Puppet - What is Infrastructure as Code</a></p></li>
<li><p><a href="https://aws.amazon.com/what-is/iac/">AWS - What is IaC?</a></p></li>
</ul>


</section>

 ]]></description>
  <category>Overviews</category>
  <guid>https://vsriram24.github.io/posts/infrastructure-as-code/</guid>
  <pubDate>Mon, 21 Apr 2025 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/infrastructure-as-code/sisters.JPEG" medium="image"/>
</item>
<item>
  <title>Breaking the urgency trap with the Eisenhower Matrix</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/eisenhower-matrix/</link>
  <description><![CDATA[ 





<p>I recently came across an article titled “<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10159458/">The Illusion of Urgency</a>,” published in 2022 by <a href="https://wne.edu/directory/daniel-kennedy.cfm">Daniel R. Kennedy</a> and <a href="https://apps.pharmacy.wisc.edu/sopdir/andrea_porter/">Andrea L Porter</a> in the <em>American Journal of Pharmaceutical Education</em>. The publication is focused on time management for academic faculty, and I found the following paragraph to be particularly insightful:</p>
<ul>
<li><em>“One of the many rewarding aspects of an academic career is the numerous, varied responsibilities that create a stimulating job with a daily schedule that is different from one day to the next. Yet, these stimuli and responsibilities, coming from students, faculty, administrators, professional organizations, colleagues, and external stakeholders, can get overwhelming, especially as many of the requests can feel like they require an immediate response. To prosper as an academic, <strong>one must be able to identify the truly urgent issues over those that give an illusion of urgency</strong>, or else the short-term requests may interfere with the important time-intensive and productivity-driven tasks that the promotion and tenure process is primarily based upon.”</em></li>
</ul>
<p>In today’s fast-paced work environment, this statement rings true beyond the circles of academia and into many of our daily lives - being able to manage tasks efficiently is crucial for not just our productivity and success, but also our sanity!</p>
<p>In this week’s blog post, I will highlight the <strong>Eisenhower Matrix</strong>, a powerful tool for prioritizing tasks based on urgency and importance. Following the principles established in this tool can help us focus on the tasks that truly matter over those that distract us each and every day.</p>
<p><img src="https://vsriram24.github.io/posts/eisenhower-matrix/divine_dessert.jpg" class="img-fluid" width="388"></p>
<section id="a-bit-of-history" class="level2">
<h2 class="anchored" data-anchor-id="a-bit-of-history">A bit of history</h2>
<p>71 years ago, <a href="https://en.wikipedia.org/wiki/Dwight_D._Eisenhower">Dwight D. Eisenhower</a> was facing a variety of demands in his presidency, including managing the Cold War, domestic affairs, and other national issues. In spite of (or perhaps, in the face of) the breadth of his committments, Eisenhower developed a reputation for productivity and decision-making skills. In particular, Eisenhower created a new method for task prioritization that later became known as the “Eisenhower Matrix.”</p>
</section>
<section id="what-is-the-eisenhower-matrix" class="level2">
<h2 class="anchored" data-anchor-id="what-is-the-eisenhower-matrix">What is the Eisenhower Matrix?</h2>
<p>The Eisenhower Matrix is a simple framework that helps individuals and teams classify tasks into four categories, also known as “the 4 D’s”: <strong>Do</strong>, <strong>Decide</strong>, <strong>Delegate</strong>, <strong>Delete</strong>. These categories are based on two metrics: <strong>urgency</strong> and <strong>importance</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/eisenhower-matrix/The_Eisenhower_matrix_Illustration.png" class="img-fluid figure-img"></p>
<figcaption>Illustration from Spica</figcaption>
</figure>
</div>
<p>Let’s dive a little deeper into each of our quadrants.</p>
<section id="quadrant-1-do-urgent-and-important" class="level3">
<h3 class="anchored" data-anchor-id="quadrant-1-do-urgent-and-important"><strong>Quadrant 1: Do</strong> <em>(Urgent and Important)</em></h3>
<p>Tasks in this category require immediate attention and have significant consequences if not completed. Examples include:</p>
<ul>
<li><p>Meeting a critical project deadline</p></li>
<li><p>Handling a last-minute client request</p></li>
<li><p>Resolving a major issue or crisis</p></li>
</ul>
<p><strong>Strategy:</strong> Address these tasks immediately to avoid negative consequences.</p>
</section>
<section id="quadrant-2-decide-important-but-not-urgent" class="level3">
<h3 class="anchored" data-anchor-id="quadrant-2-decide-important-but-not-urgent">Quadrant 2: Decide <em>(Important but Not Urgent)</em></h3>
<p>These tasks contribute to long-term success but do not require immediate attention. Examples include:</p>
<ul>
<li><p>Strategic planning and goal setting</p></li>
<li><p>Professional development and training</p></li>
<li><p>Relationship-building and networking</p></li>
</ul>
<p><strong>Strategy:</strong> Schedule dedicated time to work on these tasks to prevent them from becoming urgent.</p>
</section>
<section id="quadrant-3-delegate-urgent-but-not-important" class="level3">
<h3 class="anchored" data-anchor-id="quadrant-3-delegate-urgent-but-not-important">Quadrant 3: Delegate <em>(Urgent but Not Important)</em></h3>
<p>Tasks that require immediate attention but do not significantly impact long-term goals belong here. Examples include:</p>
<ul>
<li><p>Responding to non-critical emails</p></li>
<li><p>Attending unnecessary meetings</p></li>
<li><p>Handling minor administrative tasks</p></li>
</ul>
<p><strong>Strategy:</strong> Delegate these tasks to others whenever possible.</p>
</section>
<section id="quadrant-4-delete-neither-urgent-nor-important" class="level3">
<h3 class="anchored" data-anchor-id="quadrant-4-delete-neither-urgent-nor-important">Quadrant 4: Delete <em>(Neither Urgent nor Important)</em></h3>
<p>These tasks provide little to no value and can often be eliminated. Examples include:</p>
<ul>
<li><p>Excessive social media browsing</p></li>
<li><p>Unnecessary paperwork</p></li>
<li><p>Low-priority busywork</p></li>
</ul>
<p><strong>Strategy:</strong> Minimize or eliminate these tasks to free up time for higher-priority work.</p>
</section>
</section>
<section id="deciding-what-is-important" class="level2">
<h2 class="anchored" data-anchor-id="deciding-what-is-important">Deciding what is important</h2>
<p>While the Eisenhower Matrix is an excellent way to organize the priority of your to-do’s, the challenge still remains to evaluate the importance of each task. Determining what truly matters to you when categorizing tasks involves understanding their impact and their alignment with your long-term goals. Here are some key ways to evaluate importance:</p>
<section id="evaluate-alignment-with-long-term-aspirations" class="level3">
<h3 class="anchored" data-anchor-id="evaluate-alignment-with-long-term-aspirations"><strong>1. Evaluate alignment with long-term aspirations</strong></h3>
<p>Ask yourself, <em>“Does this task contribute to my key objectives or long-term success?”</em> A task that directly impacts a project deadline or a business goal is important, while one that has minimal long-term effect is less so.</p>
</section>
<section id="assess-the-impact-of-completion-vs.-non-completion" class="level3">
<h3 class="anchored" data-anchor-id="assess-the-impact-of-completion-vs.-non-completion"><strong>2. Assess the impact of completion vs.&nbsp;non-completion</strong></h3>
<p>Ask yourself, <em>“What happens if this task is not done?”</em> If skipping a task leads to major consequences (missed revenue, regulatory penalties, lost customers, etc.), then it’s important.</p>
</section>
<section id="consider-the-value-addition" class="level3">
<h3 class="anchored" data-anchor-id="consider-the-value-addition"><strong>3. Consider the value addition</strong></h3>
<p>Ask yourself, <em>“Does this task create meaningful value for me, my team, or my organization?”</em> Developing a strategic report that informs future decisions is important, whereas responding to routine emails may not be.</p>
</section>
<section id="distinguish-between-importance-and-urgency" class="level3">
<h3 class="anchored" data-anchor-id="distinguish-between-importance-and-urgency"><strong>4. Distinguish between importance and urgency</strong></h3>
<p><strong>Important tasks</strong> drive meaningful results and progress. <strong>Urgent tasks</strong> require immediate attention but don’t always have long-term value. A last-minute request for a non-critical report may feel urgent but isn’t necessarily important.</p>
</section>
<section id="use-the-pareto-principle-the-8020-rule" class="level3">
<h3 class="anchored" data-anchor-id="use-the-pareto-principle-the-8020-rule"><strong>5. Use the <a href="https://en.wikipedia.org/wiki/Pareto_principle">Pareto Principle</a> (the 80/20 Rule)</strong></h3>
<p>Ask yourself, <em>“Which 20% of tasks will yield 80% of the results?”</em> Focus on high-impact activities that move the needle over busywork.</p>
</section>
<section id="identify-dependencies-and-impact-on-others" class="level3">
<h3 class="anchored" data-anchor-id="identify-dependencies-and-impact-on-others"><strong>6. Identify dependencies and impact on others</strong></h3>
<p>Ask yourself, <em>“Does this task affect others’ ability to get their work done?”</em> If delaying a task will hold up an entire team, then it’s probably important!</p>
</section>
<section id="prioritize-learning-growth-potential" class="level3">
<h3 class="anchored" data-anchor-id="prioritize-learning-growth-potential"><strong>7. Prioritize learning &amp; growth potential</strong></h3>
<p>Ask yourself, <em>“Does this task help develop skills, improve efficiency, or lead to innovation?”</em> The best investment you’ll ever be able to make in your life is <strong>in yourself</strong>. Always keep yourself in a growth mindset, and position yourself to learn more at any chance you have.</p>
</section>
</section>
<section id="productive-procrastination" class="level2">
<h2 class="anchored" data-anchor-id="productive-procrastination">Productive procrastination</h2>
<p>One of the biggest challenges in prioritization is <strong>procrastination</strong>, referring to the avoidance of difficult or uncomfortable tasks. Notably, however, it is important to remember that <strong>procrastination is often more of an emotional regulation issue than a time management problem -</strong> we often assume that procrastination happens because we feel unmotivated by the importance or urgency of the task. In reality, it becomes a way for us to avoid negative emotions including stress, anxiety, boredom, or self-doubt. If a task feels overwhelming, ambiguous, or unpleasant, we tend to avoid it not because of the task itself, but because of how it makes us feel.</p>
<p>“<strong>Productive procrastination</strong>” arises when we focus on the tasks that can be classified as not important and not urgent over other, more significant deliverables. We can use the Eisenhower Matrix to appropriately classify each of our key jobs-to-be-done, but without addressing the emotions behind each job, we will continue to fall victim to the illusion of urgency.</p>
<p>Ultimately, the tasks we have to complete are not the problem - it is our perception of these tasks! A report you put off for weeks might take just 30 minutes when you finally start, revealing that the resistance was never about the difficulty - it was about how you <em>felt</em> about starting.</p>
<p>Many people wait for motivation to strike before starting, but action itself is what creates motivation. Instead of focusing on finding the motivation for a job, focus on why the task makes you feel uncomfortable in the first place. Ask yourself: <em>“Am I avoiding this because it’s unclear? Because I’m afraid of failing? Because I find it boring?”</em> Reframing the task (e.g., “this will help me grow” instead of “this is too difficult”) can help shift your emotional response.</p>
</section>
<section id="eating-the-frog" class="level2">
<h2 class="anchored" data-anchor-id="eating-the-frog">“Eating the frog”</h2>
<p>A powerful method to combat procrastination is <strong>“eating the frog,”</strong> a concept popularized by <a href="https://www.briantracy.com/blog/time-management/the-truth-about-frogs/">Brian Tracy</a>. The idea is simple: <strong>tackle your most difficult or important task first</strong>. By addressing your most challenging task early, you reduce stress, boost productivity, and create momentum for the rest of your work.</p>
<p>How can we eat our frogs? Start by identifying your frog: determine which task feels the most challenging to you and will have the highest impact on your goals. Then, prioritize this task as the first goal in your productivity time frame, whenever your energy and focus are at its highest. If the task seems too big, divide it into smaller, more manageable steps. Make sure the time you spend on this task is distraction-free, and as you work through each step of this objective, celebrate your progress and acknowledge each and every win.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/eisenhower-matrix/eatthefrog.jpeg" class="img-fluid figure-img" width="376"></p>
<figcaption>Drawing by sketchplanations</figcaption>
</figure>
</div>
<p>By understanding the psychology of avoidance and applying strategies like “Eating the Frog,” you can effectively overcome procrastination and make the Eisenhower Matrix even more effective in your daily workflow.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The Eisenhower Matrix is a simple yet effective way to improve time management and productivity. Ultimately, by distinguishing between the urgency and importance of our tasks, we can reduce stress, work more efficiently, and focus on the things that truly matter.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><p><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10159458/">The Illusion of Urgency</a></p></li>
<li><p><a href="https://www.spica.com/blog/the-eisenhower-matrix">Spica - the Eisenhower Matrix</a></p></li>
<li><p><a href="https://www.inc.com/nick-hobson/69-years-ago-president-eisenhower-came-up-with-best-matrix-for-making-better-decisions.html">Inc - The 4 D’s of how to prioritize your time effectively</a></p></li>
<li><p><a href="https://asana.com/resources/eisenhower-matrix">Asana - the Eisenhower Matrix</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Pareto_principle">Wikipedia - Pareto principle</a></p></li>
<li><p><a href="https://www.briantracy.com/blog/time-management/the-truth-about-frogs/">Brian Tracy - Eat the Frog</a></p></li>
<li><p><a href="https://sketchplanations.com/eat-the-frog">Sketchplanations - Eat The Frog</a></p></li>
</ul>


</section>

 ]]></description>
  <category>Advice</category>
  <category>Miscellaneous</category>
  <guid>https://vsriram24.github.io/posts/eisenhower-matrix/</guid>
  <pubDate>Mon, 24 Mar 2025 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/eisenhower-matrix/zola_snow.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Bridging the gaps in biomedical data science</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/msr-scireports/</link>
  <description><![CDATA[ 





<p>Welcome back to another post from <em>[VS]Codes</em>! Today I’m excited to share another scientific publication that came out of my <a href="https://www.med.upenn.edu/gcb/">PhD in biomedical informatics</a> from the University of Pennsylvania, this time in collaboration with the <a href="https://www.microsoft.com/en-us/research/lab/microsoft-health-futures/">Health Futures</a> team at Microsoft Research (<a href="https://www.microsoft.com/en-us/research/">MSR</a>): “<a href="https://doi.org/10.1038/s41598-025-90453-x">Addressing biomedical data challenges and opportunities to inform a large-scale data lifecycle for enhanced data sharing, interoperability, analysis, and collaboration across stakeholders</a>”.</p>
<p>This paper spun out of the project I completed as a “Health AI UX Research Intern” with MSR Health Futures in the summer of 2022 under the mentorship of <a href="https://www.microsoft.com/en-us/research/people/amhal/">Mandi Hall</a> - our study explored critical challenges in the biomedical data lifecycle and offered actionable recommendations to improve data sharing, interoperability, and reproducibility across research institutions, healthcare settings, and industry partners.</p>
<p>I have always been passionate about the concept of simplifying medical discovery from <a href="https://www.cancer.gov/publications/dictionaries/cancer-terms/def/bench-to-bedside">bench to bedside</a>, so this project felt like a perfect application of my interests at a scale outside of the work that I had otherwise been pursuing in my PhD. It was a true joy to be able to bring together an interdisciplinary team of experts in AI, biomedical informatics, computational biology, and data science to better understand the pain points that researchers, clinicians, and data professionals encounter when working with complex biomedical datasets.</p>
<p>You can read the official blog post from Microsoft Research summarizing our paper <a href="https://www.microsoft.com/en-us/research/blog/advancing-biomedical-discovery-overcoming-data-challenges-in-precision-medicine/">here</a>, as well as the full text of our publication <a href="https://doi.org/10.1038/s41598-025-90453-x">here</a>. With that, let’s dive into a brief overview of the takeaways of our work.</p>
<hr>
<section id="challenges-in-the-biomedical-data-lifecycle" class="level3">
<h3 class="anchored" data-anchor-id="challenges-in-the-biomedical-data-lifecycle">Challenges in the biomedical data lifecycle</h3>
<p>Biomedical research is increasingly driven by large-scale, heterogeneous (a.k.a. ‘<a href="https://en.wikipedia.org/wiki/Multiomics">multiomic</a>’) datasets, with the goal of accelerating precision medicine and improving patient outcomes. However, in spite of numerous advances in the broader fields of machine learning and data science, researchers still face major challenges when handling biomedical data.</p>
<p>To address these barriers, our study involved in-depth, semi-structured interviews with professionals placed across the biomedical research spectrum, including bench scientists, computational biologists, clinicians, and data curators, with the goal of identifying common pain points and potential solutions. The challenges we discovered included:</p>
<ul>
<li><p>Identifying and procuring the appropriate data for a given research question</p></li>
<li><p>Curating and validating procured data for downstream analysis</p></li>
<li><p>Learning how to apply new analysis methods to validated data and navigating inconsistent computational environments</p></li>
<li><p>Distributing data-driven findings effectively and reproducibly</p></li>
<li><p>Managing the flow of data across phases of the data lifecycle</p></li>
</ul>
<p>Ultimately, across the challenges identified for biomedical discovery, participant interviews echoed a single message: <strong>the significance of collaboration and trust surrounding the flow of data</strong>. Each exchange of data involved multiple professional stakeholders, including data generators, research scientists, data curators, third-party vendors, bioinformaticians, computational biologists, biologists, and clinicians. Insight and interpretation are continually needed from all stakeholders involved to ensure the accuracy and integrity of the data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/msr-scireports/images/figure3.png" class="img-fluid figure-img"></p>
<figcaption>A depiction of the biomedical data flow, developed by our study. Figure credit: Odeline Mateu-Silvernail, MSR Health Futures</figcaption>
</figure>
</div>
</section>
<section id="key-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="key-recommendations">Key recommendations</h3>
<p>Our research highlights the need for a unified approach to the biomedical data lifecycle, from data acquisition to analysis and dissemination. Some of our core recommendations based upon identified challenges included:</p>
<ul>
<li>Creating user-friendly platforms that transition from manual data collection to electronic, structured systems.</li>
<li>Standardizing analysis workflows to improve reproducibility, incorporating version control and seamless integration of computational notebooks.</li>
<li>Leveraging AI and automation to streamline data ingestion, validation, and processing for unstructured biomedical datasets.</li>
<li>Building secure, cloud-based infrastructures to facilitate real-time collaboration among researchers across institutions.</li>
</ul>
<p>By implementing these strategies, we can work toward a more integrated, scalable, and efficient biomedical data ecosystem - one that enables faster discoveries, more reproducible research, and better patient outcomes.</p>
</section>
<section id="takeaways" class="level3">
<h3 class="anchored" data-anchor-id="takeaways">Takeaways</h3>
<p>At its core, our study underscores a fundamental truth in biomedical data science: <strong>research is only as strong as the data infrastructure supporting it</strong>. If we want to harness the full potential of multiomic data, AI-driven analysis, and precision medicine, it is essential that we break down silos, standardize workflows, and rethink how we share and collaborate on data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/msr-scireports/images/figure5.png" class="img-fluid figure-img"></p>
<figcaption>A summarizing table tasks in the biomedical data lifecycle across phases and stages of research, as identified through our study.</figcaption>
</figure>
</div>
<p>I’m incredibly grateful to my co-authors from Microsoft Research and the University of Pennsylvania for making this work possible, and I hope these findings spark further discussions on how we can improve biomedical data practices for the entire research community.</p>


</section>

 ]]></description>
  <category>Personal</category>
  <category>Overviews</category>
  <guid>https://vsriram24.github.io/posts/msr-scireports/</guid>
  <pubDate>Thu, 13 Mar 2025 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/msr-scireports/sunrise_on_the_lake.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Using network theory to explore the effect of biological sex on disease multimorbidities</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/gxs-paper/</link>
  <description><![CDATA[ 





<p>Happy 2025 and welcome to a new year of <em>[VS]Codes</em>! Today I will be providing a brief overview of the latest publication to come out of my <a href="https://www.med.upenn.edu/gcb/">PhD in biomedical informatics</a> from the University of Pennsylvania: “<a href="https://humgenomics.biomedcentral.com/articles/10.1186/s40246-024-00710-9#Sec18">The interplay of sex and genotype in disease associations: a comprehensive network analysis in the UK Biobank</a>”. This was the last project that I completed for my PhD, and it brings together multiple components <a href="https://viveksriram.com/posts/phd-papers/">that I developed over the years</a> into a single paper focused on the network-based comparison of genetically-derived cross-disease associations across biological sex. I’d like to give a huge thanks to <a href="https://www.med.upenn.edu/apps/faculty/index.php/g275/p9240045">Dokyoon Kim</a>, <a href="http://yongyeol.com">YY Ahn</a>, and <a href="https://scholar.google.com/citations?user=VWvZhigAAAAJ&amp;hl=en">Jakob Woerner</a> for their help and support getting this work published!</p>
<section id="unraveling-the-sex-specific-threads-of-disease-associations" class="level2">
<h2 class="anchored" data-anchor-id="unraveling-the-sex-specific-threads-of-disease-associations">Unraveling the Sex-Specific Threads of Disease Associations</h2>
<p><img src="https://vsriram24.github.io/posts/gxs-paper/fig2.png.webp" class="img-fluid"></p>
<p>Complex diseases rarely impact patients one-at-a-time. Shared biological, environmental, and genetic factors can all contribute to onset of multiple phenotypes in a single patient. These resulting cross-phenotype associations can impact patients with increased health burdens and risk of morbidity. Through shared genetic and biological pathways, multiple diseases form complex networks of associations. Understanding this intricate web of disease multimorbidities has long been a challenge in biomedical research.</p>
<p>Sexual dimorphism is another integral component in the study of disease pathophysiology - indeed, biological sex has been found to affect the prevalence, onset, and severity of nearly all human diseases. Without a knowledge of the impact of sex on disease, the pursuit of transformed patient care through personalized disease prediction and treatment will remain incomplete. Multiple factors contribute to sex-specific effects on disease - one influential factor is genetics. The interactions between genotype and sex on disease outcomes can be referred to as “genotype-by-sex” or “GxS” effects. In 2021, <a href="https://www.nature.com/articles/s41588-021-00912-0">Bernabeu et al.</a> conducted a sex-specific phenome-wide association study (<a href="https://pubmed.ncbi.nlm.nih.gov/34982132/">PheWAS</a>) in the UK Biobank (<a href="https://www.ukbiobank.ac.uk">UKBB</a>) and found evidence of GxS effects for a number of diseases including ankylosing spondylitis, gout, and hypothyroidism.</p>
<p>In this study, we explored the role of genotype-by-sex (GxS) interactions on the influence of cross-phenotype associations, which can ultimately impact the risk of severe health outcomes. Our findings shed light on how sex-specific genetic factors shape the landscape of disease connections, providing a fresh perspective on the importance of considering sex in disease research and precision medicine.</p>
<section id="key-findings-from-our-study" class="level3">
<h3 class="anchored" data-anchor-id="key-findings-from-our-study">Key Findings from Our Study</h3>
<p><img src="https://vsriram24.github.io/posts/gxs-paper/fig1.png.webp" class="img-fluid"></p>
<p>By constructing and comparing sex-stratified disease-disease networks (<a href="https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giac002/6528770">DDNs</a>) derived from the UKBB data generated by Bernabeu et al., we made several significant observations:</p>
<ul>
<li><p>Both male and female DDNs exhibited similar overall structures, but the significantly lower Jaccard similarity score of the networks’ edge sets and the inconsistent clustering behavior of diseases across these networks indicates the presence of GxS interactions.</p></li>
<li><p>In the male-specific DDN, there was a higher concentration of edges within circulatory diseases, while the female-specific DDN exhibited a broader spread of connections across musculoskeletal, digestive, and endocrine/metabolic diseases.</p></li>
<li><p>Females displayed a stronger overall spread of disease-variant associations, with a higher average weighted degree and shorter embedding distances in the DDN, suggesting a more expansive genetic influence on multimorbidity risk.</p></li>
<li><p>Certain diseases exhibited sex-specific centrality in the DDNs: Ischemic heart diseases and metabolic disorders were central in the male DDN, while demyelinating diseases and osteoarthritis were central in the female DDN.</p></li>
<li><p>The study also highlighted the role of sexually-dimorphic genetic variants, such as rs9357120 and rs3130552, which showed differential associations with metabolic disorders in males and autoimmune disorders in females.</p></li>
</ul>
</section>
<section id="implications-for-precision-medicine" class="level3">
<h3 class="anchored" data-anchor-id="implications-for-precision-medicine">Implications for Precision Medicine</h3>
<p>Our findings underscore the necessity of incorporating sex as a fundamental variable in biomedical research. By understanding the unique ways in which diseases interconnect for males and females, we can better predict disease progression, tailor treatments, and ultimately improve health outcomes for both sexes.</p>
<p>Our study also identifies future research directions, including the comparison of sex-specific DDNs across different populations, the application of more advanced graph representation learning methods, and the investigation of individual genetic variants contributing to sex-specific disease associations.</p>
</section>
<section id="dive-deeper" class="level3">
<h3 class="anchored" data-anchor-id="dive-deeper">Dive Deeper</h3>
<p>For those interested in the intricate details of our methodology and findings, I invite you to read our full paper, published in <em>Human Genomics</em>, <a href="https://doi.org/10.1186/s40246-024-00710-9">here</a>. We provide more insight into the network-based methods used to compare our graphs, as well as a deeper discussion of the biological context underlying the takeaways of our analysis.</p>


</section>
</section>

 ]]></description>
  <category>Personal</category>
  <category>Overviews</category>
  <guid>https://vsriram24.github.io/posts/gxs-paper/</guid>
  <pubDate>Mon, 20 Jan 2025 08:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/gxs-paper/zola_sunrise.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Stanford AI + Health Conference 2024</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/stanford_health_ai_conf_24/</link>
  <description><![CDATA[ 





<p>Welcome back to another week of <em>[VS]Codes</em>! Last week, I had the opportunity to attend the fourth annual <a href="https://web.cvent.com/event/7a9f234c-3a15-445c-8ffb-0b1c719f56b3/summary">Stanford AI + Health conference</a>, a virtual conference hosted by the <a href="https://aimi.stanford.edu/">Stanford Center for Artificial Intelligence in Medicine and Imaging (AIMI)</a>, exploring real-world AI applications across healthcare. I very much enjoyed hearing perspectives on integrating AI into healthcare systems from a variety of panelists from several different organizations (and I would highly recommend anyone interested in these topics attend their conference next year!). This blog post summarizes my notes from each of the panels that I attended and offers some of my biggest overall takeaways.</p>
<p><img src="https://vsriram24.github.io/posts/stanford_health_ai_conf_24/images/banner.png" class="img-fluid"></p>
<hr>
<section id="innovating-at-the-speed-of-trust" class="level3">
<h3 class="anchored" data-anchor-id="innovating-at-the-speed-of-trust">1. Innovating at the Speed of Trust</h3>
<ul>
<li>Daniel Yang, MD - VP of AI and Emerging Technologies, Kaiser Permanente</li>
</ul>
<p>The opening keynote of the conference presented by Dr.&nbsp;Daniel Yang centered on the importance of trust in driving innovation in the field of health AI. Indeed, introducing AI to the clinic requires full involvement from the clinician. Clinicians need to understand how the AI systems being implemented work so that they can decide when such systems can be trusted and when they might lead us astray. <em>Without trust, even the most advanced AI systems will be unable fulfill their full potential.</em></p>
<p><strong>The need for AI in healthcare today</strong><br>
Today, there is an ever-growing mismatch between healthcare supply and demand. We face an aging population, as well as a new patient focus on convenience and a desire for hybrid approaches to healthcare. At the same time, the healthcare workforce is being stretched thin due to a shortage of workers and a bottleneck in the training time needed to produce new clinical experts. The consequences of this mismatch include rising healthcare costs, physician burnout, and limited access to treatment. AI has the potential to accelerate our ability to provide more healthcare to more people in need.</p>
<p><strong>Building a Framework for Trust</strong><br>
The speed of progress in health AI will ultimately depend on the speed of trust. Any missteps, such as patient harm or tools that fall short of expectations, will slow innovation considerably. A lot of time has been spent in developing a technology stack for AI to enhance its performance - key components of this “AI Tech Stack” include AI applications, cloud platforms, foundation models, and computing hardware. At the same time, the acceleration of AI and the reduction of the trust gap requires us to consider another missing stack - the “Human Stack,” made up of people, priorities, processes, and policies.</p>
<ul>
<li><p><em>People</em>: 60% of Americans are uneasy about their providers using AI. Building trust will require rigorous trials, clinical evidence, and robust safety protocols.</p></li>
<li><p><em>Priorities</em>: Focusing on “lighthouse use cases” help demonstrate AI’s value while targeting the areas of greatest impact.</p></li>
<li><p><em>Processes</em>: Seamless integration of models into existing practiced workflows is vital.</p></li>
<li><p><em>Policies</em>: Equitable adoption will require financial and policy frameworks to prevent an “AI divide” between well-resourced and underserved healthcare systems.</p></li>
</ul>
<p><strong>Financial Incentives for Responsible AI</strong><br>
A critical question raised during the discussion was how to incentivize responsible AI. Currently, the burden of risk and implementation often falls on healthcare delivery systems themselves, posing challenges for smaller organizations with fewer resources. Policymakers could draw lessons from the 2009 HITECH Act, which spurred electronic health record (EHR) adoption by ensuring minimum standards and equitable access.</p>
<p>Dr.&nbsp;Yang’s talk concluded that the key to unlocking AI’s transformative potential lies not only in advancing technology but in earning the trust of both patients and providers through safety, evidence, and equitable frameworks.</p>
</section>
<section id="building-organizational-frameworks-for-sustainable-ai-programs" class="level3">
<h3 class="anchored" data-anchor-id="building-organizational-frameworks-for-sustainable-ai-programs">2. Building Organizational Frameworks for Sustainable AI Programs</h3>
<ul>
<li><p>Kimberly McManus, MS PhD - Deputy Chief AI Officer, Department of Veterans Affairs</p></li>
<li><p>Nikesh Kotecha - Head of Data Science, Stanford Health Care</p></li>
<li><p>Mark Sendak, MD MPP - Population Health and Data Science Lead, Duke University</p></li>
<li><p>Sara Vaezy - EVP and Chief Strategy and Digital Officer, Providence Health</p></li>
</ul>
<p>Establishing sustainable AI programs in healthcare requires robust frameworks that integrate governance, infrastructure, and workforce development while prioritizing responsible implementation and patient-centric use cases. Leaders from institutions including the VA, Stanford, Duke, and Providence Health shared their approaches to scaling AI effectively.</p>
<p><strong>Key Components of Sustainable AI Frameworks</strong></p>
<ul>
<li><p><strong>Governance and Guardrails</strong></p>
<ul>
<li><p><strong>VA’s Hub-and-Spoke Governance Model</strong>: Balances local accountability with innovation under centralized oversight, including an AI Governance Council addressing ethics, privacy, and cybersecurity.</p></li>
<li><p>Public transparency is prioritized through inventories like the VA’s <a href="https://www.research.va.gov/naii/ai-inventory.cfm">Public AI Use Case Inventory</a> and its established minimum requirements for sensitive AI, such as real-world impact assessments and monitoring discrimination risks.</p></li>
<li><p>Providence Health emphasized transparency and patient/community involvement in AI initiatives to build trust and measure progress.</p></li>
</ul></li>
<li><p><strong>Infrastructure and Technology</strong></p>
<ul>
<li><p>Institutions like the VA and Stanford Medicine highlighted the need for modular and flexible data pipelines to support AI development and deployment. Examples include the VA’s Enterprise Data Platform (SUMMIT) and Stanford’s interoperable data lake connected to large language models.</p></li>
<li><p>Duke advocated for “π-shaped” rather than “T-shaped” expertise (personnel skilled in across multiple domains) to drive innovation while addressing infrastructure needs.</p></li>
</ul></li>
<li><p><strong>Workforce Development and Leadership</strong></p>
<ul>
<li><p>The VA focused on communication, training, and hiring to grow its AI workforce, while Stanford integrated leadership commitment with frameworks like the Responsible AI Lifecycle to guide operations and governance.</p></li>
<li><p>Duke emphasized forward-looking leadership and embedding expertise across domains to align project selection with innovation goals.</p></li>
</ul></li>
</ul>
<p><strong>Prioritizing Use Cases for Maximum Impact</strong></p>
<ul>
<li><p><strong>VA Priority Use Cases</strong>: Focus on decreasing clinician burnout, improving care quality, and leveraging AI for risk prediction, patient feedback, and medical devices. Pilots include AI scribes and generative AI for documentation.</p>
<ul>
<li>E.g. <a href="https://academic.oup.com/jamia/article/31/3/727/7499277">Deploying a national clinical text processing infrastructure</a></li>
</ul></li>
<li><p><strong>Stanford Medicine</strong>: Targets efficiency with solutions like reducing physician workload, streamlining clinical documentation, and using predictive models.</p></li>
<li><p><strong>Duke’s Domains of Innovation</strong>: Include health delivery science, technology innovation, and workforce development, emphasizing transformational breakthroughs that prepare organizations for future challenges.</p></li>
</ul>
<p><strong>Embedding Responsibility in AI Deployment</strong></p>
<ul>
<li><p>Effective frameworks ensure accountability across every stage of the AI lifecycle—from model development to operational monitoring.</p></li>
<li><p>All institutions stressed the importance of designing workflows around AI tools and defining clear roles for accountability.</p></li>
</ul>
<p><strong>Incorporating Patient and Community Voices</strong></p>
<ul>
<li><p>Patient and family advisory councils, focus groups, and community engagement initiatives are key to identifying pain points and building trust.</p></li>
<li><p>Transparency about AI usage in clinical care fosters alignment and trust between institutions and the communities they serve.</p></li>
</ul>
<p><strong>Guiding Principles for Sustainable AI</strong></p>
<ol type="1">
<li><p>Build responsibly by addressing systemic challenges, including the AI divide between resource-rich and under-served settings.</p></li>
<li><p>Support innovation with flexible, scalable infrastructure to ensure post-market success and sustainable impact.</p></li>
<li><p>Prioritize patient and clinician needs over adopting AI for its own sake, aligning initiatives with tangible benefits.</p></li>
</ol>
<p>By embedding governance, infrastructure, and patient engagement into AI programs, these organizations demonstrate how healthcare systems can scale AI sustainably while fostering trust and equity.</p>
</section>
<section id="evaluating-and-deploying-ai-tools-the-furm-model" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-and-deploying-ai-tools-the-furm-model">3. Evaluating and Deploying AI Tools: <a href="https://shahlab.stanford.edu/lib/exe/fetch.php?media=furm_paper_-_main_appendix_submitted_to_arxiv.pdf">The FURM Model</a></h3>
<ul>
<li><p>Dev Dash, MD, MPH - Clinical Assistant Professor, Stanford University</p></li>
<li><p>Lance Downing, MD - Clinical Assistant Professor, Stanford University</p></li>
<li><p>Sneha Jain, MD MBA - Clinical Assistant Professor, Stanford University</p></li>
<li><p>Timothy Keyes, PhD - Data Scientist, Stanford Health Care</p></li>
</ul>
<p>Deploying AI tools in healthcare involves more than just developing high-performing models - it requires a comprehensive framework to assess their fairness, usefulness, reliability, and alignment with real-world constraints. Stanford’s FURM (Fairness, Usefulness, Reliability, and Monitoring) model provides a structured approach to evaluating and integrating AI tools into clinical workflows responsibly.</p>
<p><strong>The FURM Assessment Framework</strong></p>
<ol type="1">
<li><p><strong>Stage 1: Pre-Deployment (What and Why)</strong></p>
<ul>
<li><p>Define the problem and assess the need for an AI solution.</p></li>
<li><p>Simulate workflow integration to estimate usefulness and financial impact.</p></li>
<li><p>Consider ethical implications and potential risks before proceeding.</p></li>
</ul></li>
<li><p><strong>Stage 2: Deployment (How)</strong></p>
<ul>
<li><p>Develop, test, and integrate the model into existing infrastructure and organizational workflows.</p></li>
<li><p>Ensure alignment with operational policies and stakeholder capacities.</p></li>
</ul></li>
<li><p><strong>Stage 3: Post-Deployment (Impact)</strong></p>
<ul>
<li>Conduct prospective evaluations and ongoing monitoring to assess outcomes and address emerging issues.</li>
</ul></li>
</ol>
<p><strong>Key Considerations for Deploying AI in Healthcare</strong></p>
<ul>
<li><p><strong>Identifying the “Job to Be Done” (JTBD)</strong><br>
AI tools must address specific operational or clinical needs. Stakeholders adopt tools not for their novelty but to achieve tangible progress. Defining these JTBD upfront ensures that AI solutions align with practical goals.</p></li>
<li><p><strong>AI-Guided Workflows</strong><br>
AI models operate within complex systems constrained by policies, capacities, and real-world dynamics. For example:</p>
<ul>
<li><p>AI for detecting undiagnosed Peripheral Artery Disease (PAD) was constrained by reimbursement policies, leading to targeted testing only for symptomatic patients.</p></li>
<li><p>LLMs for nursing end-of-shift reports revealed cultural and operational challenges, such as balancing template flexibility with nurse preferences.</p></li>
</ul></li>
<li><p><strong>Analyzing Usefulness</strong></p>
<ul>
<li><p>Stanford’s <a href="https://www.sciencedirect.com/science/article/pii/S1532046423000400?ref=pdf_download&amp;fr=RR-2&amp;rr=8f1908fbde7f756d">APLUS</a> Python library evaluates AI models within realistic, capacity-constrained workflows.</p></li>
<li><p>Correct and incorrect decisions are weighed according to their differing impacts on patients and healthcare systems.</p></li>
</ul></li>
<li><p><strong>Cost and Operational Implications</strong></p>
<ul>
<li><p>Deployment decisions must consider the cost of building and sustaining AI workflows, as well as downstream financial and clinical impacts.</p></li>
<li><p>Case studies highlighted how savings in time don’t always translate to financial benefits or improved care outcomes.</p></li>
</ul></li>
</ul>
<p><strong>Addressing Ethical Issues in Healthcare AI</strong></p>
<ul>
<li><p><strong>Stakeholder Alignment</strong></p>
<ul>
<li>Ethical concerns arise from misaligned values among clinicians, developers, and patients. Organizations must mediate these differences through intake interviews, stakeholder analysis, and actionable reports. For instance, skepticism about LLMs improving documentation efficiency underscores the need for transparent communication and measurable results.</li>
</ul></li>
<li><p><strong>Broader Ethical Vision</strong></p>
<ul>
<li>Ethical recommendations must translate into concrete actions to mitigate risks. Stanford is developing a “playbook” to guide other healthcare systems in addressing ethical concerns effectively.</li>
</ul></li>
</ul>
<p><strong>Realizing Value Beyond Models</strong><br>
Successful AI deployment is not just about the AI model but also the policies, workflows, and actions that surround it. By combining rigorous evaluations with thoughtful integration, the FURM framework ensures AI tools are not only effective but also equitable, ethical, and aligned with healthcare priorities.</p>
</section>
<section id="data-governance-and-quality-assurance-in-healthcare-ai" class="level3">
<h3 class="anchored" data-anchor-id="data-governance-and-quality-assurance-in-healthcare-ai">4. Data Governance and Quality Assurance in Healthcare AI</h3>
<ul>
<li><p>Leo Anthony Celi, MD MS MPH - Senior Research Scientist and Associate Professor of Medicine, MIT and Harvard Medical School</p></li>
<li><p>Cora Han, JD - Chief Health Data Officer, University of California Health</p></li>
<li><p>Michael Pfeffer, MD FACP - Chief Information Officer, Stanford Health Care and Stanford School of Medicine</p></li>
<li><p>Aditya Sharma - Senior Manager, TDS Data Science Innovation at Stanford</p></li>
</ul>
<p>Ensuring high-quality, secure, and reliable data is foundational to the success of AI in healthcare. However, the complexities of the healthcare data ecosystem demand innovative approaches to governance, interoperability, and quality assurance.</p>
<p><strong>The Reality of Healthcare Data</strong><br>
Healthcare data is not a pristine, static resource like a “lake in the Alps,” but rather a dynamic, often chaotic “deluge” of multi-modal, ever-evolving information. Addressing this fluidity is key to effective data governance.</p>
<p><strong>Key Challenges in Healthcare Data</strong></p>
<ol type="1">
<li><p><strong>Interoperability and Data Drift</strong></p>
<ul>
<li><p><strong>Data Shift</strong>: Differences between training data and real-world inference data result in mismatched populations and ontologies.</p></li>
<li><p><strong>Data Normalization</strong>: Health code definitions evolve, creating discrepancies between static model labels and changing clinical realities.</p></li>
<li><p><strong>Multimodality</strong>: The healthcare data landscape spans diverse modalities, complicating integration and analysis.</p></li>
<li><p><strong>Data Availability</strong>: Time stamps in electronic health records (EHRs) often fail to reflect real-world conditions, reducing data reliability.</p></li>
</ul></li>
<li><p><strong>Workflow Design and Transferability</strong></p>
<ul>
<li><p>AI performance depends on the workflows and data structures underlying it, which may not transfer effectively between institutions.</p></li>
<li><p>The ability to modify workflows and data structures introduces variability that impacts both model and operational performance.</p></li>
</ul></li>
<li><p><strong>Lifecycle Management</strong></p>
<ul>
<li><p>Managing data drift in externally created models is crucial to ensuring consistent performance.</p></li>
<li><p>Deciding when and how to retrain or deprecate models requires timely and sufficient data collection, which is often challenging.</p></li>
</ul></li>
</ol>
<p><strong>Social Dynamics in Data Generation</strong><br>
Healthcare data is generated within a socially patterned and non-neutral space, reflecting biases and inequities. While AI has the potential to disrupt the status quo, its design and deployment must be deliberate to avoid perpetuating existing disparities.</p>
<p><strong>Standards and Frameworks</strong><br>
The <a href="https://www.nist.gov/itl/ai-risk-management-framework"><strong>NIST AI Risk Management Framework</strong></a> provides a structured approach to addressing these challenges, emphasizing the importance of governance, transparency, and quality assurance in healthcare AI systems.</p>
<p><strong>Workforce Limitations</strong><br>
Healthcare systems face unique constraints, including difficulties in recruiting data scientists compared to tech companies. This underscores the need for innovative strategies to build internal capacity and ensure robust governance.</p>
<p><strong>Path Forward</strong><br>
AI in healthcare must be built on a foundation of rigorous data governance, ensuring not only technical excellence but also ethical and equitable practices. By addressing the fluid and complex nature of healthcare data, organizations can create AI systems that are reliable, transferable, and transformative.</p>
</section>
<section id="large-language-models-llms-in-healthcare" class="level3">
<h3 class="anchored" data-anchor-id="large-language-models-llms-in-healthcare">5. Large Language Models (LLMs) in Healthcare</h3>
<ul>
<li>Nigam Shah, Professor of Medicine and Biomedical Data Science, Stanford University</li>
</ul>
<p>Large Language Models (LLMs) represent a transformative opportunity for healthcare, leveraging vast amounts of data to generate predictions, insights, and text-based interactions. However, their effective application in clinical settings requires balancing innovation with responsible deployment.</p>
<p><strong>Understanding LLMs</strong><br>
LLMs are trained by masking portions of input data (e.g., text) and using a Transformer model to predict the missing elements. The result is a numeric embedding - a representation of the input data that can be used for downstream tasks, including:</p>
<ul>
<li><p><strong>Classifier Features</strong>: Supporting machine learning models for prediction.</p></li>
<li><p><strong>Text Output</strong>: Facilitating human-AI interaction.</p></li>
</ul>
<p>LLMs are a subset of <em>foundation models</em>, which generalize across symbol vocabularies (e.g., sounds, pixels, or ICD codes). For example:</p>
<ul>
<li>Predicting the likelihood of certain ICD codes over a timeline rather than generating the next word in text.</li>
</ul>
<p><strong>Two Key Approaches in Healthcare AI</strong></p>
<ol type="1">
<li><p><strong>Clinical Language Models (CLaMs)</strong></p>
<ul>
<li><p>Focus on textual outputs and generative tasks.</p></li>
<li><p>Example: Summarizing clinical notes or generating predictions from text representations.</p></li>
</ul></li>
<li><p><strong>Foundation Models for Electronic Medical Records (FEMRs)</strong></p>
<ul>
<li><p>Focus on patient timelines and numerical embeddings for prediction tasks.</p></li>
<li><p>Example: Predicting patient outcomes by analyzing sequences of events in their medical history.</p></li>
</ul></li>
</ol>
<p><strong>Shared EHR Structures Enable Transferability</strong><br>
EHRs exhibit consistent patterns that allow models to transfer learned embeddings to new tasks. This shared structure is a key advantage of foundation models in healthcare.</p>
<p><strong>Challenges and Priorities</strong><br>
While foundation models and LLMs hold great promise, the healthcare community faces challenges in scaling their deployment:</p>
<ul>
<li><p><strong>Verification of Benefits</strong>: There are too many models being developed and not enough focus on ensuring their utility in real-world settings.</p></li>
<li><p><strong>Responsible Innovation</strong>: As these models are integrated into health systems, balancing innovation with ethical and operational considerations is crucial.</p></li>
</ul>
<p>LLMs and foundation models are powerful tools for advancing healthcare AI, but their deployment must prioritize patient outcomes, system integration, and ethical use.</p>
</section>
<section id="generative-ai-in-healthcare" class="level3">
<h3 class="anchored" data-anchor-id="generative-ai-in-healthcare">6. Generative AI in Healthcare</h3>
<ul>
<li><p>Jason Fries, Stanford University</p></li>
<li><p>Kati Link, Hugging Face</p></li>
<li><p>David Magnus, Stanford University</p></li>
<li><p>Ron Li, Stanford University</p></li>
</ul>
<p>Generative AI offers transformative potential for healthcare, expanding the axes of evaluation beyond traditional measures. By leveraging foundation models and emerging datasets, generative AI aims to enhance clinical decision-making, streamline workflows, and introduce novel human-AI interactions.</p>
<p><strong>Expanding Axes of Evaluation</strong><br>
Generative AI enriches evaluation metrics, focusing on:</p>
<ul>
<li><p><strong>Improved Accuracy</strong>: Achieving better predictive performance.</p></li>
<li><p><strong>Reduced Need for Labeled Data</strong>: Leveraging fewer labeled examples through transfer learning.</p></li>
<li><p><strong>Simplified Deployment</strong>: Facilitating integration within hospital systems.</p></li>
<li><p><strong>Multimodal Capabilities</strong>: Combining text, imaging, and other data types.</p></li>
<li><p><strong>Emergent Applications</strong>: Discovering novel use cases.</p></li>
<li><p><strong>Human-AI Interfaces</strong>: Redefining how humans and machines collaborate.</p></li>
</ul>
<p><strong>Transfer Learning in Practice</strong><br>
Foundation models capitalize on the shared structure of healthcare data, adapting to smaller, specific use cases through techniques like <em>transfer learning</em>. An example is <a href="https://som-shahlab.github.io/ehrshot-website/">EHRSHOT</a>, a benchmark for few-shot evaluation of foundation models in EHRs.</p>
<p><strong>Multimodal Datasets and Evaluation</strong><br>
Datasets such as <a href="https://aimi.stanford.edu/shared-datasets">IMPACT</a>, a multimodal dataset for predicting pulmonary embolism outcomes, demonstrate the shift towards richer, more diverse evaluation frameworks. Moving beyond datasets like <a href="https://www.nature.com/articles/s41597-022-01899-x">MIMIC</a>, these efforts push AI systems to operate effectively across real-world scenarios.</p>
<p><strong>Two Worlds of Evaluation</strong><br>
Generative AI in healthcare spans two primary domains:</p>
<ol type="1">
<li><p><strong>Natural Language Models (CLaMs)</strong></p>
<ul>
<li><p><strong>Information Extraction</strong>: Pulling structured insights from unstructured text.</p></li>
<li><p><strong>Assistive Writing</strong>: Drafting responses to patient messages.</p></li>
<li><p><strong>Question Answering</strong>: Providing clinical insights or recommendations.</p></li>
</ul></li>
<li><p><strong>Medical Codes and Timelines (FEMRs)</strong></p>
<ul>
<li><p><strong>Risk Stratification</strong>: Identifying high-risk patients.</p></li>
<li><p><strong>Time-to-Event Models</strong>: Predicting clinical outcomes over time.</p></li>
<li><p><strong>Phenotyping</strong>: Identifying patient subgroups based on shared characteristics.</p></li>
</ul></li>
</ol>
<p><strong>Building Better Evaluation Tasks</strong><br>
There is a pressing need for new benchmarks that assess how AI models improve human decision-making and outcomes when humans are part of the workflow. Evaluation must extend beyond algorithms to the broader impact of AI systems in practice.</p>
<p><strong>Linguistics and Generative AI</strong><br>
The importance of <em>semantics</em> and <em>context</em> in medicine underscores the value of generative AI in addressing both discriminative and generative tasks, bridging gaps in communication and understanding within healthcare.</p>
<p>Generative AI continues to evolve, with organizations like HuggingFace driving democratization and innovation. By enriching evaluation criteria and focusing on meaningful human-AI collaboration, generative AI is poised to redefine healthcare delivery.</p>
</section>
<section id="building-in-house-ai-solutions-in-healthcare" class="level3">
<h3 class="anchored" data-anchor-id="building-in-house-ai-solutions-in-healthcare">7. Building In-House AI Solutions in Healthcare</h3>
<ul>
<li><p>Oliver Aalami, MD - Clinical Professor in Vascular Surgery, Stanford</p></li>
<li><p>Toyin Falola, MD - AVP of Clinical Product Strategy, Providence Health</p></li>
<li><p>Ron Li, MD - Medical Informatics Director for Digital Health, Stanford Health Care</p></li>
<li><p>Ashwin Nayak, MD MS - Clinical Assistant Professor of Medicine, Stanford University</p></li>
</ul>
<p>Developing AI solutions in healthcare requires a holistic approach that goes beyond model development to address user experience, workflows, and care models. By integrating AI into broader systems of care delivery, organizations can foster both innovation and meaningful impact.</p>
<p><strong>AI Beyond Models</strong><br>
The innovation space in healthcare extends far beyond creating better AI models. True transformation happens when AI:</p>
<ul>
<li><p>Delivers actionable information paired with intuitive user experiences.</p></li>
<li><p>Integrates into care models that encompass people, processes, and technologies.</p></li>
<li><p>Focuses not just on building better models, but also on enhancing user interfaces, streamlining processes, and reimagining care delivery systems.</p></li>
</ul>
<p><strong>Two-Speed System for Innovation</strong><br>
Healthcare innovation can follow two complementary tracks:</p>
<ol type="1">
<li><p><strong>Enterprise Care Delivery Transformation</strong></p>
<ul>
<li><p>Focuses on deploying scalable solutions across the system.</p></li>
<li><p>Leverages strong clinician engagement to drive adoption.</p></li>
</ul></li>
<li><p><strong>Grassroots Innovation</strong></p>
<ul>
<li><p>Emphasizes rapid prototyping and iteration.</p></li>
<li><p>Leverages clinician feedback to refine solutions and address specific care delivery challenges.</p></li>
</ul></li>
</ol>
<p><strong>Stakeholder Commitment</strong><br>
The success of healthcare AI depends on deep commitment from all stakeholders. The “chicken and the pig” parable highlights the importance of active engagement and ownership across teams, rather than passive involvement.</p>
<p><strong>Health System Product Development Principles</strong><br>
To build effective in-house AI tools, healthcare organizations should:</p>
<ul>
<li><p><strong>Identify the Problem</strong>: Clearly define the issue to solve.</p></li>
<li><p><strong>Explore All Options</strong>: Consider a wide range of approaches.</p></li>
<li><p><strong>Focus on Use-Cases and Users</strong>: Center development around real-world applications and user needs.</p></li>
<li><p><strong>Secure Executive Sponsorship</strong>: Obtain leadership buy-in to ensure resources and alignment.</p></li>
<li><p><strong>Assemble Multi-Disciplinary Teams</strong>: Bring together diverse expertise.</p></li>
<li><p><strong>Obsess Over ROI and KPIs</strong>: Keep a sharp focus on measurable outcomes.</p></li>
<li><p><strong>Engage Stakeholders</strong>: Build support across the organization.</p></li>
<li><p><strong>Align with System-Wide Goals</strong>: Ensure AI initiatives complement broader priorities.</p></li>
</ul>
<p><strong>Privacy and Cost Innovations</strong><br>
Concepts like <em>fog computing</em> (privacy-preserving, low-cost distributed computing) offer promising solutions for AI deployment in healthcare, addressing concerns around data security and cost.</p>
<p><strong>Fostering a Culture of Innovation</strong><br>
To align strategic priorities while encouraging creativity:</p>
<ul>
<li><p>Use idea canvases and templates to structure brainstorming.</p></li>
<li><p>Apply design thinking to keep efforts human-centered.</p></li>
<li><p>Adopt shared technology tools to facilitate collaboration.</p></li>
<li><p>Focus on solutions that directly address the specific problems at hand.</p></li>
</ul>
<p>By taking a user-centered, strategic, and multidisciplinary approach, healthcare organizations can build in-house AI solutions that are impactful, scalable, and aligned with their mission.</p>
</section>
<section id="shaping-the-future-of-responsible-ai-in-healthcare" class="level3">
<h3 class="anchored" data-anchor-id="shaping-the-future-of-responsible-ai-in-healthcare">8. Shaping the Future of Responsible AI in Healthcare</h3>
<ul>
<li><p>David Magnus, PhD - Thomas Raffin Professor of Medicine and Biomedical Ethics, Stanford University</p></li>
<li><p>Daryl Oakes, PhD - Associate Dean of Graduate Medical Education and Clinical Professor of Anesthesiology, Stanford University</p></li>
<li><p>Ziad Obermeyer, MD - University of Berkeley</p></li>
<li><p>David Rhew, MD - Global Chief Medical Officer and VP Healthcare, Microsoft</p></li>
</ul>
<p>The future of AI in healthcare lies in its ability to act as an ally in improving care delivery, rather than merely a replacement for existing processes. By addressing challenges in deployment, ensuring transparency, and prioritizing patient outcomes, AI can transform healthcare responsibly and equitably.</p>
<p><strong>Federated Learning and Data Standardization</strong><br>
Federated learning offers a promising path for collaborative model training without sharing sensitive data. However, its success hinges on overcoming barriers such as data standardization and interoperability. Tools like those offered by <a href="https://aimi.stanford.edu/shared-datasets">AIMI’s shared datasets</a> aim to advance these efforts by providing accessible resources for model development.</p>
<p><strong>Four Pivotal Themes</strong></p>
<ol type="1">
<li><p><strong>Integration Over Innovation Alone</strong><br>
Success in healthcare AI depends not just on creating the best algorithms but embedding them into workflows that support, rather than replace, clinicians.</p></li>
<li><p><strong>Deployment in Real-World Settings</strong><br>
Moving AI from research to practice requires addressing complex challenges across hospitals, clinics, and outpatient centers, where the focus is on maintaining high-quality care.</p></li>
<li><p><strong>Training, Transparency, and Trust</strong><br>
To foster adoption, prioritize clear communication about how AI systems work, offer robust clinician training, and ensure user-centered design.</p></li>
<li><p><strong>Equity and Ethical Data Use</strong><br>
Work with representative, accurate, and ethically sourced data to build equitable AI systems that benefit all patients.</p></li>
</ol>
<p><strong>Challenges and Priorities</strong><br>
Deployment in healthcare settings often encounters hurdles such as:</p>
<ul>
<li><p><strong>Data Interoperability</strong>: Bridging gaps between systems and standards.</p></li>
<li><p><strong>Clinician Burnout</strong>: Avoiding tools that add to administrative burdens.</p></li>
<li><p><strong>Trust and Transparency</strong>: Ensuring users understand and can rely on AI outputs.</p></li>
</ul>
<p><strong>A Commitment to Patients</strong><br>
Ultimately, responsible AI in healthcare demands equity, transparency, collaboration, and a steadfast focus on improving patients’ lives. By aligning technology with clinical needs and ethical principles, healthcare systems can shape a future where AI enhances care quality, efficiency, and accessibility.</p>
</section>
<section id="genai-and-llms-in-healthcare-potential-and-risks" class="level3">
<h3 class="anchored" data-anchor-id="genai-and-llms-in-healthcare-potential-and-risks">9. GenAI and LLMs in Healthcare: Potential and Risks</h3>
<ul>
<li>Roxana Daneshjou, MD PhD - Stanford University</li>
</ul>
<p>The integration of Generative AI (GenAI) and Large Language Models (LLMs) into healthcare represents both groundbreaking potential and significant risks. Understanding their underlying mechanics, evaluating their performance, and addressing safety and bias concerns are critical steps to ensure responsible adoption.</p>
<p><strong>How LLMs Work</strong></p>
<ul>
<li><p><strong>Transformers</strong>&nbsp;revolutionize AI by enabling models to focus on contextually distant parts of text, enhancing comprehension and prediction.</p></li>
<li><p><strong>Training Process</strong>:</p>
<ol type="1">
<li><p><strong>Pre-training</strong>: Models learn patterns from large datasets (e.g., the internet, textbooks) using self-supervised learning by predicting masked words without human involvement.</p></li>
<li><p><strong>Fine-tuning</strong>: Refines models with reinforcement learning and human feedback to improve output quality.</p></li>
<li><p><strong>Prompting</strong>: Adjusts model responses without modifying the model itself, using techniques like Chain-of-Thought prompting to guide reasoning.</p></li>
</ol></li>
</ul>
<p><strong>Challenges in LLM Development</strong></p>
<ul>
<li><p><strong>Rapidly Evolving Landscape</strong>: Surveys of LLMs become outdated quickly due to constant advancements.</p></li>
<li><p><strong>Ethical Concerns</strong>:</p>
<ul>
<li><p>Risk of perpetuating false race-based medical practices (e.g., <a href="https://www.nature.com/articles/s41746-023-00939-z">Omiye et al., <em>npj Digital Medicine</em>, 2023</a>).</p></li>
<li><p>Bias and safety issues uncovered through “red-teaming,” where vulnerabilities like factual inaccuracies and inappropriate responses are tested in simulated clinical settings.</p></li>
</ul></li>
</ul>
<p><strong>Vision-Language Models (VLMs)</strong></p>
<ul>
<li><p>Utilize contrastive learning to link text and images for tasks such as zero-shot predictions.</p></li>
<li><p>While promising, VLMs are not yet reliable for clinical application.</p></li>
</ul>
<p><strong>Key Takeaways</strong></p>
<ul>
<li><p><strong>Emerging Impact</strong>: GenAI is beginning to influence healthcare, offering tools to improve clinical workflows and patient care.</p></li>
<li><p><strong>Potential Benefits</strong>: Improved diagnosis, decision support, and efficiency in medical settings.</p></li>
<li><p><strong>Caution Required</strong>: VLMs and other GenAI tools remain in development, requiring careful evaluation, ethical considerations, and stakeholder collaboration before widespread deployment.</p></li>
</ul>
<p>To responsibly leverage GenAI, healthcare must prioritize safety, transparency, and equity, ensuring these technologies enhance care without introducing new risks.</p>
</section>
<section id="section" class="level3">
<h3 class="anchored" data-anchor-id="section"></h3>
</section>
<section id="scaling-ai-and-overcoming-deployment-challenges" class="level3">
<h3 class="anchored" data-anchor-id="scaling-ai-and-overcoming-deployment-challenges">10. Scaling AI and Overcoming Deployment Challenges</h3>
<ul>
<li><p>Angela Aristidou - University College London and Stanford University</p></li>
<li><p>Julia Adler-Milstein, PhD - Professor of Medicine and Chief of the Division of Clinical Informatics and Digital Transformation, UCSF</p></li>
<li><p>Jonathan Hron, MD - Associate Chief Medical Information Officer, Boston Children’s Hospital</p></li>
<li><p>Vincent Liu, MD MS - Northern California Division of Research and Chief Data Officer, Kaiser Permanente</p></li>
<li><p>Ida Sim, MD PhD - Professor of Medicine and Computational Precision Health, UCSF</p></li>
</ul>
<p>The implementation of AI in healthcare presents significant challenges, particularly when transitioning from AI development to real-world clinical use. Key discussions from experts highlight various approaches to scaling AI, ensuring trustworthiness, and overcoming deployment hurdles.</p>
<p><strong>Bridging the Gap Between AI and Clinical Implementation</strong></p>
<ul>
<li><p>Dr.&nbsp;Aristidou outlined two primary approaches to scaling AI:</p>
<ol type="1">
<li><p><strong>Homegrown AI</strong>: Single-function AI models that are scaled across different institutions, a process that requires adaptation to diverse environments.</p></li>
<li><p><strong>Platform AI</strong>: AI apps hosted on a unified platform, with scalability built into the model. The challenge is whether real-world organizations will adopt such systems.</p></li>
</ol></li>
</ul>
<p><strong>Critical Thinking and Foundation Building in AI</strong></p>
<ul>
<li><p>Dr.&nbsp;Hron emphasized the importance of applying AI thoughtfully in healthcare. He stresses:</p>
<ul>
<li><p>Avoid using AI just for the sake of using it.</p></li>
<li><p>The need to build a solid foundation that aligns AI with healthcare delivery, ensuring equity, effective use, and proper training.</p></li>
<li><p>Tools like ambient Llstening and GenAI Reference Tools (e.g., <a href="https://subscriptions.elsevier.com/clinicalkey-ai.html">ClinicalKey</a>) can enhance AI utility, but their impact must be evaluated carefully.</p></li>
</ul></li>
</ul>
<p><strong>Ensuring Trustworthiness in AI</strong></p>
<ul>
<li><p>Dr.&nbsp;Adler-Milstein highlighted the difficulty health systems face in assessing AI’s trustworthiness:</p>
<ul>
<li><p>AI tools often lack standard regulations, and healthcare systems may not have the capacity to evaluate them.</p></li>
<li><p>She proposes a robust AI Oversight Committee to oversee the AI lifecycle from discovery to adoption, ensuring tools are trustworthy and continuously monitored.</p></li>
<li><p><strong>IMPACT</strong> (AI Monitoring Infrastructure and Adjudication Process) can provide ongoing vigilance, tracking AI’s effectiveness and outcomes in real time.</p></li>
</ul></li>
</ul>
<p><strong>Incentives and System Integration</strong></p>
<ul>
<li><p>Dr.&nbsp;Liu discussesed the shift from “push” to “pull” in AI adoption:</p>
<ul>
<li><p><strong>Pull</strong>: Creating systems where AI’s value is clear - improved care, productivity gains, reduced errors, and cost savings.</p></li>
<li><p><strong>Push</strong>: Overcoming challenges such as cost, data privacy, and technical issues that impede adoption.</p></li>
<li><p>The <em>Prediction:Action Dyad</em> emphasizes balancing system capacity, intervention costs, and potential harms when implementing AI-driven decisions.</p></li>
</ul></li>
</ul>
<p><strong>Modular Approaches to AI Integration</strong></p>
<ul>
<li><p>Dr.&nbsp;Sim stressed the need for modular, evolving AI solutions that can integrate with health systems:</p>
<ul>
<li><p>AI must be adaptable, with individualized solutions that evolve over time rather than a one-size-fits-all approach.</p></li>
<li><p>Platforms like <a href="https://jupyterhealth.org/">JupyterHealth</a> enable data ingestion and management, fostering a collaborative ecosystem of modular interventions.</p></li>
<li><p>It’s crucial to differentiate between AI effectiveness (its performance) and AI utility (its practical use in real-world settings).</p></li>
</ul></li>
</ul>
<p>These discussions highlight the complexities of scaling and deploying AI in healthcare, emphasizing the need for thoughtful integration, continuous monitoring, and a focus on trustworthiness and usability in clinical settings.</p>
</section>
<section id="conclusions" class="level3">
<h3 class="anchored" data-anchor-id="conclusions">11. Conclusions</h3>
<p>This concludes my summary of the Stanford Health + AI Conference 2024!</p>
<p>My biggest takeaways from the conference were the following:</p>
<ul>
<li><p>Remember the significance of relationship-building and trust during AI deployment. The “human stack” is much more important than the AI tech stack.</p></li>
<li><p>Keep the interests of the ultimate stakeholder (the patient) at the heart of any decision in infrastructure change.</p></li>
<li><p>AI model development is great, but it is just the start - successful deployment of AI into a healthcare system requires consideration of the surrounding processes, workflows, and ecosystems into which it will be placed.</p></li>
</ul>
<p>With that, I’d like to give a huge thank you to Stanford AIMI and the organizers of this conference for all of their hard work as well as to all of the speakers and panelists for contributing their time and knowledge to such outstanding discussions. I left this conference feeling extremely inspired about my own work with the Fred Hutch Chief Data Office, and I am excited to bring my insights from these talks into my upcoming work as we productionize robust AI workflows for clinical research and care at our own institution. Until next time, [VS]Coders!</p>


</section>

 ]]></description>
  <category>Conferences</category>
  <guid>https://vsriram24.github.io/posts/stanford_health_ai_conf_24/</guid>
  <pubDate>Mon, 16 Dec 2024 08:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/stanford_health_ai_conf_24/lookup.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>An overview of data democratization</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/data-democratization/</link>
  <description><![CDATA[ 





<p>Welcome back to my personal biomedical data science blog, <em>[VS]Codes</em>! In today’s post, I will provide a brief overview of the concept of “data democratization” and why this process holds an essential place in the data and AI strategies for a wide swath of data-driven organizations.</p>
<p><img src="https://vsriram24.github.io/posts/data-democratization/shattle.JPEG" class="img-fluid" width="556"></p>
<hr>
<section id="what-is-data-democratization" class="level4">
<h4 class="anchored" data-anchor-id="what-is-data-democratization"><strong>What is data democratization?</strong></h4>
<p>In today’s digital world, data, the information we gather about everything around us, are a vital asset. However, the collection, processing, and analysis of data have traditionally been limited only to individuals who have sufficient technical expertise. This barrier to entry in the field of data science has created a significant gap between those who can derive data-driven insights from the information around them and those who cannot, resulting in unequal access to opportunities and resources. <strong>Data democratization</strong> aims to bridge this gap by making data accessible to everyone within an organization, regardless of technical skill.</p>
<p>Data democratization focuses on breaking down the barriers around data. It empowers individuals and communities to use data to make informed decisions, drive innovation, and create positive social impact. Not only does data democratization ensure that all individuals in an organization have the ability to access data appropriately, but it also gives them the tools and training necessary to understand the data around them.</p>
</section>
<section id="the-goals-of-data-democratization" class="level4">
<h4 class="anchored" data-anchor-id="the-goals-of-data-democratization"><strong>The goals of data democratization</strong></h4>
<p>Data democratization aims to give all end-users, including employees, stakeholders, and consumers, the confidence to work with data and trust the results of their analyses. We can summarize the key goals of data democratization as follows:</p>
<ul>
<li><p>Remove barriers to access and understanding</p></li>
<li><p>Help non-specialists view and leverage an organization’s data and optimize its usefulness</p></li>
<li><p>Ensure that the right people can see the right data at the right time for the right purpose</p></li>
<li><p>Help produce informed decisions and identify both opportunities and problems without requiring prior in-depth knowledge</p></li>
</ul>
<p>Data democratization is designed to eliminate complicated frameworks and bottlenecks in the data access pipeline by widening the range of stakeholders who use data to enhance their day-to-day work. Ultimately, data democratization shifts how decisions are made, moving from a model of centralized data control to a more inclusive, collaborative approach.</p>
</section>
<section id="the-steps-to-a-data-democratization-strategy" class="level4">
<h4 class="anchored" data-anchor-id="the-steps-to-a-data-democratization-strategy"><strong>The steps to a data democratization strategy</strong></h4>
<p>The following sequence of steps offer a potential way to help foster data democratization at your organization:</p>
<ol type="1">
<li>Perform a data audit
<ul>
<li><p>Determine where your data are currently stored (on-premises or in the cloud), who has access to them, and what tools are currently used for collection, management, and analysis.</p></li>
<li><p>Determine which parts of the existing system work well and where the bottlenecks/inefficiencies exist.</p></li>
<li><p>Evaluate the data literacy of your employees and identify current security/compliance protocols.</p></li>
</ul></li>
<li>Define your data democratization goals
<ul>
<li>Align your data democratization goals with the overall goals of the organization as much as possible.</li>
</ul></li>
<li>Centralize your data
<ul>
<li><p>Cloud storage is ideal - it is scalable, accessible from anywhere, and has low cost of entry. Furthermore, centralized cloud storage ensures that end-users can visit a single platform for their data needs.</p></li>
<li><p>Make sure the data kept in storage are well-organized and searchable.</p></li>
</ul></li>
<li>Enact data governance policies
<ul>
<li>Set guidelines for how data are stored and protected, who can see (and edit) which data, and how data should be used.</li>
<li>Define roles and responsibilities for data management, establish policies for data security and privacy, and outline procedures for data sharing and collaboration.</li>
</ul></li>
<li>Maintain ongoing training to ensure that all end users are able to handle data effectively and securely
<ul>
<li><p>Invest in full and regular training at all levels of the organization so that everyone has the required data literacy to identify, discover, and analyze the data they need</p></li>
<li><p>Train users to apply relevant data democratization tools as well as on general data awareness</p></li>
</ul></li>
</ol>
</section>
<section id="benefits-and-challenges-of-data-democratization" class="level4">
<h4 class="anchored" data-anchor-id="benefits-and-challenges-of-data-democratization"><strong>Benefits and challenges of data democratization</strong></h4>
<p>A variety of benefits and challenges exist in the implementation of data democratization.</p>
<p><strong>Benefits</strong></p>
<ul>
<li><p><strong><em>Data democratization breaks down data silos</em></strong>: Data silos occur when information is stored on separate systems, with each accessible by only a particular sub-team or department. Data democratization centralizes the data, making it easier to share data across teams and improving cross-functional decisions. Having centralized, standardized data ensures that everyone sees the same information in the same format, boosting accuracy and fostering a culture of collaboration and knowledge sharing.</p></li>
<li><p><strong><em>Data democratization removes bottlenecks</em></strong>: Data democratization grants access to and educates everyone on where data are stored, how to find the right information, and how to use it effectively. IT departments no longer have to worry about granting data access to individuals, and data teams no longer have to manage multitudes of data requests.</p></li>
<li><p><strong><em>Data democratization optimizes data management</em></strong>: Data federation software can be applied to collate a virtual database of information from different sources ready for business intelligence. Data are now easier to find in a centralized hub, simplifying the data validation process and improving data quality, accuracy, and security.</p></li>
<li><p><strong><em>Data democratization increases data-driven decision making</em></strong>: Data democratization creates a culture of data literacy where people are encouraged to use data to inform decisions and drive innovation. It can lead to greater transparency and accountability within organizations as well - individuals have access to the same data and can validate one another’s findings, encouraging openness and innovation.</p></li>
</ul>
<p><strong>Challenges</strong></p>
<ul>
<li><p><strong><em>Data democratization may introduce more compliance and security concerns</em></strong>: because data democratization removes the traditional checks and balances that come from more hands-on data management systems, end-users may misuse their data platforms and inappropriately access data or store them in external locations. There is also a reduction into visibility of the data - when more users have access, it can become harder to determine who is doing what with which information. Organizations must ensure that they have a robust infrastructure that can handle large volumes of data and provide quick and easy access to authorized users. Clear policies and guidelines for data access and usage must be established to prevent data misuse, including defining who has access to which data, how data are stored and protected, and how data can be used in decision-making processes. Lastly, organizations need to establish protocols for protecting sensitive data, including anonymization, encryption, and access controls.</p></li>
<li><p><strong><em>Coupled with improperly handled tools, data democratization can amplify mistrust of the data</em></strong>: If the tools available on an organization’s data platform are inaccessible, or insufficient training is provided for end-users, a “data-democratized” system may do more harm than good - organizations must ensure that the tools available on their data platforms are easily accessible and have a low barrier to entry, as well as that the training they offer is sufficient to advance the data literacy of end-users.</p></li>
<li><p><strong><em>Data democratization cannot fix low-quality data</em></strong>: Ultimately, the machine learning mantra of “garbage in, garbage out” rings true for data democratization as well - inaccuracies in the data or a lack of consistent formatting will turn a data lake into a “data swamp,” where end users spend more of their time finding and cleaning the right data than actually conducting analysis and developing data-driven insights. Organizations must institute a system of checks and balances to ensure that data are consistently entered and verified as well as that data sources are properly integrated and maintained. Tools such as automated data quality monitoring may help in these situations.</p></li>
</ul>
</section>
<section id="conclusion" class="level4">
<h4 class="anchored" data-anchor-id="conclusion"><strong>Conclusion</strong></h4>
<p>As technology evolves, so will the tools and approaches to data democratization. With advances in AI and machine learning, it will become easier to produce intuitive data tools that expand the base of users who can ask complex questions and conduct data-driven analysis without a need for prior technical expertise.</p>
<p>Ultimately, data democratization is not a trend - it is a necessary evolution for organizations looking to stay competitive and innovative. By removing the barriers to data, businesses can tap into the full potential of their workforce and foster a truly data-driven culture.</p>
<hr>
</section>
<section id="references" class="level4">
<h4 class="anchored" data-anchor-id="references"><strong>References</strong></h4>
<ul>
<li><p><a href="https://www.databricks.com/blog/data-democratization-embracing-trusted-data-transform-your-business">databricks - “Data Democratization: Embracing Trusted Data to Transform Your Business”</a></p></li>
<li><p><a href="https://www.datacamp.com/blog/what-does-democratizing-data-mean">datacamp - “What is Data Democratization? Unlocking the Power of Data Culture for Businesses”</a></p></li>
<li><p><a href="https://www.anomalo.com/blog/data-democratization-what-it-means-and-why-it-matters/">Anomalo - “Data Democratization: What It Means and Why It Matters”</a></p></li>
<li><p><a href="https://www.immuta.com/blog/exploring-data-democratization/">Immuta - “Exploring Data Democratization: Pros, Cons, and Real-World Impact”</a></p></li>
</ul>


</section>

 ]]></description>
  <category>Overviews</category>
  <guid>https://vsriram24.github.io/posts/data-democratization/</guid>
  <pubDate>Mon, 02 Dec 2024 08:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/data-democratization/zola_stood.JPEG" medium="image"/>
</item>
<item>
  <title>An overview of learning health systems</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/lhs/</link>
  <description><![CDATA[ 





<p>Welcome back to another post from my personal biomedical data science blog, <em>[VS]Codes</em>! In today’s post, I will provide a brief overview of the concept of “learning health systems” and how this model is shifting the way that healthcare is approached today.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/lhs/leaves.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="384"></p>
</figure>
</div>
<section id="translational-informatics-advancing-medical-care-from-bench-to-bedside" class="level4">
<h4 class="anchored" data-anchor-id="translational-informatics-advancing-medical-care-from-bench-to-bedside"><strong>Translational informatics: advancing medical care from “bench to bedside”</strong></h4>
<p>The discipline of “<a href="https://en.wikipedia.org/wiki/Translational_research_informatics">translational informatics</a>” focuses on the translation of data generated from research-based biomedical endeavors to applications in the clinic, allowing for the improvement of disease diagnosis, staging, prognosis, and treatment. Translational research will typically involve the development of machine learning methods to integrate multimodal, heterogeneous biomedical data for clinical decision support. Ultimately, translational informatics enables the study of increasingly large bodies of biomedical data to inform predictive, preventive, and personalized health applications. Translational informatics can also be deeply tied to the concept of “bench to bedside” biomedical discovery, where the results of research conducted in the laboratory are directly used to create actionable innovations for real-world healthcare settings.</p>
</section>
<section id="learning-health-systems-implementing-the-translational-medicine-paradigm" class="level4">
<h4 class="anchored" data-anchor-id="learning-health-systems-implementing-the-translational-medicine-paradigm"><strong>Learning Health Systems: implementing the translational medicine paradigm</strong></h4>
<p>Learning Health Systems are intrinsically linked to the concepts of “bench to bedside” care and translational informatics. A Learning Health System (LHS) refers to a healthcare model that makes use of continual data collection and analysis for the improvement of patient care. The key idea driving LHSs is the cyclical nature of advancements in research and clinical care: real-time data coming from routine care and patient experiences inform new directions in biomedical research, improving the way that clinical practice is performed. This continuous learning feedback loop is crucial for the improvement of patient outcomes and longer-term healthcare innovation.</p>
<p>As defined by the <a href="https://nam.edu/programs/value-science-driven-health-care/lhs-core-principles/">National Academy of Medicine (NAM)</a>, the four key elements of LHSs are:</p>
<ul>
<li><p>Generation, application, and improvement of scientific knowledge</p></li>
<li><p>Organizational infrastructure to support the engagement of patient communities, healthcare professionals, and researchers for the identification of evidence gaps between biomedical research and clinical care</p></li>
<li><p>Deployment of computational technologies and informatics approaches to organize and leverage large-scale electronic health data for use in research</p></li>
<li><p>Quality improvement at the point of care for each patient using new knowledge generated by research</p></li>
</ul>
<p>Some examples of downstream applications of LHSs include:</p>
<ul>
<li><p>Cohort development for clinical trials and research studies through the collection of patients with similar attributes to one another</p></li>
<li><p>Identification of both improved as well as sub-optimal examples of patient care and treatments as compared to standardized benchmarks</p></li>
<li><p>The creation of patient risk models for adverse events and outcomes</p></li>
<li><p><a href="https://www.ahrq.gov/cpi/about/otherwebsites/clinical-decision-support/index.html">Clinical Decision Support (CDS)</a> systems to recommend personalized treatment options</p></li>
<li><p>Automation of routine care processes</p></li>
<li><p>Surveillance monitoring for disease outbreaks and other treatment issues and complications</p></li>
</ul>
</section>
<section id="benefits-and-challenges-of-learning-health-systems" class="level4">
<h4 class="anchored" data-anchor-id="benefits-and-challenges-of-learning-health-systems"><strong>Benefits and challenges of Learning Health Systems</strong></h4>
<p>Key benefits of LHSs include better patient outcomes through personalized care and faster implementation of data-driven research advancements, as well as lowered healthcare costs through the implementation of improved processes and the reduction of ineffective treatments.</p>
<p>However, numerous challenges have also hindered the wide-spread adoption of LHS principles globally. Some of the biggest obstacles to the broader implementation of these guidelines include:</p>
<ol type="1">
<li><strong>Maintaining data interoperability and security</strong>: multimodal, heterogeneous data are often siloed across healthcare systems, making it challenging to integrate data across departments and organizations. Furthermore, these data typically do not follow consistent common data standards, further complicating the process for data integration. LHSs also need to ensure that a system for data democratization is in place, including appropriate data access rights as well as ethical data stewardship that promotes not only data sharing but also patient privacy.</li>
<li><strong>Overcoming cultural resistance and realigning healthcare incentives</strong>: Setting up an LHS requires significant investments in technology infrastructure, data management systems, and staff training. Healthcare organizations are typically slow to adopt such new practices, particularly when they require drastic changes in traditional workflows. Instead, gradual shifts must take place to bring researchers and healthcare providers to alignment on priorities and timelines. Overall healthcare incentives must be shifted from volume to value, and appropriate performance metrics will need to be developed to hold clinicians and their teams accountable toward patient care.</li>
<li><strong>Ensuring data quality and minimizing bias</strong>: The mantra of “garbage in, garbage out” is particularly pertinent in the LHS model of continuous data generation and ingestion - if the data used to train the healthcare system are biased, incomplete, or inaccurate, then the overall reliability and applicability of the system will be drastically reduced.</li>
<li><strong>Keeping pace with technological advancement and ensuring proper model validation</strong>: Rapid advancements in artificial intelligence and data analytics mean that healthcare organizations must constantly stay abreast of new tools and methodologies while continuing to fulfill their current care duties. Given the meteoric changes in the field of information technology, it is essential that a proper system is put in place to safely incorporate the latest shifts in technology into the workings of each LHS. Furthermore, appropriate global standards and metrics must be developed and agreed upon to help determine the efficacy of changes introduced to each LHS.</li>
</ol>
</section>
<section id="looking-toward-the-future-an-iterative-process" class="level4">
<h4 class="anchored" data-anchor-id="looking-toward-the-future-an-iterative-process"><strong>Looking toward the future: an iterative process</strong></h4>
<p>While many healthcare systems have begun to incorporate elements of LHSs into their workflows, the complete LHS is still very much an aspirational model. Becoming a complete LHS is an iterative process that requires a step-by-step cultural shift toward the effective use of data across both clinical and research settings. Future advancements toward the LHS model will necessitate progress in multiple categories, including technology, regulatory policies and funding, and the direct involvement and engagement of patients.</p>
<hr>
</section>
<section id="references" class="level4">
<h4 class="anchored" data-anchor-id="references"><strong>References</strong></h4>
<ul>
<li><p><a href="https://bioethics.jhu.edu/learning-health-systems/about/">Johns Hopkins Berman Institute of Bioethics - What is a Learning Health System?</a></p></li>
<li><p><a href="https://www.ahrq.gov/learning-health-systems/about.html">Agency for Healthcare Research and Quality - About Learning Health Systems</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Learning_health_systems">Wikipedia - Learning health systems</a></p></li>
<li><p><a href="https://informatics.bmj.com/content/26/1/e100037">McLachlan et al.&nbsp;2019 - LAGOS: learning health systems and how they can integrate with patient care</a></p></li>
<li><p><a href="https://bmir.stanford.edu/research-groups/translational-informatics.html">Stanford Medicine Center for Biomedical Informatics Research - Translational Informatics</a></p></li>
</ul>


</section>

 ]]></description>
  <category>Overviews</category>
  <guid>https://vsriram24.github.io/posts/lhs/</guid>
  <pubDate>Mon, 18 Nov 2024 08:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/lhs/zola_looking_over.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Why I think the pursuit of “work-life balance” is overrated</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/work-life-harmony/</link>
  <description><![CDATA[ 





<p><em>Triumphant music swells in the background…</em> <strong>Duh duh duhhh!!!</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/work-life-harmony/hello-there-hi-there.gif" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Welcome back to my personal biomedical data science blog, <em>[VS]Codes</em>! It’s so great to be back writing on my platform and connecting with you all once again. I’m very happy to share that my brief hiatus from this blog was extremely productive and gave me a much-needed break - even though I am (finally) no longer in school, the month of September still presented an uptick in obligations as colleagues returned from their summer vacations, and I found myself struggling to balance my other deliverables with my writing for this blog. Indeed, by the middle of August, I found myself feeling more stressed than excited each time I had to produce new content. Now, however, I am re-energized and ready to fill your headspaces once again :) In the spirit of my (self-inflicted) burnout over the past couple of months, I thought it most appropriate to write today about a topic dear to my heart: <strong>work-life harmony</strong>.</p>
<p>Work-life harmony is not a new idea by any means, but it exists as a stark reminder of the deficits of its more well-known counterpart: <strong>work-life balance.</strong> So let’s dive a little deeper into the differences between these two terms.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/work-life-harmony/tree_painting.jpg" class="img-fluid figure-img" width="479"></p>
<figcaption>A misty sunrise from one of my recent morning walks</figcaption>
</figure>
</div>
<section id="work-life-balance" class="level2">
<h2 class="anchored" data-anchor-id="work-life-balance">Work-life balance</h2>
<p>Work-life balance emphasizes the need to have a split between your professional and personal lives. The goal of this paradigm is to ensure that clear boundaries exist between the two “most important” parts of your life - when you are at work, you are fully focused on your work. And when you are out of work, you are fully focused on your personal life. The split between one’s personal and professional lives doesn’t need to be exactly 50/50 (maybe it’s 40/60 or 70/30), but there should be a relative “balance” in how much time is offered to both. The work-life balance mindset works great for many individuals, particularly for those who have to work in-person for their professional commitments. In these cases, coming home from the workday becomes a mandatory unplugging from the demands of professional life.</p>
<p>However, for individuals without these forced boundaries (i.e.&nbsp;hybrid/remote workers), maintaining a balance between work and life can end up being more stress-inducing than relaxing! When you are forced to self-define the boundaries between work and life, you may end up feeling guilty every time work eats into your time with your family, or every time personal duties or free time pull you away from a commitment or responsibility you were expected to complete at work. Ultimately, for some people, the pursuit of a work-life balance can cause them to fall between the cracks. They can never feel completely present at work or at home - rather than being there for the people around them, they are trapped in a spiral of guilt.</p>
</section>
<section id="work-life-harmony" class="level2">
<h2 class="anchored" data-anchor-id="work-life-harmony">Work-life harmony</h2>
<p>The paradigm of <em>work-life harmony</em> serves as a “reframing” of work-life balance. Where work-life balance aims to define clear boundaries between our personal and professional lives, work-life harmony emphasizes the inherent interconnectedness of these aspects of ourselves. The adage of “you are not your job” is certainly true, but at the same time, the work that you do everyday and the output that you produce are a major part of your identity! Attempting to fully silo these aspects of yourselves from the rest of your life is often more harmful than helpful.</p>
<p>Work-life harmony emphasizes flexibility over rigidity - there is no need to feel ashamed if you think about your work when you’re on vacation. In fact, especially if you are excited and passionate about the work that you’re doing, then it is only natural that you will think about it outside of your workday! In a similar vein, there is no need to feel guilty if you want to take time away in the middle of a workday to spend time with your family or to prioritize your health. <strong>At the end of the day, we should measure ourselves not by how much continuous time we have spent on a given task, but instead by the positive impact that we have on others and the contributions that we make toward society</strong>. Ultimately, if we focus more on showing up for ourselves and leading our days in a manner that is most fulfilling with respect to our own needs, then we will be able to show up a hundred times over for our personal and professional obligations.</p>
</section>
<section id="the-law-of-averages" class="level2">
<h2 class="anchored" data-anchor-id="the-law-of-averages">The law of averages</h2>
<p>The last point I want to end on today is a key piece of advice I received in the first week of my job from the head of my department, <a href="https://jtleek.com">Jeff Leek</a>. When I asked him how he was able to juggle the multitude of commitments that he faced daily, he (fittingly) responded to me with a statistical principle: “Follow the law of averages!”</p>
<p>This rule states that if you repeat a random event a sufficient number of times, the average outcome of the event will tend to converge toward an expected value. Sure, we may see outliers in our data every once in a while, but individual short-term fluctuations in outcomes do not matter - what matters at the end of the day is the expected value.</p>
<p>In a similar vein, we can treat each day of our lives as an individual outcome. It doesn’t necessarily matter if we have short-term fluctuations or trends in our data… as long as we hit our expected averages. If there’s an important work deadline coming up or you just find yourself particularly excited about a project that you’re working on, then spend time for a week or two outside of your 9-5 going relentlessly after those goals. And if you find yourself feeling a lack of motivation on a given day or have to take time off for a couple of weeks to attend to a friend or family member in need, then do so without guilt. Ultimately, by following the law of averages, we can ensure that we’re staying flexible with respect to our requirements and showing up fully for ourselves and our loved ones.</p>
</section>
<section id="additional-resources" class="level2">
<h2 class="anchored" data-anchor-id="additional-resources">Additional resources</h2>
<p>Hopefully my personal interpretation of the differences between work-life harmony and balance was useful to you! At the same time, as I mentioned earlier in this post, work-life harmony is not a new concept, and there are several resources freely available online that go into more detail and provide more varied perspectives. Here are some additional links related to work-life harmony (and a video on the law of averages) that you may find informative:</p>
<ul>
<li><a href="https://www.linkedin.com/pulse/from-work-life-balance-harmony-new-path-chibs/">“From Work-Life Balance to Work-Life Harmony: A New Path for High Achievers,”</a> by Chibs Okereke</li>
<li><a href="https://online.maryville.edu/blog/work-life-balance-vs-harmony/">“What Is Work-Life Balance vs.&nbsp;Work-Life Harmony?”</a> by Maryville University</li>
<li><a href="https://meetharlow.com/blog/work-life-harmony/">“What is Work-Life Harmony? (and how you can achieve it),”</a> by Afoma Umesi</li>
<li><a href="https://www.youtube.com/watch?v=D_pGQBdX55Y">“Law of Averages - How To Be Successful In Anything You do,”</a> by Improvement Pill</li>
</ul>
<hr>
<p>This concludes my blog post for today - It’s great to be posting again, and I look forward to sharing similarly styled overviews of work concepts and paradigms in the future. My cadence of blog posts may shift from weekly to bi-weekly depending on work commitments, but I will definitely be back soon! Until next time!</p>


</section>

 ]]></description>
  <category>Advice</category>
  <category>Miscellaneous</category>
  <category>Personal</category>
  <guid>https://vsriram24.github.io/posts/work-life-harmony/</guid>
  <pubDate>Mon, 04 Nov 2024 08:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/work-life-harmony/zola_cat.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>My blog / newsletter recommendations</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/newsletters/</link>
  <description><![CDATA[ 





<p>Welcome back to another edition of <em>[VS]Codes</em>! In this week’s post, I’ll be summarizing a list of the top blogs / newsletters to which I subscribe, including content that covers biotech and health AI, data science and software, and a few miscellaneous topics at the end. Note that all of these newsletters allow for free base subscriptions (with options for paid tiered subscriptions to support the creators if desired). Without further ado, let’s get started!</p>
<p><img src="https://vsriram24.github.io/posts/newsletters/bear.jpeg" class="img-fluid"></p>
<hr>
<p><strong><em>Medical Informatics, Biotechnology, and Health AI</em></strong></p>
<ol type="1">
<li><p>“<a href="https://newsletter.smartbrief.com/rest/sign-up/FCE90269-079A-4211-B3E0-85FCEDCFBE17?pageName=TELLMEMORE">AMIA Informatics SmartBrief</a>” by the <a href="https://amia.org">American Medical Informatics Association</a></p>
<p>“AMIA Informatics SmartBrief” offers a daily snapshot of biomedical informatics with news from a variety of health informatics sources. In particular, I appreciate that subscribers can select the topic categories that most interest them: I am personally subscribed to “Top News,” “Clinical Informatics and Analytics,” “Health Data Science and Artificial Intelligence,” “Population Health,” and “AMIA News”.</p></li>
<li><p>“<a href="https://decodingbio.substack.com">Decoding Bio</a>” by <a href="https://x.com/ameekapadia">Amee Kapadia</a>, <a href="https://x.com/pablolubroth">Pablo Lubroth</a>, <a href="https://x.com/patricksmalone">Patrick Malone</a>, <a href="https://x.com/morgancheatham">Morgan Cheatham</a>, <a href="https://x.com/KetanYerneni">Ketan Yerneni</a>, and <a href="https://x.com/ZahraKhwaja">Zahra Khwaja</a></p>
<p>“Decoding Bio” breaks down advancements at the intersection of computation and the life sciences. The authors focus on translating technical developments and trends in the biotech and “tech-bio” spaces into an easy-to-read format. The goal of the newsletter and the larger writing collective is to start conversation around changes in the biotech industry, including the rapid advancement of health AI.</p></li>
<li><p>“<a href="https://doctorpenguin.substack.com">Doctor Penguin</a>” by <a href="https://scholar.google.com/citations?user=89ANml4AAAAJ&amp;hl=en">Emma Chen</a>, <a href="https://www.linkedin.com/in/shreya-johri/">Shreya Johri,</a> <a href="https://dbmi.hms.harvard.edu/people/pranav-rajpurkar">Pranav Rajpurkar</a>, and <a href="https://www.scripps.edu/faculty/topol/">Eric Topol</a></p>
<p>The aim of “Doctor Penguin” is to help researchers keep up with the latest research updates in the field of AI and healthcare. Each week includes brief summaries of some of the latest top papers, as well as links to the original publications for subscribers to read.</p></li>
<li><p>“<a href="https://centuryofbio.com">The Century of Biology</a>” by <a href="https://elliothershberg.com">Elliot Hershberg</a></p>
<p>Elliot is a current PhD student in the Department of Genetics at Stanford, as well as a biotechnology investor. His blog, “The Century of Biology,” summarizes the latest progress in the field of biotechnology and offers three main topics of content: Data/Research, Companies/Strategy/Analysis, and Philosophy. Elliot’s long term ambition is “to develop a philosophy outlining why it is a moral and aesthetic imperative to pursue biotechnology.” Elliot is currently taking a break from writing his blog to defend his PhD, but there is a trove of content freely available that provides insightful examinations of biotech developments. I very much look forward to his return to writing later this year!</p></li>
</ol>
<p><strong><em>Data Science and Software</em></strong></p>
<ol type="1">
<li><p>“<a href="https://tidyfirst.substack.com">Software Design: Tidy First?</a>” by <a href="https://en.wikipedia.org/wiki/Kent_Beck">Kent Beck</a></p>
<p>Kent Beck’s self-proclaimed mission is “to help geeks feel safe in the world.” His blog achieves this goal by covering a variety of topics in programming and software development that encourage a perspective shift in the way that we approach work in the field of tech and beyond.</p></li>
<li><p>“<a href="https://alphasignal.ai">AlphaSignal</a>” by <a href="https://www.linkedin.com/in/liorsinclair/">Lior Sinclair</a></p>
<p>AlphaSignal is an AI-driven technical newsletter that helps the scientific community stay up to date with the Machine Learning industry by providing a round-up of publications and breakthroughs identified by its algorithm. Topics covered include relevant AI news, models, research, and repositories;.</p></li>
<li><p>“<a href="https://presentofcoding.substack.com">The Present of Coding</a>” by <a href="https://github.com/abigailhaddad">Abigail Haddad</a></p>
<p>Abigail Haddad is a machine learning engineer and data scientist. Her blog focuses on topics in data science, coding, and Large Language Models. I very much appreciated two of most recent posts, “<a href="https://presentofcoding.substack.com/p/how-to-become-a-government-data-scientist">How to Become a Government Data Scientist</a>” and “<a href="https://presentofcoding.substack.com/p/a-developer-is-you">A Developer is You</a>,” both of which have shifted the way that I think about learning and upskilling.</p></li>
<li><p>“<a href="https://fhdata.substack.com">Monday Morning Data Science (MMDS)</a>” by the <a href="https://hutchdatascience.org">Fred Hutch Data Science Lab (DaSL)</a></p>
<p>MMDS offers a weekly dose of data news, curated by the Fred Hutch Data Science Lab (a.k.a. my team!). The newsletter covers not only updates from DaSL but also blog posts on data science and statistical content from a variety of other data science content creators.</p></li>
<li><p>“<a href="https://www.allendowney.com/blog/">Probably Overthinking It</a>” by <a href="https://www.allendowney.com/wp/">Allen Downey</a></p>
<p>Allen Downey is a Principal Data Scientist at PyMC Labs, professor emeritus at Olin College, and along with other titles, the author of <em>Probably Overthinking It</em>. His blog of the same name tackles a variety of topics with non-intuitive answers from a statistical lens, proving with the power of computational statistics that everything is not as it seems!</p></li>
<li><p>“<a href="https://buttondown.com/ready4r/archive/">Ready for R</a>” by <a href="https://laderast.github.io">Ted Laderas</a></p>
<p>Ted is a co-worker of mine at Fred Hutch in charge of developing training material and directing communities of practice for Fred Hutch staff. His newsletter offers a glimpse into his outstanding abilities as an educator, helping readers learn the basics of rstats and the tidyverse. He also posts about new R packages and other resources that help readers upskill in their R knowledge.</p></li>
</ol>
<p><strong><em>Miscellaneous</em></strong></p>
<ol type="1">
<li><p>“<a href="https://bakadesuyo.com/newsletter/">Barking Up The Wrong Tree</a>” by <a href="https://bakadesuyo.com/about/">Eric Barker</a></p>
<p>Eric’s weekly newsletter offers “a scientific deep dive that will improve your life.” Following along with much of the content from his <em>The Wall Street Journal</em> bestseller “<a href="https://bakadesuyo.com/barking-up-the-wrong-tree/">Barking Up the Wrong Tree</a>,” Eric’s blog offers actionable tips based on psychology, neuroscience, and expert insight to “make your life more awesome.”</p></li>
<li><p>“<a href="https://www.thedoctorskitchen.com">The Doctor’s Kitchen</a>” by <a href="https://www.thedoctorskitchen.com/about/the_doctors_kitchen.php">Rupy Aujla</a></p>
<p>The goal of Rupy’s platform “The Doctor’s Kitchen” is to “teach people how to cook their way to health.” Rupy started the Doctor’s Kitchen as a way of teaching everybody how they can cook their way to health and to showcase the beauty of food and medicinal effects of eating and living well.</p></li>
<li><p>“<a href="https://andrewknapp.substack.com">How To Be A Dog</a>” by <a href="https://andrewknapp.com">Andrew Knapp</a></p>
<p>Andrew Knapp is the best-selling author of the <em>Find Momo</em> series, and he is a true artist in his ability to capture the spirit of life and adventure with his dogs and his camera. His blog highlights the lessons that dogs can teach us in our own lives with respect to appreciating the beauty around us and developing a deeper connection and appreciation for each and every day.</p></li>
</ol>
<hr>
<p>This concludes my blog post for today! Now a note for my subscribers… with fall rolling around and work / life picking up, it’s becoming more challenging to keep up with my weekly schedule of creating new content for the blog! So, I’ll be taking a hiatus for the next month to turn my attention to some immediate deliverables, while also developing new content in the background. When I start posting again toward the end of October, I will likely switch to a bi-weekly release schedule to make this whole project more sustainable with respect to my personal schedule! Until we meet again~</p>



 ]]></description>
  <category>Personal</category>
  <guid>https://vsriram24.github.io/posts/newsletters/</guid>
  <pubDate>Mon, 16 Sep 2024 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/newsletters/autumn.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>An intro to Convolutional Neural Networks</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/cnn-overview/</link>
  <description><![CDATA[ 





<p>In this week’s blog, I will be summarizing the introductory “Convolutional Neural Networks” webpage from Fei-Fei Li’s Stanford CS 231n: “Deep Learning for Computer Vision” course. You can read the original page that I am summarizing <a href="https://cs231n.github.io/convolutional-networks/">here</a>, and you can took a look at Fei-Fei’s full course page <a href="https://cs231n.github.io">here</a>. Note that the original webpage includes about twice as much content as what I have in this post, including a more thorough discussion of real-world examples of neural networks, additional schemas such as parameter sharing, and a few coding examples in <code>numpy</code>. My goal for this post is to offer a “CliffsNotes of a CliffsNotes,” presenting just the basic intuition behind CNNs without any further detail. If you want to learn more, I would highly suggest reading the original webpage that this post summarizes, or additional resources such as <a href="https://arxiv.org/abs/1511.08458">this 2015 arXiv review paper</a> or <a href="https://www.geeksforgeeks.org/introduction-convolution-neural-network/">this GeeksForGeeks walkthrough</a> that includes some code examples. Without further ado, let’s get started!</p>
<p><img src="https://vsriram24.github.io/posts/cnn-overview/introPage.png" class="img-fluid"></p>
<hr>
<p>Convolutional Neural Networks (also known as CNNs or ConvNets) are very similar to standard Neural Networks. They are also made up of neurons that have learnable weights and biases, with each neuron receiving an input, performing a dot product, and leading into an optional non-linearity. The overall network will also express a single differentiable score function, as well as include a loss function for the last fully-connected layer of the network.</p>
<p>So what is the main difference between CNNs and other NNs? CNNs make the explicit assumption that the inputs are images. This assumption allows us to encode certain properties into the architecture, as well as make the forward function more efficient to implement and reduce the number of parameters in the network.</p>
<section id="architecture-overview" class="level3">
<h3 class="anchored" data-anchor-id="architecture-overview">Architecture Overview</h3>
<p>All neural networks will receive an input (a single vector) and transform it through a series of hidden layers. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and the neurons in a single layer function completely independently from one another (i.e.&nbsp;they share no connections with each other). The last fully-connected layer of the neural network is called the output layer - in classification settings, it represents the class scores.</p>
<p>When working with images, the full connectivity of a neural network makes it difficult to scale neurons for large inputs - for instance, with an image that is 32x32 pixels and has 3 color channels, each individual neuron would need to have 32*32*3 = 3072 weights. So how do we eliminate the wastefulness of fully connected neural networks?</p>
<p>CNNs constrain their architecture to match the dimensionality of images as input - in a CNN, neurons are arranged in three dimensions: <strong>width</strong>, <strong>height</strong>, and <strong>depth</strong>. Further, the neurons in a layer of a CNN will only be connected to a small region of the layer before them. Lastly, the final output layer will have dimensionality limited to 1x1xn, where n is the depth of the input layer, because the CNN architecture reduces the full input image into a single vector of class scores.</p>
<p><img src="https://vsriram24.github.io/posts/cnn-overview/3d.png" class="img-fluid"></p>
</section>
<section id="cnn-layers" class="level3">
<h3 class="anchored" data-anchor-id="cnn-layers">CNN Layers</h3>
<p>Every layer of a CNN will transform one volume of activations to another through a differentiable function. There are three main types of layers used to build CNN architectures:</p>
<ul>
<li><p>Convolutional Layer</p></li>
<li><p>Pooling Layer</p></li>
<li><p>Fully-Connected Layer</p></li>
</ul>
<p>These layers are stacked together to form a full CNN architecture. CNNs will then transform the original image layer by layer from its original pixel values down to the final class scores.</p>
<p>A point to note is that some layers contain parameters, while others do not. For instance, the convolutional layers and the fully-connected layers perform transformations that are a function of not only the activations in the input volume, but also the parameters of the neurons. On the other hand, ReLU layers (applying an elementwise activation function such as thresholding at 0 with max[0,x]) or pooling layers will implement a fixed function. Parameters in the convolutional and fully connected layers will be trained via gradient descent to get the class scores predicted by the CNN to match with the labels in the training set of images.</p>
<p>Let’s now dive deeper into the individual CNN layers and their hyperpameters / connectivities</p>
<section id="convolutional-conv-layer" class="level4">
<h4 class="anchored" data-anchor-id="convolutional-conv-layer">Convolutional (CONV) Layer</h4>
<p>The convolutional layer (CONV) is the core building block of a CNN - these layers perform the most computational heavy lifting. The CONV layer’s parameters include a set of learnable filters - each filter will have a small width and height but will extend through the full depth of the input volume (e.g.&nbsp;5x5x3 for an RGB picture). During the forward pass, we slide (a.k.a. convolve) each filter across the width and height of the input volume, computing dot products between the entries of the filter and the input at any position. As we slide the filter over the input volume, we produce a 2D activation map that gives the responses of that filter at every spatial position. We can think of the activation maps as masks that activate when they see a specific visual feature (e.g an edge or a blotch of color in the first layer; wheel or honeycomb shape in the last layer).</p>
<p><img src="https://vsriram24.github.io/posts/cnn-overview/depthSlice.png" class="img-fluid"></p>
<section id="local-connectivity" class="level5">
<h5 class="anchored" data-anchor-id="local-connectivity">Local Connectivity</h5>
<p>Convolutional layers also maintain local connectivity within the CNN - each neuron is connected only to a local region of the input volume. The spatial extent of this connectivity is a a hyperparameter known as the <strong>receptive field</strong>. While the connections exist locally along the width and height dimensions, they will extend along the full depth of the input volume.</p>
</section>
<section id="spatial-arrangement" class="level5">
<h5 class="anchored" data-anchor-id="spatial-arrangement">Spatial Arrangement</h5>
<p>Three different hyperparameters control how many neurons exist in the output volume and how they are arranged - these hyperparameters include depth, stride, and zero-padding.</p>
<ol type="1">
<li>Depth: corresponds to the number of filters we would like to use. The more filters we include in our CNN, the more features we may be able to identify.</li>
<li>Stride: corresponds to how we slide each filter in two-dimensional space. If the stride is 1, then we move the filters one pixel at a time. Changing the stride allows us to spatially manipulate the output volume of a layer.</li>
<li>Zero-padding: corresponds to padding the input volume with 0’s around the border. Zero padding allows us to control the spatial size of the output volumes - most commonly, zero padding can be used to ensure that the input and output width and height are kept consistent.</li>
</ol>
</section>
</section>
<section id="pooling-layer" class="level4">
<h4 class="anchored" data-anchor-id="pooling-layer">Pooling Layer</h4>
<p>Pooling layers can be found in between successive CONV layers in a CNN. The objective of each pooling layer is to reduce the spatial size of the representation, allowing us to control the number of parameters in the network, reduce required computation, and prevent overfitting to the training data. Each pooling layer operates independently on every depth slice of the input and resizes it spatially using the MAX operation (for instance, taking just the maximum value from a 2x2 square of values). Similar to the CONV layer, pooling occurs over the width and height dimensions, leaving the depth of the input unchanged. Note that it is not a <strong>requirement</strong> to include a pooling layer - many modern architectures have dropped pooling by sticking with CONV layers and manipulating the stride and depth hyperparameters instead.</p>
</section>
<section id="fully-connected-fc-layer" class="level4">
<h4 class="anchored" data-anchor-id="fully-connected-fc-layer">Fully-connected (FC) Layer</h4>
<p>Neurons in a FC layer have full connections to all activations in the previous layer, just like in neural networks.</p>
</section>
</section>
<section id="cnn-architectures" class="level3">
<h3 class="anchored" data-anchor-id="cnn-architectures">CNN Architectures</h3>
<p>The most common form of a CNN architecture will stack a few CONV-ReLu layers, follow them with a pooling layer, and then repeat this pattern until the image has been merged spatially to a smaller size. At some point, one might transition to fully-connected layers from CONV layers. The last FC layer will hold the output (e.g.&nbsp;the class scores).</p>
<p><img src="https://vsriram24.github.io/posts/cnn-overview/exampleArch.png" class="img-fluid"></p>
<p>If you’re feeling a bit of fatigue in thinking about NN architectures, then you will be relieved to know that in most cases, you really do not have to worry about how the structure of your network will affect the performance of your model. You really never have to train a CNN from scratch - just download a pretrained model (whatever works best on ImageNet) and finetune it on your data.</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>This concludes my overview of the introductory “Convolutional Neural Networks” chapter from Fei-Fei Li’s CS231n course. I look forward to summarizing more such chapters in the future and sharing more examples of code walkthroughs of these topics in the future. Until next time!</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://cs231n.github.io/convolutional-networks/" class="uri">https://cs231n.github.io/convolutional-networks/</a></li>
</ul>


</section>

 ]]></description>
  <category>Overviews</category>
  <guid>https://vsriram24.github.io/posts/cnn-overview/</guid>
  <pubDate>Mon, 09 Sep 2024 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/cnn-overview/donut.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>posit::conf 2024</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/posit-conf-24/</link>
  <description><![CDATA[ 





<p>Welcome back to another week of <em>[VS]Codes</em>! A few weeks ago, I had the opportunity to attend <a href="https://posit.co/conference/">posit::conf 2024</a>, a national data science conference organized annually by <a href="https://posit.co">Posit, PBC</a>. This year, the conference was held in my home base of Seattle, and it was a wonderful experience getting to attend so many inspiring talks and meet so many members of the data science community. This blog post will summarize content from the workshop I attended on the first day of the conference, as well as some of the talks that I attended over the following couple of days. I’ll conclude this post with my personal overall takeaways from the conference.</p>
<p><img src="https://vsriram24.github.io/posts/posit-conf-24/positConf.jpg" class="img-fluid"></p>
<hr>
<section id="package-development-the-rest-of-the-owl---jenny-bryan" class="level3">
<h3 class="anchored" data-anchor-id="package-development-the-rest-of-the-owl---jenny-bryan">“Package Development: The Rest of the Owl” - <a href="https://jennybryan.org/about/">Jenny Bryan</a></h3>
<p>The title of this talk was inspired by this <a href="https://karlastarr.substack.com/p/draw-the-rest-of-the-fucking-owl">blog post</a>, and serves to explain how one goes from the basics of creating a new package in R to fleshing out a more complete data product.</p>
<p>The key package this workshop highlighted was <code>devtools</code>: you can learn more about this package <a href="https://devtools.r-lib.org">here</a>.</p>
<p>The first thing you should always do when starting to develop a new package is to call <code>library(devtools)</code>. You can also force yourself to do this by including the line <code>require(devtools)</code> in your <code>.Rprofile</code>. With <code>devtools</code> loaded, you can call <code>dev_sitrep()</code> and <code>git_sitrep()</code> to get package development and git/GitHub ‘situation reports’ respectively as you work on your package.</p>
<p>When you call the <code>library</code> command in R, your code will go from installed packages to memory. On the other hand, calling the <code>load_all()</code> function from <code>devtools</code> will go through the entire process from source to memory. This includes the following steps:</p>
<ul>
<li><p>simulates building, installing, and attaching your package</p></li>
<li><p>makes all of the functions in your package available to use</p></li>
<li><p>makes anything you’ve imported available to use</p></li>
<li><p>allows fast iteration of editing and test-driving your functions.</p></li>
</ul>
<p>As you write code, it is also helpful to call the <code>check()</code> function often to ensure that you are not breaking things as you develop. Your workflow for writing and testing code in your package should emulate the following process:</p>
<p><img src="https://vsriram24.github.io/posts/posit-conf-24/packDev.png" class="img-fluid"></p>
<p>Another thing that folks don’t really consider when developing packages is that <strong><em>GitHub code search is your friend</em></strong>. You can check out <a href="https://github.com/cran" class="uri">https://github.com/cran</a> on GitHub - as packages get uploaded to CRAN, this user mirrors the full source code for the package onto GitHub for you. The same thing applies for <a href="https://github.com/tidyverse" class="uri">https://github.com/tidyverse</a> and <a href="https://github.com/r-lib" class="uri">https://github.com/r-lib</a>.</p>
<p>Regarding testing - when you are working on package development, most of your time will be spent looking at test files. For testing within the <code>devtools</code> framework, we call <code>use_testthat()</code>. Then we can run <code>test_file()</code> on a test R script that we’ve generated. A good workflow for macro-iteration across all files is to call <code>test()</code> followed by <code>test_coverage()</code> and finally followed by <code>check()</code>. <strong>Always aim to make tests self-sufficient and self-contained</strong>. It is better to repeat code than to introduce dependencies across tests. Don’t include <code>library()</code> or <code>source()</code> calls in your test files - include these in helper files instead.</p>
<p>Lastly, with respect to documentation - help topics are saved in <code>.Rd</code> files and will live in the <code>man/</code> folder of your R package. You can build a <code>README.md</code> file from your <code>README.Rmd</code> using the <code>build_readme()</code> function. It is also helpful to insert a <code>roxygen</code> skeleton into your code.</p>
<p>The following figure offers a good summary of how to approach package development with <code>devtools</code>:</p>
<p><img src="https://vsriram24.github.io/posts/posit-conf-24/packDev2.png" class="img-fluid"></p>
</section>
<section id="keynote-session-updates-from-posit---hadley-wickham" class="level3">
<h3 class="anchored" data-anchor-id="keynote-session-updates-from-posit---hadley-wickham">“Keynote Session: Updates from Posit” - <a href="https://hadley.nz">Hadley Wickham</a></h3>
<p>The company that organized this conference, formerly known as RStudio, is now known as Posit, PBC. But what is a PBC? PBC stands for “positive benefit corporation.” PBC’s sit between charities and full for-profit corporations. The mission of Posit is <strong>to create free and open source software for data science, scientific research, and technical communication</strong>. Posit supports both free and commercial tools - the free tools are meant to allow anyone to do data science work. The commercial tools are intended for larger organizations that apply Posit’s tooling at large.</p>
</section>
<section id="github-how-to-tell-your-professional-story---abigail-haddad" class="level3">
<h3 class="anchored" data-anchor-id="github-how-to-tell-your-professional-story---abigail-haddad">“GitHub: How To Tell Your Professional Story” - <a href="https://github.com/abigailhaddad">Abigail Haddad</a></h3>
<p>GitHub doesn’t just have to be a way for you to version control your code - it can be a platform for showcasing the work that you care about. You can update the <code>README</code> file for the repo that has the same name as your GitHub username to be able to provide an introduction to your GitHub page. Make sure to also pin repositories to your profile that can showcase your skillset in the right way.</p>
<p>*Note: I went ahead and did this right after Abigail’s talk. You can check out the result <a href="https://github.com/vsriram24">here</a> :)</p>
</section>
<section id="oops-im-a-manager---finding-your-minimal-viable-process---andrew-holz" class="level3">
<h3 class="anchored" data-anchor-id="oops-im-a-manager---finding-your-minimal-viable-process---andrew-holz">“Oops, I’m A Manager - Finding your Minimal Viable Process” - <a href="https://www.linkedin.com/in/andrewholz/">Andrew Holz</a></h3>
<p>What’s the minimal viable process (MVP) to being a manager? Let’s break these terms down with some layman’s definitions:</p>
<ul>
<li><p>P(rocess): how sh*t gets done!</p></li>
<li><p>V(iable): someone is paying the bills, so they expect the process to be <em>meaningful</em>.</p></li>
<li><p>M(inimal): heavy processes don’t promote progress - they paralyze it. Keep the process as lean as possible.</p></li>
</ul>
<p>We can distill this process into three phases: gather, do, and deliver</p>
<ol type="1">
<li>Gather</li>
</ol>
<ul>
<li>Intentionally consider all stakeholders and the level of input they can and should provide</li>
<li>Specify as much detail as the team needs, and no more (see https://tidyfirst.substack.com/p/responsible-slack)</li>
</ul>
<ol start="2" type="1">
<li>Do</li>
</ol>
<ul>
<li>Make sure you leave as much space as possible for the ‘who,’ ‘what,’ and ‘how’</li>
<li>Don’t make one person do the same thing over and over again. Also avoid dictating how they go about doing their tasks.</li>
<li>Lastly, be careful when you set up meetings - consider purpose and frequency as well as who needs to be there.
<ul>
<li>8 people in a meeting is too much - everyone will just be waiting for their turn to speak instead of actively communicating. A 4-person meeting is much better. The best kind of meeting? Context pairs where two individuals are fully aware of the context of their discussion.</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li>Delivery</li>
</ol>
<ul>
<li>Beyond shipping your product, other aspects of delivery are often neglected. First, delivering your product involves being able to tell a coherent story with your stakeholders. Second, make sure that your entire team gets credit for its wins. Finally, do your best to own your mistakes.</li>
</ul>
<p>Ultimately, being a good manager is a process of trial and error. There is no “right” process - there is only the “right NOW” process. Observe and reflect along with your team, and make changes to your process over time.</p>
</section>
<section id="some-notes-from-lightning-talks---eric-leung-ben-arancibia-claire-bai-luis-d.-verde-arregoitia-mika-braginsky-andrew-gard" class="level3">
<h3 class="anchored" data-anchor-id="some-notes-from-lightning-talks---eric-leung-ben-arancibia-claire-bai-luis-d.-verde-arregoitia-mika-braginsky-andrew-gard">Some notes from Lightning Talks - <a href="https://erictleung.com">Eric Leung</a>, <a href="https://www.linkedin.com/in/bcarancibia/">Ben Arancibia</a>, <a href="https://www.linkedin.com/in/claire-bai-388727122/">Claire Bai</a>, <a href="https://luisdva.github.io">Luis D. Verde Arregoitia</a>, <a href="https://mikabr.io">Mika Braginsky</a>, <a href="https://www.lakeforest.edu/academics/faculty/agard">Andrew Gard</a></h3>
<ul>
<li>When you’re faced with a new challenging task, don’t reinvent the wheel! Use the best (available) tool for the job</li>
<li>Companies like <a href="https://cotahealthcare.com">COTA Healthcare</a> are developing R packages like <code>rwnavigator</code> to facilitate outcomes analysis for cancer. The goal of such tasks is to incorporate medical expertise from oncologists and functionality to standardize and simplify code.</li>
<li>The ALARM project has developed an R package called “fifty-states” to simulate alternative congressional redistricting plans for all 50 states. You can learn more about their package <a href="https://alarm-redist.org/fifty-states/">here</a>.</li>
<li>Documenting your code… comment next to the packages you call and also comment above individual steps. Comment as little and as clearly as possible. Automate informative comments by leveraging built-in descriptions, checking code for package components, and examining comments in code.</li>
<li><a href="https://datapages.github.io">Datapages</a> are a great tool for findable, accessible, interoperable, reusable (FAIR), and interactive data sharing. Uploading a static <code>.csv</code> file is easy for you but hard for the audience. On the other hand, making a custom repository and website is easy for the audience but hard for you. Sharing a datapage gives you the best of both worlds and makes it easy to share your data analysis in a robust manner while communicating its impact to your audience.</li>
<li>Learning in the age of AI… even if we don’t need to code ourselves in the future, we need to have the right vocabulary to be able to tell the AI what to do. Instead of telling students what they should implement, have them analyze AI output and figure out what didn’t work and why it is missing.</li>
<li>Check out the TV show <em>The Expanse</em>!</li>
</ul>
</section>
<section id="please-let-me-merge-before-i-start-crying-and-other-things-ive-said-at-the-git-terminal---meghan-harris" class="level3">
<h3 class="anchored" data-anchor-id="please-let-me-merge-before-i-start-crying-and-other-things-ive-said-at-the-git-terminal---meghan-harris">“Please Let Me Merge Before I Start Crying: And Other Things I’ve Said at the Git Terminal” - <a href="https://github.com/Meghansaha">Meghan Harris</a></h3>
<p><a href="https://git-scm.com">Git</a> is not the same as <a href="https://github.com">GitHub</a>. Git is the version control system, while GitHub is the developer platform that uses the Git software. R users can interact with Git through:</p>
<ul>
<li><p>a command-line interface (CLI)</p></li>
<li><p>the RStudio graphical user interface (GUI)</p></li>
<li><p>a third-party UI (e.g.&nbsp;<a href="https://github.com/apps/desktop">GitHub Desktop</a>)</p></li>
</ul>
<p>Merging in git involves the joining of two or more development histories (also known as branches) together. Merging allows you to safely modify work when collaborating with others. A lot of people think they are scared of merges… the truth is, they’re really scared of merge conflicts! Merge conflicts occur when competing changes are made to the same line of a file (a content conflict) or someone edits a file and someone else deletes the file (a structure conflict).</p>
<p>Here are some tips to dealing with merge conflicts:</p>
<p>1. <strong>Don’t panic!</strong> You can use <code>git merge --abort</code> like a time machine to get you to where you were before. REmember, you are in control. You can choose which code to use.</p>
<p>2. <strong>Assess the damage.</strong> You can call <code>git status</code> to see what has happened. Don’t be scared, but be careful. It doesn’t matter how complicated the conflict is… the process is the same.</p>
<p>Remember, merge conflicts are not git problems. They are communication, workflow, or knowledge gap problems. When you are working, be thoughtful before, during, and after your coding sessions.</p>
<ul>
<li>Before you code: check your git environment and check the branch status. Always pull first before touching anything. Emergencies are not real!</li>
<li>While you code: commit often, push thoughtfully, and use git stashes when needed.</li>
<li>After you code: you are reviewer #1! Make your code as clear as can be.</li>
</ul>
<p>With respect to learning how to get better with git… do what you need to do, however you need to do it. Sometimes, all you really need is a bit more practice.</p>
</section>
<section id="keynote-session-a-future-of-data-science---allen-downey" class="level3">
<h3 class="anchored" data-anchor-id="keynote-session-a-future-of-data-science---allen-downey">“Keynote Session: A Future of Data Science” - <a href="https://www.allendowney.com/wp/">Allen Downey</a></h3>
<p>What does it mean for data work to be successful? It means that we have successfully answered a question of interest. Now how do we go about answering a question? We need the following things:</p>
<ul>
<li><p>data</p></li>
<li><p>a simple method / basic visualization</p></li>
<li><p>free software and tools for reliable science</p></li>
<li><p>a distribution system (e.g.&nbsp;a <a href="https://vsriram24.github.io/blog.html">blog</a>)</p></li>
</ul>
<p><img src="https://vsriram24.github.io/posts/posit-conf-24/statTest.png" class="img-fluid"></p>
<p>Data science involves the application of tools and processes to answer questions, resolve disagreements, and make better decisions. Data science has been on the <a href="https://en.wikipedia.org/wiki/Gartner_hype_cycle">Gartner hype cycle</a> for a while… where are we today with respect to this cycle for data science? The peak of inflated expectations happened around 2009 to 2012, while the trough of disillusionment happened from 2016 to 2018. Now, we’re starting to hit the plateau of productivity… and the reason for this is because we have not fully embraced <a href="https://en.wikipedia.org/wiki/Computational_statistics">computational statistics</a>.</p>
<p>What is the difference between mathematical statistics and computational statistics? Mathematical statistics can be thought of as the start of the field, while computational statistics can be considered its final evolution. Data science as a field exists because statistics missed the boat on computers. Topics like general purpose programming languages, machine learning, and more were not introduced into the statistical discipline early on enough.</p>
<p>Nevertheless, there are many reasons to be optimistic about the progression of statistics and data science. Not only do we have more and more available data, but we also have improved data literacy. However, data bias from the increased consumption of negative media is skewing the trajectory of data science progress downward. Ultimately, data itself is the antidote to this negativity bias. We can use data to understand the world better, so that we know how to make the world better! This is the ultimate benefit of open data!</p>
<p><img src="https://vsriram24.github.io/posts/posit-conf-24/openSci.png" class="img-fluid"></p>
</section>
<section id="uniquely-human-data-storytelling-in-the-age-of-ai---laura-gast" class="level3">
<h3 class="anchored" data-anchor-id="uniquely-human-data-storytelling-in-the-age-of-ai---laura-gast">“Uniquely Human: Data Storytelling in the Age of AI” - <a href="https://www.linkedin.com/in/laurabgast/">Laura Gast</a></h3>
<p>You have to speak for the data! The data will NOT speak for itself. You can do this with the four following modes of persuasion:</p>
<ul>
<li><p>Logos (logic): data and methods</p></li>
<li><p>Ethos (trust): credibility</p></li>
<li><p>Pathos (emotion): narrative</p></li>
<li><p>Kairos (time): audience and timing</p></li>
</ul>
<p>Context feeds story. Story feeds impact. Context includes the following three components: background, framing, and circumstance. Make sure to contextualize both the inputs and the outputs of your data story. Data visualization can be used to communicate scale of impact.</p>
<p>Story includes both an arc and a narrative. A story provides your audience with access. Your audience is not passive - you want them to be excited and rooting for the outcome of your analysis. At the same time, you must be careful in a world where “great stories” and things that aren’t true aren’t really the same thing. You want to make sure that you do not lose credibility.</p>
<p>Regarding impact: data is not information, information is not knowledge, and knowledge is not wisdom. Data does not act, and AI does not have intent. It thus becomes evident that <strong>humans cannot be cut out of the process</strong>. Remember that you will always have to speak for the data through context, story, and impact.</p>
<hr>
</section>
<section id="personal-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="personal-takeaways">Personal Takeaways</h3>
<p>I had a great time at my first posit::conf, and I look forward to attending more in the future. Here are a few key takeaways related to trends in the field of data science as well as ways that I could update my personal workflows:</p>
<ul>
<li><p>In the coming years, with platforms like <a href="https://github.com/posit-dev/positron">Positron</a> and <a href="https://github.com/r-wasm/quarto-live">Quarto Live</a>, we will see expanded ways that people can learn and practice data science across languages and disciplines.</p></li>
<li><p>GitHub should be used not just as a version control platform but as a way to share your projects with the public, as well as to learn from other people’s work. Learn how to use GitHub code search!</p></li>
<li><p><a href="https://duckdb.org">DuckDB</a> (and <a href="https://github.com/tidyverse/duckplyr">duckplyr</a>) is a simple and highly effective way of storing and manipulating data tables. Stop using <code>.csv</code>’s!</p></li>
<li><p>Generative AI tools like ChatGPT aren’t the end-all, be-all to data science work, but they can be extremely helpful tools for implementing new tools that have a high learning curve (e.g.&nbsp;<a href="https://developer.mozilla.org/en-US/docs/Web/CSS">CSS</a>)</p></li>
<li><p>Learning new concepts can be a challenging task, particularly when there are multiple ways to do a single thing. It’s often easiest to pick a single way of doing things and master it. Go with what works for you!</p></li>
<li><p>Regardless of how the field of AI continues to progress (whether the hype grows or shrinks), people will remain at the heart of data science. It is our responsibility as ethical data scientists to promote open sharing of data and tools, as well as to advocate for the data and tell the stories we wish to tell when communicating with stakeholders who are not as data literate.</p></li>
<li><p>Nothing is ever really an <u>emergency</u>. If someone’s life is on the line, then you can worry. Otherwise, there’s no need for you to stress as much as you currently are!</p></li>
</ul>
<hr>
<p>This concludes my summary of my experience at posit::conf 2024! I’d like to give a huge thank you to Posit for bringing not just the national but also international data science community together in such a fun, educational event. Until next time, [VS]Coders!</p>


</section>

 ]]></description>
  <category>Conferences</category>
  <guid>https://vsriram24.github.io/posts/posit-conf-24/</guid>
  <pubDate>Mon, 02 Sep 2024 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/posit-conf-24/pensiveZola.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>An intro to NLP for oncology</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/nlp-oncology-overview/</link>
  <description><![CDATA[ 





<p>In this week’s blog, I will be summarizing Yim et al’s 2016 review paper from <em>JAMA Oncology</em>, titled “Natural Language Processing in Oncology: A Review.” This paper was written by researchers at the University of Washington Medical Center, including individuals from the Departments of Biomedical and Health Informatics, the Department of Linguistics, the Division of Oncology at the Department of Medicine, and the Department of Radiology. You can read the abstract for this paper <a href="https://pubmed.ncbi.nlm.nih.gov/27124593/">here</a>.</p>
<p>The full text of this manuscript is unfortunately not publicly available (you or your institution must have access to <em>JAMA Oncology</em> to read the full text), so I hope that this summary provides those without access to this journal some insight into some exciting content. Furthermore, given that this paper is from 2016, it does not include many of the recent advances in the past 8 years related to transformer / large language models that have become more relevant in NLP applications. Nevertheless, I found this paper to be an excellent summary of how to frame the workflow for tackling NLP research questions, particularly in terms of the collaborations needed among oncologists and informaticians. Much of the content covered remains as relevant today as it did 8 years ago, and I look forward to using my takeaways from this material in <a href="https://hutchdatascience.org/tr-analytics/">my own work at the Fred Hutch Cancer Center</a>.</p>
<p>With context out of the way, let’s get started!</p>
<p><img src="https://vsriram24.github.io/posts/nlp-oncology-overview/paperAbstract.png" class="img-fluid"></p>
<hr>
<section id="oncology-focused-nlp" class="level2">
<h2 class="anchored" data-anchor-id="oncology-focused-nlp">Oncology-focused NLP</h2>
<p><strong>Natural Language Processing</strong> (NLP) refers to any computer-based algorithm that can handle, augment, and transform natural language so that it can be represented for computation.</p>
<p>NLP is particularly attractive for clinical research applications for the following reasons. One can:</p>
<ol type="1">
<li>define new variables that are not readily available in electronic health records as stored values</li>
<li>automate the process of reviewing clinical notes for patient diagnosis instead of requiring a resource-intensive manual review process.</li>
<li>expedite biomedical discovery by empowering clinicians with the ability to analyze outcomes in the context of big data</li>
</ol>
<p>With respect to the field of oncology, NLP applications typically have the following objectives:</p>
<ol type="1">
<li><strong>Case identification:</strong> determine which patients may have a case of the disease of interest
<ul>
<li>Here, we process the free text from clinical notes to augment the diagnosis codes that correspond to a patient’s labeled set of diseases or symptoms</li>
</ul></li>
<li><strong>Staging:</strong> determine the stage of a patient’s cancer progress
<ul>
<li>Here, we process the free text from clinical notes to determine the stage of a patient’s cancer progression. This objective is more challenging than case identification because it is highly context-specific depending on the type of cancer.</li>
</ul></li>
<li><strong>Outcome determination:</strong> determine the patient’s ultimate outcome (cancer advancement vs.&nbsp;remission vs.&nbsp;death)
<ul>
<li>Here, we process the free text from clinical notes to determine the patient’s ultimate outcome (advancement vs.&nbsp;remission vs.&nbsp;death). This objective is the most challenging of the three because it essentially requires us to perform cancer staging with respect to time.</li>
</ul></li>
</ol>
</section>
<section id="the-role-of-the-oncologist" class="level2">
<h2 class="anchored" data-anchor-id="the-role-of-the-oncologist">The role of the oncologist</h2>
<p>As with many objectives in the domain of biomedical informatics, developing NLP systems for oncology applications requires significant involvement from a domain expert.</p>
<p>An oncologist can contribute to the development of an NLP system in the following ways:</p>
<ul>
<li><strong>Project conception:</strong> using their domain expertise, an oncologist can devise practical and impactful applications of NLP</li>
<li><strong>Corpus annotation:</strong> an oncologist can assist with annotation of training data by providing the “correct answers” for a body of text based upon a set of annotation guidelines.</li>
<li><strong>System evaluation and error analysis:</strong> based on measures such as precision and recall, an oncologist can manually review true and false positives and negatives in the test data to help determine the strengths and weaknesses of the system</li>
</ul>
</section>
<section id="nlp-tasks-and-strategies-for-oncology" class="level2">
<h2 class="anchored" data-anchor-id="nlp-tasks-and-strategies-for-oncology">NLP tasks and strategies for oncology</h2>
<p>Within the context of oncology, NLP is typically used for <strong>Information Extraction</strong> (IE). IE refers to the transformation of unstructured data into a structured form. IE encompasses multiple subtasks, including <strong>Named Entity Recognition</strong> (NER), <strong>Relation Extraction</strong> (RE), <strong>Text Classification</strong>, and <strong>Template Extraction</strong>.</p>
<ul>
<li>NER involves grouping words in a text and assigning them to a pre-defined “concept</li>
<li>RE involves the assignment of relationships between entities</li>
<li>Text Classification assigns categorical label for a body of text</li>
<li>Template Extraction collects a set of related entities, relations, and labels to define a form for structured data.</li>
</ul>
<p>You can refer to figure 1 from the paper to see an example of clinical information extraction.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/nlp-oncology-overview/nlp_fig1.png" class="img-fluid figure-img"></p>
<figcaption>Fig 1 from the manuscript - an example of free-text processing, IE, and text classification</figcaption>
</figure>
</div>
<p>Strategies to tackle the above sub-tasks can be grouped into the following three buckets: rule-based, statistical, and hybrid approaches.</p>
<ul>
<li><strong>Rule-based</strong> approaches involve heuristic algorithms designed by domain specialists. They can be as simple as individual keyword look-ups or can be defined by complex conditional logic. When the defined rules are simple, rule-based approaches can significantly improve the interpretability of a model. However, if the rules are more complicated, the established model will lose interpretability and become harder to replicate or update.</li>
<li><strong>Statistical</strong> (a.k.a machine learning) systems are algorithms designed to statistically maximize the probability of finding the correct answer based on the distribution of the training data. Such models are less prone to overfitting compared to rule-based approaches, and they can be easily adapted to new data. However, these models require an extensive amount of unbiased, representative training data to work accurately.</li>
<li><strong>Hybrid</strong> approaches work to minimize the disadvantages of rule-based and statistical algorithms by combining these two methods.</li>
</ul>
</section>
<section id="resources-needed-for-clinical-nlp" class="level2">
<h2 class="anchored" data-anchor-id="resources-needed-for-clinical-nlp">Resources needed for clinical NLP</h2>
<p>Creating an NLP system requires the use of three common related NLP resources: extraction tools, ontologies, and corpora.</p>
<ul>
<li><p><strong>Extraction tools</strong> refer to developed subsystems that can be used on new datasets with little to no modifications. Typical clinical NLP pipelines can involve the following tasks:</p>
<ol type="1">
<li>Section Identification: identifying sections is particularly useful for clinical notes, where different sections will contain different types of content</li>
<li>Medical NER: this is a more specialized version of general English NER, taking into account acronyms, abbreviations, and synonyms that are common in medical text. Generally, out-of-the-box NER systems are usually insufficient, requiring the application of in-house rule-based and statistical NER systems. Two freely available development tools include MetaMap and the Mayo Clinical Text Analysis and Knowledge Extraction System</li>
<li>Negation detection: this helps distinguish the phrase “patient has fever” from “patient has no signs of fever.” This can be a complicated task depending on the language used in the notes.</li>
</ol>
<ul>
<li>Other common tasks in NLP pipelines include: coreference resolution, temporal classification, medication information extraction, family history extraction, assertion detection, and polarity detection. You can see some examples of these tasks in Figure 4 from the paper:</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/nlp-oncology-overview/nlp_fig4.png" class="img-fluid figure-img"></p>
<figcaption>Fig 4 from the manuscript - examples of common tasks in NLP pipelines</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Ontologies</strong> refer to knowledge bases that reference how various concepts are related to one another. The most basic form of an ontology could be a medical dictionary. A more complicated form of an ontology might involve a knowledge graph of concepts and their relationships.</p></li>
<li><p><strong>Corpora</strong> are collections of (sometimes annotated) clinical text that can be used to train or test an NLP system. Some examples of publicly available de-identified medical corpora including the i2b2 challenge sets, the Conference and Laboratories of the Evaluation Forum datasets, the MIMIC-II corpus, the MIPACQ corpus, and <a href="http://MTSamples.com">MTSamples.com</a>.</p></li>
</ul>
</section>
<section id="challenges-in-designing-an-nlp-system" class="level2">
<h2 class="anchored" data-anchor-id="challenges-in-designing-an-nlp-system">Challenges in designing an NLP system</h2>
<p>We can create a desired NLP system by framing our desired objective (case identification, staging, or outcome prediction) as a series of NLP operations. For instance, a basic case identification task may involve NER followed by negation detection. On the other hand, an outcome prediction task may involve a simple NER task or a more complicated algorithm that identifies tumor sizes and associated dates before performing logic operations to deduce that a change took place.</p>
<p>Ultimately, the final success of an NLP system will depend on two key points:</p>
<ol type="1">
<li><strong>how do you frame the problem at hand?</strong></li>
<li><strong>how effective are the individual components of your pipeline?</strong></li>
</ol>
<p>Multiple challenges can affect the efficacy of your model. Firstly, creation of an NLP system requires careful planning and investment to develop an annotated text corpus for system training and testing. Model performance can also be highly variable depending on the task at hand – new larger, heterogeneous test data can challenge the accuracy of your system. Multiple iterations of software development are usually needed before you can arrive at a robust NLP product. Lastly, the data science mantra of “garbage in, garbage out” continues to ring true – the tools that you develop will only be as good the quality of the data that train the systems.</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>This concludes my overview of “Natural Language Processing in Oncology: A Review.” I look forward to covering more such papers in the future as I progress with my work in clinical NLP at the Hutch.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://pubmed.ncbi.nlm.nih.gov/27124593/">“Natural Language Processing in Oncology: A Review”</a></li>
</ul>


</section>

 ]]></description>
  <category>Overviews</category>
  <guid>https://vsriram24.github.io/posts/nlp-oncology-overview/</guid>
  <pubDate>Mon, 19 Aug 2024 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/nlp-oncology-overview/minnesota.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Factor analysis for multiomics data with MOFA2</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/multiomics-fa/</link>
  <description><![CDATA[ 





<p>In this week’s blog, I’ll be providing a brief overview on multiomic data and the utility of factor analysis. I will also summarize a tutorial and case study developed by Alex Gurbych from blackthorn.ai that highlights how to perform factor analysis using the Bioconductor package <code>MOFA2</code>. You can review Alex’s original tutorial <a href="https://levelup.gitconnected.com/multi-omics-analysis-3857956a7a3d">here</a>. With context out of the way, let’s get started!</p>
<hr>
<section id="introduction" class="level1">
<h1>1. Introduction</h1>
<p>In the following workflow, we will derive biological insights from a multiomic dataset through the application of factor analysis with the <code>MOFA2</code> package. Before we get into our case study example, let’s go over some definitions.</p>
<section id="multiomics" class="level2">
<h2 class="anchored" data-anchor-id="multiomics">1.1. Multiomics</h2>
<p><strong>Multiomics</strong> refers to a biological analysis approach that considers multiple data modalities concurrently to study a biological system holistically. The individual data modalities of multiomic analysis, also known as <em>’omes</em>, are comprised of large-scale data and typically summarize the entire set of biomolecules for an organism. Some examples of individual ’omes include the genome, proteome, transcriptome, epigenome, and phenome.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/multiomics-fa/kimLabOmics.PNG" class="img-fluid figure-img"></p>
<figcaption>An overview of how multiomic data can lend itself to precision medicine applications - Dokyoon Kim Lab for Integrative ’Omics and Biomedical Informatics at the University of Pennsylvania</figcaption>
</figure>
</div>
<p>Multiomic studies can lend themselves to the improved characterization of biological processes across molecular layers. By integrating multiple ’omes together, researchers can analyze complex biological big data to find novel associations between biological entities, pinpoint relevant biomarkers, and build elaborate markers of disease and physiology.</p>
<p>Motivated by this improved ability to represent the underlying biology of a system, multiomic profiling has been increasingly applied across a variety of biological domains, including cancer biology, regulatory genomics, microbiology, and host‐pathogen biology. A common aim of these applications is to characterize heterogeneity between samples, as manifested in one or several of the data modalities. Multiomic profiling is particularly appealing if the relevant axes of variation are not known beforehand, and hence may be missed by studies that consider a single data modality or targeted approaches. One approach to capture the relevant axes of variation in a biological system is <em>factor analysis</em>.</p>
</section>
<section id="factor-analysis" class="level2">
<h2 class="anchored" data-anchor-id="factor-analysis">1.2. Factor analysis</h2>
<p><strong>Factor analysis</strong> is a statistical method that summarizes the variability across observed, correlated variables in a smaller set of variables. These newly generated variables correspond to the ‘factors’ of the original data - they are generated by determining linear combinations of the observed variables and adding small fixed deviations. Correlations between the factors of an input set of predictors and an output response variable can reveal previously unobserved latent axes of variation that affect the response of the data. Thus, the main goal of factor analysis is to identify hidden variables and evaluate their correlations with output variables of interest.</p>
</section>
<section id="mofa2" class="level2">
<h2 class="anchored" data-anchor-id="mofa2">1.3. <code>MOFA2</code></h2>
<p>Given the utility of factor analysis for the identification of relevant axes of variation in biological systems, it becomes particularly beneficial to have a simple method for integrating and analyzing multiomic data in this manner - <em>Multiomic Factor Analysis (MOFA)</em> offers one such option.</p>
<p><strong>MOFA</strong> is a probabilistic factor model that performs unsupervised integration of multiple modalities of omics data and discovers the principal axes of variation in multiomic data sets. Intuitively, MOFA can be viewed as a versatile and statistically rigorous generalization of principal component analysis (PCA) for multiomics data. MOFA infers a set of hidden factors that capture biological and technical sources of variability. It disentangles axes of heterogeneity that are shared across multiple modalities as well as those specific to individual data modalities. The learnt factors enable a variety of downstream analyses, including identification of sample subgroups, data imputation and the detection of outlier samples. The inferred factor loadings can be sparse, thereby facilitating the linkage between the factors and the most relevant molecular features. Importantly, MOFA disentangles to what extent each factor is unique to a single data modality or is manifested in multiple modalities, thereby revealing shared axes of variation between the different omics layers. Once trained, the model output can be used for a range of downstream analyses, including visualization, clustering, and classification of samples in the low‐dimensional spaces spanned by the factors.</p>
<p>You can read more about the <code>MOFA2</code> package on its Bioconducter documentation page <a href="https://www.bioconductor.org/packages/release/bioc/html/MOFA2.html">here</a>.</p>
</section>
</section>
<section id="dataset-description" class="level1">
<h1>2. Dataset description</h1>
<p>With background on multiomics and factor analysis covered, let’s jump into our case study: multiomics factor analysis for a dataset of cancer patients.</p>
<p>For our example dataset, we will make use of a cohort of 200 patients diagnosed with chronic lymphocytic leukemia (CLL). The CLL dataset can be downloaded for free from <a href="https://bioconductor.org/packages/release/data/experiment/html/BloodCancerMultiOmics2017.html">here</a>.</p>
<p>The CLL dataset we will be using consists of four data modalities:</p>
<ul>
<li><p>genomics (somatic mutations)</p></li>
<li><p>epigenomics (DNA methylation)</p></li>
<li><p>transcriptomics (RNA-seq)</p></li>
<li><p>phenotypes (drug response).</p></li>
</ul>
<p>Around 40% of the feature values are absent.</p>
<p>Let’s start by importing the required libraries:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">setwd</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"~/Documents/Developer/vsriram24.github.io/posts/multiomics-fa"</span>)</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(ggplot2)</span>
<span id="cb1-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(tidyverse)</span>
<span id="cb1-5"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(data.table)</span>
<span id="cb1-6"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(MOFA2)</span>
<span id="cb1-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(MOFAdata)</span></code></pre></div>
</div>
<p>Now, we import the CLL dataset. Data are stored as a list of matrices with features as rows and samples as columns. We can use the <code>lapply</code> function to get the number of features and samples for each “’ome” in our data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">utils<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CLL_data"</span>)</span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lapply</span>(</span>
<span id="cb2-4">  CLL_data, </span>
<span id="cb2-5">  dim</span>
<span id="cb2-6">)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$Drugs
[1] 310 200

$Methylation
[1] 4248  200

$mRNA
[1] 5000  200

$Mutations
[1]  69 200</code></pre>
</div>
</div>
</section>
<section id="model-training" class="level1">
<h1>3. Model training</h1>
<p>With our data uploaded into our environment, we can begin to fit a model. We start by creating a new <code>MOFA</code> model for our data using the <code>create_mofa</code> function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">MOFAobject <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">create_mofa</span>(CLL_data)</span></code></pre></div>
</div>
<p>We can visualize the number of features and samples in our data as well as missing values using the <code>plot_data_overview</code> function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot_data_overview</span>(MOFAobject)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/multiomics-fa/index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>In the above figure, <code>D</code> tells us the number of features for each data modality, while <code>N</code> represents the number of samples. Missing data are represented by gray bars.</p>
<p>With our model initialized, we can set various options to customize the way that we train it on our data. In our case, we will specify that we want to generate 15 factors, and that we want to set the maximum number of model training iterations to be 100.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">data_opts <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">get_default_data_options</span>(MOFAobject)</span>
<span id="cb6-2"></span>
<span id="cb6-3">model_opts <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">get_default_model_options</span>(MOFAobject) </span>
<span id="cb6-4">model_opts<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>num_factors <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span></span>
<span id="cb6-5"></span>
<span id="cb6-6">train_opts <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">get_default_training_options</span>(MOFAobject)</span>
<span id="cb6-7">train_opts<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>seed <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb6-8">train_opts<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>save_interrupted <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span></span>
<span id="cb6-9">train_opts<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>maxiter <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb6-10">train_opts<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>convergence_mode <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"slow"</span></span></code></pre></div>
</div>
<p>Finally, with our options specified, we can go ahead and train our model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1">MOFAobject <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prepare_mofa</span>(</span>
<span id="cb7-2">  MOFAobject,</span>
<span id="cb7-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">training_options =</span> train_opts,</span>
<span id="cb7-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model_options =</span> model_opts,</span>
<span id="cb7-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data_options =</span> data_opts</span>
<span id="cb7-6">)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Checking data options...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Checking training options...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Checking model options...</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1">MOFAobject_trained <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">run_mofa</span>(</span>
<span id="cb11-2">  MOFAobject,</span>
<span id="cb11-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">outfile =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MOFA2_CLL_trained.hdf5"</span></span>
<span id="cb11-4">)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Output file MOFA2_CLL_trained.hdf5 already exists, it will be replaced</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Connecting to the mofapy2 python package using reticulate (use_basilisk = FALSE)... 
    Please make sure to manually specify the right python binary when loading R with reticulate::use_python(..., force=TRUE) or the right conda environment with reticulate::use_condaenv(..., force=TRUE)
    If you prefer to let us automatically install a conda environment with 'mofapy2' installed using the 'basilisk' package, please use the argument 'use_basilisk = TRUE'</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
        #########################################################
        ###           __  __  ____  ______                    ### 
        ###          |  \/  |/ __ \|  ____/\    _             ### 
        ###          | \  / | |  | | |__ /  \ _| |_           ### 
        ###          | |\/| | |  | |  __/ /\ \_   _|          ###
        ###          | |  | | |__| | | / ____ \|_|            ###
        ###          |_|  |_|\____/|_|/_/    \_\              ###
        ###                                                   ### 
        ######################################################### 
       
 
        
use_float32 set to True: replacing float64 arrays by float32 arrays to speed up computations...

Successfully loaded view='Drugs' group='group1' with N=200 samples and D=310 features...
Successfully loaded view='Methylation' group='group1' with N=200 samples and D=4248 features...
Successfully loaded view='mRNA' group='group1' with N=200 samples and D=5000 features...
Successfully loaded view='Mutations' group='group1' with N=200 samples and D=69 features...


Model options:
- Automatic Relevance Determination prior on the factors: False
- Automatic Relevance Determination prior on the weights: True
- Spike-and-slab prior on the factors: False
- Spike-and-slab prior on the weights: False
Likelihoods:
- View 0 (Drugs): gaussian
- View 1 (Methylation): gaussian
- View 2 (mRNA): gaussian
- View 3 (Mutations): gaussian




######################################
## Training the model with seed 1 ##
######################################


ELBO before training: -19615100.54 

Iteration 1: time=0.58, ELBO=-3087652.10, deltaELBO=16527448.442 (84.25880055%), Factors=15
Iteration 2: time=0.55, Factors=15
Iteration 3: time=0.52, Factors=15
Iteration 4: time=0.56, Factors=15
Iteration 5: time=0.50, Factors=15
Iteration 6: time=0.42, ELBO=-2873482.21, deltaELBO=214169.888 (1.09186230%), Factors=15
Iteration 7: time=0.44, Factors=15
Iteration 8: time=0.49, Factors=15
Iteration 9: time=0.47, Factors=15
Iteration 10: time=0.40, Factors=15
Iteration 11: time=0.54, ELBO=-2867923.23, deltaELBO=5558.977 (0.02834029%), Factors=15
Iteration 12: time=0.47, Factors=15
Iteration 13: time=0.41, Factors=15
Iteration 14: time=0.39, Factors=15
Iteration 15: time=0.53, Factors=15
Iteration 16: time=0.50, ELBO=-2865486.12, deltaELBO=2437.114 (0.01242468%), Factors=15
Iteration 17: time=0.63, Factors=15
Iteration 18: time=0.97, Factors=15
Iteration 19: time=1.05, Factors=15
Iteration 20: time=0.54, Factors=15
Iteration 21: time=0.53, ELBO=-2863390.64, deltaELBO=2095.482 (0.01068300%), Factors=15
Iteration 22: time=0.46, Factors=15
Iteration 23: time=0.44, Factors=15
Iteration 24: time=0.59, Factors=15
Iteration 25: time=0.46, Factors=15
Iteration 26: time=0.46, ELBO=-2862445.15, deltaELBO=945.491 (0.00482022%), Factors=15
Iteration 27: time=0.49, Factors=15
Iteration 28: time=0.55, Factors=15
Iteration 29: time=0.58, Factors=15
Iteration 30: time=0.38, Factors=15
Iteration 31: time=0.31, ELBO=-2861818.14, deltaELBO=627.006 (0.00319655%), Factors=15
Iteration 32: time=0.39, Factors=15
Iteration 33: time=0.31, Factors=15
Iteration 34: time=0.34, Factors=15
Iteration 35: time=0.46, Factors=15
Iteration 36: time=0.41, ELBO=-2861358.43, deltaELBO=459.708 (0.00234364%), Factors=15
Iteration 37: time=0.54, Factors=15
Iteration 38: time=0.32, Factors=15
Iteration 39: time=0.36, Factors=15
Iteration 40: time=0.30, Factors=15
Iteration 41: time=0.45, ELBO=-2860993.31, deltaELBO=365.123 (0.00186144%), Factors=15
Iteration 42: time=0.28, Factors=15
Iteration 43: time=0.29, Factors=15
Iteration 44: time=0.30, Factors=15
Iteration 45: time=0.30, Factors=15
Iteration 46: time=0.25, ELBO=-2860681.72, deltaELBO=311.589 (0.00158852%), Factors=15
Iteration 47: time=0.25, Factors=15
Iteration 48: time=0.24, Factors=15
Iteration 49: time=0.28, Factors=15
Iteration 50: time=0.26, Factors=15
Iteration 51: time=0.27, ELBO=-2860392.99, deltaELBO=288.726 (0.00147196%), Factors=15
Iteration 52: time=0.29, Factors=15
Iteration 53: time=0.26, Factors=15
Iteration 54: time=0.26, Factors=15
Iteration 55: time=0.26, Factors=15
Iteration 56: time=0.30, ELBO=-2860101.84, deltaELBO=291.150 (0.00148432%), Factors=15
Iteration 57: time=0.29, Factors=15
Iteration 58: time=0.25, Factors=15
Iteration 59: time=0.29, Factors=15
Iteration 60: time=0.39, Factors=15
Iteration 61: time=0.26, ELBO=-2859789.25, deltaELBO=312.592 (0.00159363%), Factors=15
Iteration 62: time=0.26, Factors=15
Iteration 63: time=0.28, Factors=15
Iteration 64: time=0.28, Factors=15
Iteration 65: time=0.23, Factors=15
Iteration 66: time=0.29, ELBO=-2859453.86, deltaELBO=335.396 (0.00170989%), Factors=15
Iteration 67: time=0.27, Factors=15
Iteration 68: time=0.25, Factors=15
Iteration 69: time=0.25, Factors=15
Iteration 70: time=0.24, Factors=15
Iteration 71: time=0.28, ELBO=-2859111.11, deltaELBO=342.743 (0.00174734%), Factors=15
Iteration 72: time=0.26, Factors=15
Iteration 73: time=0.25, Factors=15
Iteration 74: time=0.28, Factors=15
Iteration 75: time=0.27, Factors=15
Iteration 76: time=0.26, ELBO=-2858782.22, deltaELBO=328.898 (0.00167676%), Factors=15
Iteration 77: time=0.25, Factors=15
Iteration 78: time=0.28, Factors=15
Iteration 79: time=0.31, Factors=15
Iteration 80: time=0.29, Factors=15
Iteration 81: time=0.29, ELBO=-2858489.40, deltaELBO=292.820 (0.00149283%), Factors=15
Iteration 82: time=0.29, Factors=15
Iteration 83: time=0.28, Factors=15
Iteration 84: time=0.28, Factors=15
Iteration 85: time=0.27, Factors=15
Iteration 86: time=0.33, ELBO=-2858247.17, deltaELBO=242.225 (0.00123489%), Factors=15
Iteration 87: time=0.28, Factors=15
Iteration 88: time=0.25, Factors=15
Iteration 89: time=0.33, Factors=15
Iteration 90: time=0.26, Factors=15
Iteration 91: time=0.28, ELBO=-2858055.75, deltaELBO=191.417 (0.00097587%), Factors=15
Iteration 92: time=0.29, Factors=15
Iteration 93: time=0.26, Factors=15
Iteration 94: time=0.26, Factors=15
Iteration 95: time=0.27, Factors=15
Iteration 96: time=0.32, ELBO=-2857907.84, deltaELBO=147.911 (0.00075407%), Factors=15
Iteration 97: time=0.26, Factors=15
Iteration 98: time=0.25, Factors=15
Iteration 99: time=0.27, Factors=15


#######################
## Training finished ##
#######################


Warning: Output file MOFA2_CLL_trained.hdf5 already exists, it will be replaced
Saving model in MOFA2_CLL_trained.hdf5...</code></pre>
</div>
</div>
<p>Our model has been successfully trained!</p>
</section>
<section id="postprocessing" class="level1">
<h1>4. Postprocessing</h1>
<section id="add-sample-metadata" class="level2">
<h2 class="anchored" data-anchor-id="add-sample-metadata">4.1. Add sample metadata</h2>
<p>Now that we have trained our model on our data, we can incorporate metadata from the input dataset to evaluate correlations between our different ’omes and additional variables. Let’s get data for age, sex, death status, and treatment status:</p>
<ul>
<li><p>Age: age in years</p></li>
<li><p>Died: (T/F) did the patient die?</p></li>
<li><p>Sex: (M/F)</p></li>
<li><p>treatedAfter: (T/F) was the patient treated after?</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load sample metadata</span></span>
<span id="cb15-2">CLL_metadata <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fread</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ftp://ftp.ebi.ac.uk/pub/databases/mofa/cll_vignette/sample_metadata.txt"</span>)</span>
<span id="cb15-3"></span>
<span id="cb15-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add sample metadata to the model</span></span>
<span id="cb15-5"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">samples_metadata</span>(MOFAobject_trained) <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> CLL_metadata</span></code></pre></div>
</div>
</section>
<section id="correlation-analysis-of-factors" class="level2">
<h2 class="anchored" data-anchor-id="correlation-analysis-of-factors">4.2. Correlation analysis of factors</h2>
<p>In order for our model to work accurately, we must ensure that the factors that we have generated are not correlated with one another. If we observe significant correlations between factors, we either used too many factors or performed and insufficient amount of normalization. We can visualize correlations across factors using the <code>plot_factor_cor</code> function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot_factor_cor</span>(MOFAobject_trained)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/multiomics-fa/index_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We can see from our figure that our generated factors show no significant correlations with one another.</p>
</section>
</section>
<section id="variance-breakdown" class="level1">
<h1>5. Variance Breakdown</h1>
<section id="explained-variance-decomposition-by-factor" class="level2">
<h2 class="anchored" data-anchor-id="explained-variance-decomposition-by-factor">5.1. Explained variance decomposition by factor</h2>
<p>We can use the <code>plot_variance_explained</code> function to determine how much of the variance in each of our individual data modalities is explained by each of our factors:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot_variance_explained</span>(</span>
<span id="cb17-2">  MOFAobject_trained, </span>
<span id="cb17-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">max_r2 =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span></span>
<span id="cb17-4">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/multiomics-fa/index_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>From our explained variance plot, we can see that Factor 1 explains a high amount of variance across all four of our data modalities. Factor 2 explained a high amount of variance for drug response, Factor 3 explains a high amount of variance for drug response, transcriptomics, and genomics, and Factor 4 explains a high amount of variance for transcriptomics.</p>
</section>
<section id="explained-variance-per-omic-modality" class="level2">
<h2 class="anchored" data-anchor-id="explained-variance-per-omic-modality">5.2. Explained variance per omic modality</h2>
<p>We can also use the <code>plot_variance_explained</code> function to determine the amount of variance that is explained across our four data modalities given all 15 factors in our model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot_variance_explained</span>(</span>
<span id="cb18-2">  MOFAobject_trained,</span>
<span id="cb18-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">plot_total =</span> T</span>
<span id="cb18-4">)[[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]]</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/multiomics-fa/index_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Here, we can see that the phenotypic and transcriptomic modalities have more variance explained by our model than the genomic or epigenomic modalities.</p>
</section>
</section>
<section id="factor-association-analysis" class="level1">
<h1>6. Factor association analysis</h1>
<p>We can now compare our generated factors to metadata from our input data to see if we capture any associations with variables related to CLL. Here, we generate correlation plots using the <code>correlate_factors_with_covariates</code> function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">correlate_factors_with_covariates</span>(</span>
<span id="cb19-2">  MOFAobject_trained, </span>
<span id="cb19-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">covariates =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Gender"</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"died"</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"age"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"treatedAfter"</span>), </span>
<span id="cb19-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">plot =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"log_pval"</span></span>
<span id="cb19-5">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/multiomics-fa/index_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>It appears that most of our factors have an association with at least one of our covariates under consideration.</p>
</section>
<section id="zooming-in-on-an-individual-factor" class="level1">
<h1>7. Zooming in on an individual factor</h1>
<section id="factor-values" class="level2">
<h2 class="anchored" data-anchor-id="factor-values">7.1. Factor values</h2>
<p>We can make use of the <code>plot_factor</code> function to evaluate how the data points in our dataset are distributed with respect to a factor of choice.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot_factor</span>(</span>
<span id="cb20-2">  MOFAobject_trained,</span>
<span id="cb20-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dodge =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>,</span>
<span id="cb20-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">add_violin =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>,</span>
<span id="cb20-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">factors=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>)</span>
<span id="cb20-6">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/multiomics-fa/index_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="factor-1s-association-with-the-genomic-data-modality" class="level2">
<h2 class="anchored" data-anchor-id="factor-1s-association-with-the-genomic-data-modality">7.2. Factor 1’s association with the genomic data modality</h2>
<p>We can use the <code>plot_weights</code> function to determine how different factors are weighted with respect to the features in our data. In the following graph, we focus on the influence of Factor 1 on features present in the genomic data modality. Features with a higher weight represent a stronger association with the factor of interest.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot_weights</span>(</span>
<span id="cb21-2">  MOFAobject_trained,</span>
<span id="cb21-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">view =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Mutations"</span>,</span>
<span id="cb21-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">factor=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb21-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">nfeatures =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,</span>
<span id="cb21-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">scale =</span> T</span>
<span id="cb21-7">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/multiomics-fa/index_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>From this graph, we can see that most features have a weight of 0 with respect to Factor 1, indicating a lack of association with the factor. However, IGHV (immunoglobulin heavy chain variable) has a weight close to 1 with respect to Factor 1. Indeed, mutations in the genetic region are a main clinical marker for CLL!</p>
<p>The <code>plot_top_weights</code> function lets us visualize this same output, sorted by absolute value of the weight:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot_top_weights</span>(</span>
<span id="cb22-2">  MOFAobject_trained,</span>
<span id="cb22-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">view =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Mutations"</span>,</span>
<span id="cb22-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">nfeatures =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,</span>
<span id="cb22-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">scale =</span> T,</span>
<span id="cb22-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">factor=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb22-7">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/multiomics-fa/index_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Once again, we can see that the weight for IGHV with respect to Factor 1 far exceeds the weights for any other genomic features in our dataset.</p>
<p>Based on these results, we’d expect to see that samples that have a high positive value for Factor 1 will have IGHV mutations. To confirm this hypothesis, let’s plot the distribution of factor 1 values, colored by IGHV mutation status:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot_factor</span>(</span>
<span id="cb23-2">  MOFAobject_trained,</span>
<span id="cb23-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dodge =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>,</span>
<span id="cb23-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">add_violin =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>,</span>
<span id="cb23-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color_by =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"IGHV"</span>,</span>
<span id="cb23-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">factors =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb23-7">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/multiomics-fa/index_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>It does seem like our Factor 1 values correlate with presence/absence of IGHV mutation!</p>
</section>
<section id="factor-1s-association-with-the-transcriptomic-data-modality" class="level2">
<h2 class="anchored" data-anchor-id="factor-1s-association-with-the-transcriptomic-data-modality">7.3. Factor 1’s association with the transcriptomic data modality</h2>
<p>From the variance explained plot we know that Factor 1 drives variation across all four of our data modalities. Let’s visualize the mRNA expression changes that are associated with Factor 1 using the <code>plot_weights</code> function again:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot_weights</span>(</span>
<span id="cb24-2">  MOFAobject_trained,</span>
<span id="cb24-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">nfeatures =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb24-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">view =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mRNA"</span>,</span>
<span id="cb24-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">factor =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb24-6">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/multiomics-fa/index_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We can see from our generated plot that there are a substantial number of transcripts with weights close to -1 or 1 with respect to Factor 1. It is likely that genes with large positive mRNA expression values are more heavily expressed in samples with IGHV mutation. Let’s verify this assumption.</p>
</section>
<section id="molecular-signature-clustering-for-factor-1" class="level2">
<h2 class="anchored" data-anchor-id="molecular-signature-clustering-for-factor-1">7.4. Molecular signature clustering for factor 1</h2>
<p>Let’s use the <code>plot_data_heatmap</code> function to generate a heatmap of gene expression values against Factor 1 values. Furthermore, we color our heatmap by IGHV mutation status.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot_data_heatmap</span>(</span>
<span id="cb25-2">  MOFAobject_trained,</span>
<span id="cb25-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">scale =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"row"</span>,</span>
<span id="cb25-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">cluster_cols =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">FALSE</span>,</span>
<span id="cb25-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">cluster_rows =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">FALSE</span>,</span>
<span id="cb25-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">show_colnames =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">FALSE</span>,</span>
<span id="cb25-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">denoise =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>,</span>
<span id="cb25-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">features =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>,</span>
<span id="cb25-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">view =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mRNA"</span>,</span>
<span id="cb25-10">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">factor=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb25-11">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/multiomics-fa/index_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>From our heatmap, we can see that various transcripts correlate with high Factor 1 value, and that these samples are also positive for IGHV status - our assumption is confirmed!</p>
</section>
</section>
<section id="summary" class="level1">
<h1>8. Summary</h1>
<p>This concludes my walkthrough of applying the <code>MOFA2</code> package to perform factor analysis on multiomics data. Here, we applied factor analysis to multiomics data to integrate and analyze relevant features related to different ’omic subtypes as well as patient metadata of interest. A big thank you again to Alex Gurbych from blackthorn.ai and the authors of the <code>MOFA2</code> package. I look forward to covering more tutorials involving multiomic data analysis and precision medicine in the future.</p>
</section>
<section id="references" class="level1">
<h1>9. References</h1>
<ul>
<li><a href="https://levelup.gitconnected.com/multi-omics-analysis-3857956a7a3d">“Multi-Omics Data Factor Analysis, by Alex Gurbych”</a></li>
<li><a href="https://www.bioconductor.org/packages/release/bioc/html/MOFA2.html">Bioconductor MOFA2 documentation</a></li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6010767/">“Multi‐Omics Factor Analysis—a framework for unsupervised integration of multi‐omics data sets”, by Argelaguet et al.</a></li>
<li><a href="https://biofam.github.io/MOFA2/">MOFA Homepage</a></li>
<li><a href="https://www.biomedinfolab.com">Dokyoon Kim Lab Homepage</a></li>
</ul>


</section>

 ]]></description>
  <category>Overviews</category>
  <category>Tutorials</category>
  <guid>https://vsriram24.github.io/posts/multiomics-fa/</guid>
  <pubDate>Mon, 12 Aug 2024 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/multiomics-fa/googleLens.PNG" medium="image"/>
</item>
<item>
  <title>Obliteride 2024</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/obliteride-24/</link>
  <description><![CDATA[ 





<p>Welcome back to another week of <em>[VS]Codes</em>! I currently have some longer tutorials and overviews cooking in the background, which I will be sharing in the coming weeks. In the meantime, I’d like to use this week’s post to share some information about an upcoming fundraising event in which I will be participating.</p>
<hr>
<p>This coming weekend, I will be taking part in a 25-mile bike ride as a part of <a href="https://www.obliteride.org">Fred Hutch Obliteride</a>. Obliteride is an annual event that raises money for cancer research at Fred Hutch. Since 2013, more than 27,000 participants and volunteers, along with sponsors and over 104,000 Obliteride donors, have raised more than $48 million for breakthrough work at Fred Hutch.</p>
<p><img src="https://vsriram24.github.io/posts/obliteride-24/images/12907812_web1_M-Riders-finish.jpg" class="img-fluid"></p>
<p>Here are a few examples of work that was supported at Fred Hutch by Obliteride in just the past year:</p>
<ul>
<li><p>Research and clinical advances for a variety of cancers and disease areas, including breast, colon, head and neck, and prostate cancers.</p></li>
<li><p>Partnerships to increase health equity and reduce disparities in labs, clinics, classrooms, and more.</p></li>
<li><p>Advances in patient care, including diagnostics, surgery, and follow-up.</p></li>
</ul>
<p><strong>My participation in Obliteride 2024 is focused on amplifying the impact of data science across Fred Hutch.</strong> My team (the <a href="https://hutchdatascience.org">Fred Hutch Data Science Lab</a>) focuses on getting both data and tools into the hands of researchers at Fred Hutch through the creation of centralized data resources and advanced models and workflows for simplified data analysis, as well as the development of comprehensive training resources and communities of practice across a variety of subjects in biomedical data science.</p>
<p>This year, with the support of our donors, <strong>we aim to accelerate the next discoveries that the Fred Hutch will make via expanded data access, advanced technology, and artificial intelligence</strong>. Any donations toward my profile or that of my team’s will directly facilitate the research conducted by groups at Fred Hutch who strive to advance their work by engaging with our data science and AI services.</p>
<p>My team and I are a part of Obliteride because we want to help Fred Hutch overcome cancer once and for all. Please consider sponsoring me <a href="https://secure.fredhutch.org/site/TR/?px=2167539&amp;fr_id=2070&amp;pg=personal&amp;success=true">here</a> - every donation helps!</p>
<p>Until next week, [VS]Coders!</p>



 ]]></description>
  <category>Miscellaneous</category>
  <guid>https://vsriram24.github.io/posts/obliteride-24/</guid>
  <pubDate>Mon, 05 Aug 2024 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/obliteride-24/zolaTongue.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Introduction to PyTorch</title>
  <dc:creator>Vivek Sriram</dc:creator>
  <link>https://vsriram24.github.io/posts/pytorch-tutorial/</link>
  <description><![CDATA[ 





<p>In today’s blog post, we’ll go through the <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">“Introduction to PyTorch</a>” tutorial available from the PyTorch’s online <a href="https://pytorch.org/tutorials/index.html">learning community</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/pytorch-tutorial/pytorch.png" class="img-fluid figure-img"></p>
<figcaption>A screen-grab of the PyTorch introductory documentation</figcaption>
</figure>
</div>
<p>This tutorial is designed to walk through every component you would need to start developing models in PyTorch. With that, many of the elements show you how to complete certain steps in multiple ways… In this walkthrough, I will be selecting a subset of elements from these components and streamlining the steps of data processing and model training to showcase exactly how one would use these tools for an example dataset.</p>
<p>In a future blog post, I will take the foundation developed through this walkthrough and explore a modifiedd data analysis workflow, with a more complicated neural network and additional hyperparameter training for a different set of data.</p>
<p>With background out of the way, let’s get started~</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><span class="in" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">```{r}</span></span>
<span id="cb1-2"><span class="in" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">library(reticulate)</span></span>
<span id="cb1-3"><span class="in" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">use_python('/opt/anaconda3/bin/python')</span></span>
<span id="cb1-4"><span class="in" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">```</span></span></code></pre></div>
<p>Most machine learning workflows involve uploading data, creating models, optimizing model parameters, and performing predictions on input data. This tutorial introduces you to a complete ML workflow implemented in PyTorch.</p>
<p>In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters. Tensors are a specialized data structure that are very similar to arrays and matrices.</p>
<div id="58dcd33a" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Import required packages</span></span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span></code></pre></div>
</div>
<div id="a0abb367" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> nn</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.utils.data <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Dataset</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.utils.data <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DataLoader</span></code></pre></div>
</div>
<div id="75f4655a" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#import torchvision</span></span>
<span id="cb4-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchvision <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> datasets, transforms</span>
<span id="cb4-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchvision.transforms <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ToTensor, Lambda</span>
<span id="cb4-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchvision.io <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> read_image</span></code></pre></div>
</div>
<p>We start by checking to see if we can train our model on a hardware accelerator like the GPU or MPS if available. Otherwise, we’ll use the CPU.</p>
<div id="c306642b" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get cpu, gpu or mps device for training.</span></span>
<span id="cb5-2">device <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb5-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span></span>
<span id="cb5-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> torch.cuda.is_available()</span>
<span id="cb5-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mps"</span></span>
<span id="cb5-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> torch.backends.mps.is_available()</span>
<span id="cb5-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cpu"</span></span>
<span id="cb5-8">)</span>
<span id="cb5-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Using </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>device<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> device"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using mps device</code></pre>
</div>
</div>
<p>Code for processing data samples can get messy and hard to maintain. We ideally want our dataset code to be de-coupled from model training code for better readability and modularity. We will break our analysis into the following two sections: <strong>Data</strong> and <strong>Modeling</strong>.</p>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<section id="importing-and-transforming-the-data" class="level3">
<h3 class="anchored" data-anchor-id="importing-and-transforming-the-data">Importing and transforming the data</h3>
<p>PyTorch offers domain-specific libraries such as <code>TorchText</code>, <code>TorchVision</code>, and <code>TorchAudio</code>. In this tutorial, we will be using a <code>TorchVision</code> dataset. The <code>torchvision.datasets</code> module contains <code>Dataset</code> objects for many real-world vision data. Here, we will use the FashionMNIST dataset. Fashion-MNIST is a dataset of images consisting of 60,000 training examples and 10,000 test examples. Each example includes a 28x28 grayscale image and an associated label from one of 10 classes.</p>
<p>Each PyTorch <code>Dataset</code> stores a set of samples and their corresponding labels. Data do not always come in the final processed form that is required for training ML algorithms. We use <em>transforms</em> to perform some manipulation of the data and make it suitable for training. Every TorchVision <code>Dataset</code> includes 2 arguments: <code>transform</code> to modify the samples and <code>target_transform</code> to modify the labels.</p>
<p>The FashionMNIST features are in PIL Image format. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. To make this transformation, we use the <code>ToTensor()</code> function. <code>ToTensor()</code> converts a PIL image or NumPy <code>ndarray</code> into a <code>FloatTensor</code> and scales the image’s pixel intensity values in the range [0., 1.]</p>
<div id="0a1ab475" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Download training data from open datasets.</span></span>
<span id="cb7-2">training_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datasets.FashionMNIST(</span>
<span id="cb7-3">    root<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"data"</span>,</span>
<span id="cb7-4">    train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb7-5">    download<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb7-6">    transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ToTensor()</span>
<span id="cb7-7">)</span>
<span id="cb7-8"></span>
<span id="cb7-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Download test data from open datasets.</span></span>
<span id="cb7-10">test_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datasets.FashionMNIST(</span>
<span id="cb7-11">    root<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"data"</span>,</span>
<span id="cb7-12">    train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>,</span>
<span id="cb7-13">    download<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb7-14">    transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ToTensor()</span>
<span id="cb7-15">)</span></code></pre></div>
</div>
<p>We now have our data uploaded! We can index an input <code>Dataset</code> manually (i.e.&nbsp;<code>training_data[index]</code>) to get individual samples. Here, we use <code>matplotlib</code> to visualize some samples in our training data.</p>
<div id="0ea7bd1a" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">labels_map <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb8-2">    <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"T-Shirt"</span>,</span>
<span id="cb8-3">    <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Trouser"</span>,</span>
<span id="cb8-4">    <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Pullover"</span>,</span>
<span id="cb8-5">    <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Dress"</span>,</span>
<span id="cb8-6">    <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Coat"</span>,</span>
<span id="cb8-7">    <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sandal"</span>,</span>
<span id="cb8-8">    <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Shirt"</span>,</span>
<span id="cb8-9">    <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sneaker"</span>,</span>
<span id="cb8-10">    <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Bag"</span>,</span>
<span id="cb8-11">    <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Ankle Boot"</span>,</span>
<span id="cb8-12">}</span>
<span id="cb8-13"></span>
<span id="cb8-14">figure <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb8-15">cols, rows <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb8-16"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> rows <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb8-17">    sample_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randint(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(training_data), size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,)).item()</span>
<span id="cb8-18">    img, label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> training_data[sample_idx]</span>
<span id="cb8-19">    figure.add_subplot(rows, cols, i)</span>
<span id="cb8-20">    plt.title(labels_map[label])</span>
<span id="cb8-21">    plt.axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"off"</span>)</span>
<span id="cb8-22">    plt.imshow(img.squeeze(), cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gray"</span>)</span>
<span id="cb8-23">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/pytorch-tutorial/index_files/figure-html/cell-7-output-1.png" width="613" height="631" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="preparing-data-for-training-with-dataloaders" class="level3">
<h3 class="anchored" data-anchor-id="preparing-data-for-training-with-dataloaders">Preparing data for training with <code>DataLoaders</code></h3>
<p>While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Pythonic multiprocessing to speed up data retrieval. <code>DataLoader</code> is an iterable that abstracts this complexity for us in an easy API. We pass our input <code>Dataset</code> as an argument to <code>DataLoader</code>. This wraps an iterable over our dataset, supporting automatic batching, sampling, shuffling, and multiprocess data loading in the process.</p>
<p>Here we define a batch size of 64 - each element in the <code>DataLoader</code> iterable will return a batch of 64 features and labels.</p>
<div id="4d5c6ec1" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span></span>
<span id="cb9-2"></span>
<span id="cb9-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create data loaders.</span></span>
<span id="cb9-4">train_dataloader <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataLoader(training_data, batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batch_size, shuffle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb9-5">test_dataloader <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataLoader(test_data, batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batch_size, shuffle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb9-6"></span>
<span id="cb9-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> X, y <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> test_dataloader:</span>
<span id="cb9-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shape of X [N, C, H, W]: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>X<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb9-9">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shape of y: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>y<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>y<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>dtype<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb9-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])
Shape of y: torch.Size([64]) torch.int64</code></pre>
</div>
</div>
</section>
<section id="iterating-over-the-dataloader" class="level3">
<h3 class="anchored" data-anchor-id="iterating-over-the-dataloader">Iterating over the <code>DataLoader</code></h3>
<p>Now that we have loaded our data into a <code>DataLoader</code>, we can iterate through the dataset as needed (using <code>next(iter(DataLoader)</code>). Each iteration returns a batch of <code>train_features</code> and <code>train_labels</code> (containing <code>batch_size=64</code> features and labels respectively). Because we specified <code>shuffle=True</code>, the data are shuffled after we iterate over all of our batches.</p>
<div id="787bbacb" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Display image and label.</span></span>
<span id="cb11-2">train_features, train_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>(train_dataloader))</span>
<span id="cb11-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Feature batch shape: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>train_features<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>size()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Labels batch shape: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>train_labels<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>size()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-5">img <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_features[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].squeeze()</span>
<span id="cb11-6">label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_labels[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb11-7">plt.imshow(img, cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gray"</span>)</span>
<span id="cb11-8">plt.show()</span>
<span id="cb11-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Label: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>label<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Feature batch shape: torch.Size([64, 1, 28, 28])
Labels batch shape: torch.Size([64])
Label: 5</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://vsriram24.github.io/posts/pytorch-tutorial/index_files/figure-html/cell-9-output-2.png" width="415" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="modeling" class="level2">
<h2 class="anchored" data-anchor-id="modeling">Modeling</h2>
<section id="defining-a-neural-network" class="level3">
<h3 class="anchored" data-anchor-id="defining-a-neural-network">Defining a neural network</h3>
<p>Neural networks comprise of layers/modules that perform operations on data. The <code>torch.nn</code> namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the <code>nn.Module</code>. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.</p>
<p>To define a neural network in PyTorch, we create a class that inherits from <code>nn.Module</code>. We define the layers of the network in the <code>__init__</code> function and specify how data will pass through the network in the <code>forward</code> function.</p>
<div id="10b85434" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define model</span></span>
<span id="cb13-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> NeuralNetwork(nn.Module):</span>
<span id="cb13-3">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb13-4">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb13-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.flatten <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Flatten()</span>
<span id="cb13-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.linear_relu_stack <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb13-7">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>),</span>
<span id="cb13-8">            nn.ReLU(),</span>
<span id="cb13-9">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>),</span>
<span id="cb13-10">            nn.ReLU(),</span>
<span id="cb13-11">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb13-12">        )</span>
<span id="cb13-13"></span>
<span id="cb13-14">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb13-15">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.flatten(x)</span>
<span id="cb13-16">        logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.linear_relu_stack(x)</span>
<span id="cb13-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> logits</span></code></pre></div>
</div>
<p>Now that we’ve defined the structure of our <code>NeuralNetwork</code>, we can create an instance of it and move it to a faster device (GPU or MPS) if available to accelerate operations. We can also print its structure to see the layers that we’ve just defined.</p>
<div id="a04a753a" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> NeuralNetwork().to(device)</span>
<span id="cb14-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Model structure: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model structure: NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<p>Many layers inside a neural network are parameterized, meaning that they have associated weights and biases that are optimized during training. Subclassing <code>nn.Module</code> automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s <code>parameters()</code> or <code>named_parameters()</code> methods.</p>
<p>The linear layer is a module that applies a linear transformation on the input using its stored weights and biases. Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.</p>
<p>In this model, we use <code>nn.ReLU</code> between our linear layers, but there are other options for activations to introduce non-linearity in your model.</p>
<div id="f0865109" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> name, param <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> model.named_parameters():</span>
<span id="cb16-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Layer: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | Size: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>size()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | Values : </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>param[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0069, -0.0090, -0.0053,  ...,  0.0014, -0.0055, -0.0313],
        [-0.0027, -0.0257, -0.0303,  ..., -0.0329,  0.0320, -0.0336]],
       device='mps:0', grad_fn=&lt;SliceBackward0&gt;) 

Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0255, -0.0069], device='mps:0', grad_fn=&lt;SliceBackward0&gt;) 

Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0210, -0.0186,  0.0411,  ...,  0.0346,  0.0432, -0.0231],
        [-0.0199,  0.0335, -0.0396,  ..., -0.0416,  0.0382,  0.0423]],
       device='mps:0', grad_fn=&lt;SliceBackward0&gt;) 

Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0328, -0.0148], device='mps:0', grad_fn=&lt;SliceBackward0&gt;) 

Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0231, -0.0346, -0.0262,  ..., -0.0030, -0.0107,  0.0297],
        [ 0.0025, -0.0110, -0.0214,  ...,  0.0298, -0.0307,  0.0295]],
       device='mps:0', grad_fn=&lt;SliceBackward0&gt;) 

Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0358,  0.0367], device='mps:0', grad_fn=&lt;SliceBackward0&gt;) 
</code></pre>
</div>
</div>
</section>
<section id="optimizing-the-model-parameters" class="level3">
<h3 class="anchored" data-anchor-id="optimizing-the-model-parameters">Optimizing the Model Parameters</h3>
<p>Now that we have our data and our model, it’s time to train, validate and test our model by optimizing its parameters on the input data! Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters, and optimizes these parameters using gradient descent.</p>
<section id="hyperparameters" class="level4">
<h4 class="anchored" data-anchor-id="hyperparameters">Hyperparameters</h4>
<p>Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates.</p>
<p>We define the following hyperparameters for training: - Number of Epochs - the number times to iterate over the dataset - Batch Size - the number of data samples propagated through the network before the parameters are updated - Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.</p>
<div id="4762a8a7" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-3</span></span>
<span id="cb18-2">batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span></span>
<span id="cb18-3">epochs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span></code></pre></div>
</div>
</section>
<section id="optimization-loop" class="level4">
<h4 class="anchored" data-anchor-id="optimization-loop">Optimization Loop</h4>
<p>To train our model, we need a loss function as well as an optimizer.</p>
<p>Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an epoch.</p>
<p>Each epoch consists of two main parts: - The Train Loop - iterate over the training dataset and try to converge to optimal parameters. - The Validation/Test Loop - iterate over the test dataset to check if model performance is improving.</p>
<p>The loss function measures the degree of dissimilarity between an obtained result and the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.</p>
<p>Common loss functions include <code>nn.MSELoss</code> (Mean Square Error) for regression tasks, and <code>nn.NLLLoss</code> (Negative Log Likelihood) for classification. <code>nn.CrossEntropyLoss</code> combines <code>nn.LogSoftmax</code> and <code>nn.NLLLoss</code>.</p>
<p>We pass our model’s output logits to <code>nn.CrossEntropyLoss</code>, which will normalize the logits and compute the prediction error.</p>
<div id="e0f2514b" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize the loss function</span></span>
<span id="cb19-2">loss_fn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.CrossEntropyLoss()</span></code></pre></div>
</div>
<p>Optimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer; additionally, there are many different optimizers available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.</p>
<p>We initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter.</p>
<div id="0c4c879d" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.SGD(model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>learning_rate)</span></code></pre></div>
</div>
<p>Inside the training loop, optimization happens in three steps: - Call <code>optimizer.zero_grad()</code> to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration. - Backpropagate the prediction loss with a call to <code>loss.backward()</code>. PyTorch deposits the gradients of the loss w.r.t. each parameter. - Once we have our gradients, we call <code>optimizer.step()</code> to adjust the parameters by the gradients collected in the backward pass.</p>
</section>
<section id="full-implementation" class="level4">
<h4 class="anchored" data-anchor-id="full-implementation">Full Implementation</h4>
<p>We define <code>train_loop</code> that loops over our optimization code, and <code>test_loop</code> that evaluates the model’s performance against our test data.</p>
<div id="5e10e012" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> train_loop(dataloader, model, loss_fn, optimizer):</span>
<span id="cb21-2">    size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(dataloader.dataset)</span>
<span id="cb21-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Set the model to training mode - important for batch normalization and dropout layers</span></span>
<span id="cb21-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Unnecessary in this situation but added for best practices</span></span>
<span id="cb21-5">    model.train()</span>
<span id="cb21-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> batch, (X, y) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(dataloader):</span>
<span id="cb21-7">        X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.to(device)</span>
<span id="cb21-8">        y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y.to(device)</span>
<span id="cb21-9">        </span>
<span id="cb21-10">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute prediction and loss</span></span>
<span id="cb21-11">        pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(X)</span>
<span id="cb21-12">        loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> loss_fn(pred, y)</span>
<span id="cb21-13"></span>
<span id="cb21-14">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Backpropagation</span></span>
<span id="cb21-15">        loss.backward()</span>
<span id="cb21-16">        optimizer.step()</span>
<span id="cb21-17">        optimizer.zero_grad()</span>
<span id="cb21-18"></span>
<span id="cb21-19">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb21-20">            loss, current <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> loss.item(), batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(X)</span>
<span id="cb21-21">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&gt;7f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">  [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>current<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&gt;5d}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>size<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&gt;5d}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]"</span>)</span>
<span id="cb21-22"></span>
<span id="cb21-23"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> test_loop(dataloader, model, loss_fn):</span>
<span id="cb21-24">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Set the model to evaluation mode - important for batch normalization and dropout layers</span></span>
<span id="cb21-25">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Unnecessary in this situation but added for best practices</span></span>
<span id="cb21-26">    model.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb21-27">    size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(dataloader.dataset)</span>
<span id="cb21-28">    num_batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(dataloader)</span>
<span id="cb21-29">    test_loss, correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb21-30"></span>
<span id="cb21-31">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode</span></span>
<span id="cb21-32">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True</span></span>
<span id="cb21-33">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb21-34">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> X, y <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> dataloader:</span>
<span id="cb21-35">            X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.to(device)</span>
<span id="cb21-36">            y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y.to(device)</span>
<span id="cb21-37">        </span>
<span id="cb21-38">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(X)</span>
<span id="cb21-39">            test_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> loss_fn(pred, y).item()</span>
<span id="cb21-40">            correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> (pred.argmax(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> y).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>().item()</span>
<span id="cb21-41"></span>
<span id="cb21-42">    test_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/=</span> num_batches</span>
<span id="cb21-43">    correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/=</span> size</span>
<span id="cb21-44">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Test Error: </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> Accuracy: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>correct)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&gt;0.1f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">%, Avg loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>test_loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&gt;8f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
<p>In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and then backpropagates the prediction error to adjust the model’s parameters.</p>
<p>We can also check the model’s performance against the test dataset to ensure it is learning.</p>
<p>The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch.</p>
<div id="ccaff7e7" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(epochs):</span>
<span id="cb22-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Epoch </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>t<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-------------------------------"</span>)</span>
<span id="cb22-3">    train_loop(train_dataloader, model, loss_fn, optimizer)</span>
<span id="cb22-4">    test_loop(test_dataloader, model, loss_fn)</span>
<span id="cb22-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Done!"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1
-------------------------------
loss: 2.296714  [   64/60000]
loss: 2.285919  [ 6464/60000]
loss: 2.289640  [12864/60000]
loss: 2.269527  [19264/60000]
loss: 2.238294  [25664/60000]
loss: 2.245018  [32064/60000]
loss: 2.233091  [38464/60000]
loss: 2.209641  [44864/60000]
loss: 2.167873  [51264/60000]
loss: 2.210138  [57664/60000]
Test Error: 
 Accuracy: 45.4%, Avg loss: 2.163287 

Epoch 2
-------------------------------
loss: 2.148807  [   64/60000]
loss: 2.146786  [ 6464/60000]
loss: 2.100028  [12864/60000]
loss: 2.086866  [19264/60000]
loss: 2.088841  [25664/60000]
loss: 2.022998  [32064/60000]
loss: 2.008321  [38464/60000]
loss: 1.980193  [44864/60000]
loss: 1.942847  [51264/60000]
loss: 1.935418  [57664/60000]
Test Error: 
 Accuracy: 47.7%, Avg loss: 1.898360 

Epoch 3
-------------------------------
loss: 1.934336  [   64/60000]
loss: 1.874416  [ 6464/60000]
loss: 1.709932  [12864/60000]
loss: 1.756551  [19264/60000]
loss: 1.738622  [25664/60000]
loss: 1.706300  [32064/60000]
loss: 1.609682  [38464/60000]
loss: 1.628336  [44864/60000]
loss: 1.539030  [51264/60000]
loss: 1.561772  [57664/60000]
Test Error: 
 Accuracy: 59.2%, Avg loss: 1.528465 

Epoch 4
-------------------------------
loss: 1.439419  [   64/60000]
loss: 1.322930  [ 6464/60000]
loss: 1.461314  [12864/60000]
loss: 1.465872  [19264/60000]
loss: 1.336003  [25664/60000]
loss: 1.346315  [32064/60000]
loss: 1.377878  [38464/60000]
loss: 1.287794  [44864/60000]
loss: 1.289096  [51264/60000]
loss: 1.287725  [57664/60000]
Test Error: 
 Accuracy: 62.3%, Avg loss: 1.254067 

Epoch 5
-------------------------------
loss: 1.233157  [   64/60000]
loss: 1.233771  [ 6464/60000]
loss: 1.178213  [12864/60000]
loss: 1.106026  [19264/60000]
loss: 1.188458  [25664/60000]
loss: 1.199469  [32064/60000]
loss: 1.154457  [38464/60000]
loss: 1.008805  [44864/60000]
loss: 1.108838  [51264/60000]
loss: 1.131137  [57664/60000]
Test Error: 
 Accuracy: 64.1%, Avg loss: 1.088662 

Done!</code></pre>
</div>
</div>
<p>We can now use this model to make individual predictions.</p>
<p>To use the model, we pass it the input data. This executes the model’s <code>forward</code>, along with some background operations. Note that we do not call <code>model.forward()</code> directly!</p>
<div id="32e55b06" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">classes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb24-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"T-shirt/top"</span>,</span>
<span id="cb24-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Trouser"</span>,</span>
<span id="cb24-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Pullover"</span>,</span>
<span id="cb24-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Dress"</span>,</span>
<span id="cb24-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Coat"</span>,</span>
<span id="cb24-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sandal"</span>,</span>
<span id="cb24-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Shirt"</span>,</span>
<span id="cb24-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sneaker"</span>,</span>
<span id="cb24-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Bag"</span>,</span>
<span id="cb24-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Ankle boot"</span>,</span>
<span id="cb24-12">]</span>
<span id="cb24-13"></span>
<span id="cb24-14">model.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb24-15">x, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_data[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], test_data[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb24-16"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb24-17">    x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.to(device)</span>
<span id="cb24-18">    pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(x)</span>
<span id="cb24-19">    predicted, actual <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> classes[pred[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].argmax(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)], classes[y]</span>
<span id="cb24-20">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Predicted: "</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>predicted<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">", Actual: "</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>actual<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Predicted: "Ankle boot", Actual: "Ankle boot"</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>This concludes my walkthrough of using basic PyTorch to process data and train a neural network. As mentioned, in a future post in the coming weeks, I will iterate on this pipeline, demonstrating a different neural network architecture and more data exploration for a different dataset. Until next time, [VS]Coders!</p>


</section>

 ]]></description>
  <category>Tutorials</category>
  <guid>https://vsriram24.github.io/posts/pytorch-tutorial/</guid>
  <pubDate>Mon, 29 Jul 2024 07:00:00 GMT</pubDate>
  <media:content url="https://vsriram24.github.io/posts/pytorch-tutorial/zolaCake.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
